{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for logging \n",
    "\n",
    "from comet_ml import Experiment\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.loggers import TestTubeLogger\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "# For metrics\n",
    "from pytorch_lightning import metrics\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import io\n",
    "import torchtext\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping # The EarlyStopping callback can be used to monitor a validation metric and stop the training when no improvement is observed.\n",
    "\"\"\"\n",
    "    To enable it:\n",
    "\n",
    "    Import EarlyStopping callback.\n",
    "\n",
    "    Log the metric you want to monitor using log() method.\n",
    "\n",
    "    Init the callback, and set monitor to the logged metric of your choice.\n",
    "\n",
    "    Pass the EarlyStopping callback to the Trainer callbacks flag.\n",
    "\"\"\"\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.metrics import Metric\n",
    "from pytorch_lightning.metrics.utils import _input_format_classification\n",
    "from sklearn.metrics import classification_report\n",
    "class MyClassificationReport(Metric):\n",
    "    def __init__(self,threshold: float = 0.5,compute_on_step: bool = True,dist_sync_on_step: bool = False):\n",
    "        super().__init__(\n",
    "            compute_on_step=compute_on_step,\n",
    "            dist_sync_on_step=dist_sync_on_step,\n",
    "        )\n",
    "\n",
    "        self.threshold = threshold\n",
    "        self.add_state(\"preds\", default=[], dist_reduce_fx=None)\n",
    "        self.add_state(\"target\", default=[], dist_reduce_fx=None)\n",
    "\n",
    "        # rank_zero_warn(\n",
    "        #     'Metric `MyClassificationReport` will save all targets and predictions in buffer.'\n",
    "        #     ' For large datasets this may lead to large memory footprint.'\n",
    "        # )\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        preds = preds.cpu()\n",
    "        target = target.cpu()\n",
    "        y_hat, y = preds.max(1).indices, target\n",
    "        assert y_hat.shape == y.shape\n",
    "        self.preds.append(y_hat)\n",
    "        self.target.append(y)\n",
    "\n",
    "    def compute(self):\n",
    "        preds = torch.cat(self.preds, dim=0)\n",
    "        target = torch.cat(self.target, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class AlarmDataset(Dataset):\n",
    "    def __init__(self,data,seq_len,batch_size):\n",
    "        self.length = len(data)//seq_len # how much data i have         \n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "       \n",
    "    def __getitem__(self, index: int):\n",
    "        x = self.data[index*self.seq_len:(index*self.seq_len)+self.seq_len]\n",
    "        y = self.data[1+index*self.seq_len:1+(index*self.seq_len)+self.seq_len]\n",
    "        return x,y\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        dir_path = self.config['dir-path']\n",
    "        file_name = 'train.tokens'\n",
    "\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.vocab = build_vocab_from_iterator(map(self.tokenizer,iter(io.open(dir_path+file_name,encoding=\"utf8\"))))\n",
    "    \n",
    "\n",
    "        train_data = self.data_process(iter(io.open(dir_path +\"train.tokens\", encoding=\"utf8\")))\n",
    "        val_data = self.data_process(iter(io.open(dir_path +\"val.tokens\", encoding=\"utf8\")))\n",
    "        test_data = self.data_process(iter(io.open(dir_path +\"test.tokens\", encoding=\"utf8\")))\n",
    "\n",
    "    \n",
    "        self.train_dataset = AlarmDataset(train_data, self.config['seq-len'], self.config['batch-size'])\n",
    "        self.valid_dataset = AlarmDataset(val_data,self.config['seq-len'], self.config['batch-size'])\n",
    "        self.test_dataset = AlarmDataset(test_data, self.config['seq-len'], self.config['batch-size'])\n",
    "\n",
    "    \n",
    "    def data_process(self, raw_text_iter):\n",
    "        data = [torch.tensor([self.vocab[token] for token in self.tokenizer(item)],dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "    \n",
    "    def get_weight_per_class(self):\n",
    "        def lambdaFun(total,v,num_classes):\n",
    "            if v>0:\n",
    "                return total/(v*num_classes) \n",
    "            return 0\n",
    "        \n",
    "        index_2_count = {self.vocab.stoi[k]:self.vocab.freqs[k]  for k in list(self.vocab.stoi)}\n",
    "        total = sum(index_2_count.values())\n",
    "        index_2_ws = {k:lambdaFun(total,v,len(index_2_count)) for k,v in index_2_count.items()}\n",
    "        index_2_ws[1] = 0.0 # MANUALLY Setting the weights to zero for the padding\n",
    "        # index_2_ws[0] = 0.0 # MANUALLY Setting the weights to zero for the padding\n",
    "        ws = torch.tensor([index_2_ws[i] for i in range(len(index_2_ws))])\n",
    "\n",
    "        return ws\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "            Use this method to do things that might write to disk or that need to be done only from a single GPU in distributed settings.\n",
    "            e.g., download,tokenize,etc…\n",
    "        \"\"\" \n",
    "        return None\n",
    "\n",
    "\n",
    "    def setup(self, stage: None):\n",
    "        \"\"\"\n",
    "            There are also data operations you might want to perform on every GPU. Use setup to do things like:\n",
    "            count number of classes,build vocabulary,perform train/val/test splits,apply transforms (defined explicitly in your datamodule or assigned in init),etc…\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.config['batch-size'], shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.valid_dataset, batch_size=self.config['batch-size'], shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.config['batch-size'], shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.accuraccy_50_count = 0\n",
    "        self.config = config        \n",
    "        self.lr = self.config[\"lr\"]\n",
    "        self.weight_decay = self.config[\"weight-decay\"]\n",
    "    \n",
    "        self.pos_encoder = PositionalEncoding(self.config['em-size'], self.config['dropout'])\n",
    "        encoder_layers = torch.nn.TransformerEncoderLayer(self.config['em-size'], self.config['nhead'], self.config['nhid'], self.config[\"dropout\"])\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layers, self.config['nlayers'])\n",
    "        self.encoder = torch.nn.Embedding(self.config[\"vocab-size\"], self.config['em-size'])\n",
    "        self.decoder = torch.nn.Linear(self.config['em-size'], self.config[\"vocab-size\"])\n",
    "        self.src_mask = self.generate_square_subsequent_mask(self.config['seq-len'])\n",
    "        self.init_weights()\n",
    "\n",
    "        self.class_weight = self.config['weight_per_class']\n",
    "\n",
    "        # self.train_F1 = metrics.classification.F1(num_classes=self.config[\"vocab-size\"],average = 'micro')\n",
    "        # self.val_F1 = metrics.classification.F1(num_classes=self.config[\"vocab-size\"],average = 'micro')\n",
    "        # self.test_F1 = metrics.classification.F1(num_classes=self.config[\"vocab-size\"],average = 'micro')\n",
    "        \n",
    "        self.val_CM_normalized = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"],normalize ='true')\n",
    "        self.val_CM_raw = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"])\n",
    "\n",
    "        self.train_CM_normalized = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"],normalize ='true')\n",
    "        self.train_CM_raw = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"])\n",
    "\n",
    "        self.test_CM = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"],normalize ='true')\n",
    "\n",
    "        self.val_MCR = MyClassificationReport()\n",
    "        self.test_MCR = MyClassificationReport()\n",
    "\n",
    "        self.log(\"Sequence length\",self.config['seq-len'])\n",
    "        self.log(\"lr\",self.lr)\n",
    "        self.log(\"# of tokens/vocab_size (unique alarms)\",self.config['vocab-size'])\n",
    "        self.log(\"weight_decay\",self.weight_decay)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src_mask = src_mask.to(self.device)\n",
    "        src = self.encoder(src) * math.sqrt(self.config['em-size'])\n",
    "        src = self.pos_encoder(src)\n",
    "        src_mask = src_mask.to(self.device)\n",
    "      \n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "   # The ReduceLROnPlateau scheduler requires a monitor\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr,weight_decay=self.weight_decay)\n",
    "        d = {\n",
    "       'optimizer': optimizer,\n",
    "       'lr_scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = \"min\", factor = 0.5, patience=10, verbose=True),\n",
    "       'monitor': 'val_epoch_loss',\n",
    "        'interval': 'epoch'\n",
    "        }\n",
    "        return d \n",
    "\n",
    "    def loss_function(self,logits,y):\n",
    "        return F.cross_entropy(logits,y,weight= self.class_weight,ignore_index=1) \n",
    "\n",
    "    def myPrintToFile(self,cm_normal,cm_raw,f):\n",
    "        cm_normal = cm_normal.cpu()\n",
    "        cm_raw = cm_raw.cpu()\n",
    "        \n",
    "\n",
    "        sum_of_each_class = cm_raw.sum(axis=1) # sum along the columns\n",
    "        print(f\"        ------ Epoch {self.current_epoch} ---------\",file=f)\n",
    "        print(f\"Total={[v.item() for v in sum_of_each_class]}\",file=f)\n",
    "        print(f\"Corret={[v.item() for v in torch.diagonal(cm_raw,0)]}\",file=f)\n",
    "        print(f\"Accuracy={[round(v.item(),3) for v in (torch.diagonal(cm_raw,0)/sum_of_each_class)]}\",file=f)\n",
    "\n",
    "        accs = [round(v.item(),3)  for v in torch.diagonal(cm_normal,0)]\n",
    "        print(f\"Acc2={accs}\",file=f)\n",
    "\n",
    "        a_50 = len([a for a in accs if a>=0.5])\n",
    "        a_30 = len([a for a in accs if a>=0.3])\n",
    "        out_str = f\"acc>0.5= {a_50}, acc>=0.3= {a_30}, Total={len(accs)}\"\n",
    "        print(out_str,file=f)\n",
    "\n",
    "        # if temp> self.accuraccy_50_count and train=:\n",
    "        #     self.accuraccy_50_count = temp\n",
    "        print(out_str,end=\" \") \n",
    "\n",
    "      \n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        x = x.T\n",
    "        y = y.T.reshape(-1)\n",
    "\n",
    "        if x.size(0) != self.config['seq-len']:\n",
    "           self.src_mask =  self.generate_square_subsequent_mask(x.size(0))\n",
    "        \n",
    "        y_hat = self(x,self.src_mask)\n",
    "        y_hat =  y_hat.view(-1, self.config['vocab-size'])\n",
    "        loss = self.loss_function(y_hat,y) # cross entropy itself compute softmax \n",
    "\n",
    "        self.train_CM_normalized(F.softmax(y_hat),y)\n",
    "        self.train_CM_raw(F.softmax(y_hat),y)\n",
    "        \n",
    "        self.log('train_loss',loss,logger=True)\n",
    "        # self.log('train_F1',self.train_F1(F.softmax(y_hat),y),logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch, batch_idx):\n",
    "        x,y = batch\n",
    "        x = x.T\n",
    "        y = y.T.reshape(-1)\n",
    "        \n",
    "        if x.size(0) != self.config['seq-len']:\n",
    "            # print(f\">> passed {x.size()}\")\n",
    "            self.src_mask =  self.generate_square_subsequent_mask(x.size(0))\n",
    "        \n",
    "        y_hat = self(x,self.src_mask)\n",
    "        y_hat =  y_hat.view(-1, self.config['vocab-size'])\n",
    "        loss = self.loss_function(y_hat,y)\n",
    "\n",
    "        self.val_MCR(F.softmax(y_hat),y)\n",
    "        self.val_CM_normalized(F.softmax(y_hat),y)\n",
    "        self.val_CM_raw(F.softmax(y_hat),y)\n",
    "\n",
    "        self.log('val_loss',loss,logger=True)\n",
    "        # self.log('val_F1',self.val_F1(F.softmax(y_hat) ,y),logger=True)\n",
    "        return {'val_loss':loss}\n",
    "    \n",
    "    def test_step(self,batch, batch_idx):\n",
    "        x,y = batch\n",
    "        x = x.T\n",
    "        y = y.T.reshape(-1)\n",
    "        if x.size(0) != self.config['seq-len']:\n",
    "           self.src_mask =  self.generate_square_subsequent_mask(x.size(0))\n",
    "\n",
    "        y_hat = self(x,self.src_mask)\n",
    "        y_hat =  y_hat.view(-1,  self.config['vocab-size'])\n",
    "        loss = self.loss_function(y_hat,y)\n",
    "\n",
    "        self.test_MCR(F.softmax(y_hat),y)\n",
    "        self.log('test_loss',loss,logger=True)\n",
    "        # self.log('test_F1', self.test_F1(F.softmax(y_hat) ,y),logger=True)\n",
    "        return {'test_loss':loss}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        avg_loss = torch.stack([d['loss']  for d in outputs]).mean()\n",
    "        # f1 = self.train_F1.compute()\n",
    "        print(f\"[{self.current_epoch}]E, Avg Training loss = {round(avg_loss.item(),4)}\",end=\" \")\n",
    "        \n",
    "        with open(self.config[\"train-file\"],'a') as f:\n",
    "            self.myPrintToFile(self.train_CM_normalized.compute(),self.train_CM_raw.compute(),f)\n",
    "        self.log(\"train_epoch_loss\",avg_loss,logger=True,prog_bar=True)\n",
    "        # self.log(\"train_epoch_F1\", f1, logger=True,prog_bar=True)\n",
    "  \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['val_loss'] for d in outputs]).mean()\n",
    "        # f1 = self.val_F1.compute()\n",
    "        # print(self.val_MCR.compute(),file=open(\"val-out.txt\",'w'))\n",
    "        # print(self.val_CM.compute(),file=open(\"val-cm-out.txt\",'w'))\n",
    "\n",
    "        # if self.current_epoch%4==0 and self.current_epoch>0:\n",
    "        # self.myPrintToFile(self.val_CM_normalized.compute(),self.val_CM_raw.compute())\n",
    "\n",
    "\n",
    "        print(f\"::Val Loss = {round(avg_loss.item(),4) }\",end=\" \")\n",
    "\n",
    "        with open(self.config[\"val-file\"],'a') as f:\n",
    "            self.myPrintToFile(self.val_CM_normalized.compute(),self.val_CM_raw.compute(),f)\n",
    "        \n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "        self.log(\"val_epoch_loss\",avg_loss,logger=True)\n",
    "        # self.log(\"val_epoch_F1\",f1,logger=True,prog_bar=True)\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['test_loss'] for d in outputs]).mean()\n",
    "        # f1 = self.test_F1.compute()\n",
    "        # print(self.test_MCR.compute(),file=open(\"test-out.txt\",'w'))\n",
    "        print(f\">Average Test Loss = {avg_loss.item()}\")\n",
    "        self.log(\"test_epoch_loss\",avg_loss, logger = True)\n",
    "        # self.log(\"test_epoch_F1\",f1, logger=True)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning\n",
    "\n",
    "**Note: When monitoring any parameter after the validation epoch end then you should pass check_val_every_n_epoch=1  not to other. This is very important.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightCondition(w,avg_w):\n",
    "    if w<avg_w:\n",
    "        return w\n",
    "    else:\n",
    "        return avg_w\n",
    "\n",
    "# setup data\n",
    "config_data = {\n",
    "'dir-path' : \"../.data/\",\n",
    "'batch-size' :512, # Batch Size \n",
    "'seq-len' :128, # Sequence length\n",
    "}\n",
    "\n",
    "dm = MyDataModule(config=config_data)\n",
    "ws = dm.get_weight_per_class().cuda()\n",
    "\n",
    "print(\"Before\",[round(w.item(),3) for w in ws])\n",
    "# avg_w = sum(ws)/len(ws)\n",
    "# ws = torch.tensor([weightCondition(w,avg_w) for w in ws]).cuda()\n",
    "print(\"After\",[round(w.item(),3) for w in ws])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config_model = {\n",
    "    'lr' : 0.001,\n",
    "    'dropout' : 0.2,\n",
    "    'weight-decay': 3.1,\n",
    "    'em-size' :256, # embedding dimension \n",
    "    'nhid' : 128, # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "    'nlayers' :4, # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    'nhead' : 2, # the number of heads in the multiheadattention models\n",
    "    'seq-len': config_data['seq-len'], # dont use wandb config \n",
    "    'vocab-size':len(dm.vocab.stoi), # the size of vocabulary /also called tokens\n",
    "    'weight_per_class':ws,\n",
    "    \"val-file\":\"val-out.txt\",\n",
    "    \"train-file\":'train-out.txt'\n",
    "}\n",
    "\n",
    "with open (config_model[\"val-file\"],'w') as f:\n",
    "    f.write(\">> Starting\")\n",
    "\n",
    "with open (config_model[\"train-file\"],'w') as f:\n",
    "    f.write(\">> Starting\")\n",
    "\n",
    "# setup model - note how we refer to sweep parameters with wandb.config\n",
    "model = TransformerModel(config=config_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(\n",
    "        monitor='val_epoch_loss',\n",
    "        min_delta=0,\n",
    "        patience=600,\n",
    "        verbose=True,\n",
    "        mode='min'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(auto_lr_find=0.0001, precision=16,gpus=-1, num_nodes=1,  max_epochs=1200, check_val_every_n_epoch=1,deterministic=True,gradient_clip_val=0.5,enable_pl_optimizer=True,callbacks=[early_stop_callback],progress_bar_refresh_rate=0)\n",
    "\n",
    "# Run learning rate finder\n",
    "lr_finder = trainer.tuner.lr_find(model,dm)\n",
    "\n",
    "# Results can be found in\n",
    "lr_finder.results\n",
    "\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()\n",
    "\n",
    "# Pick point based on plot, or get suggestion\n",
    "new_lr = lr_finder.suggestion()\n",
    "\n",
    "print(f\"Suggested lr = {new_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hparams.lr = new_lr/100 #7.5e-12 # can devide by 10\n",
    "\n",
    "# Fit model\n",
    "trainer.fit(model,dm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(datamodule=dm) # testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb_logger = WandbLogger(project=\"Alarm-Transformers-Net\")\n",
    "# comet_logger = CometLogger(\n",
    "#     api_key=\"YZWScOiWdE8FwQSUj725dRmor\",\n",
    "#     project_name=\"Alarm-Transformers-Net\" # Optional\n",
    "# )\n",
    "# test_tube_logger = TestTubeLogger('tb_logs', name='Alarm-Transformers-Net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stop_callback = EarlyStopping(\n",
    "#    monitor='val_epoch_loss',\n",
    "#    min_delta=0.00,\n",
    "#    patience=20,\n",
    "#    verbose=True,\n",
    "#    mode='min'\n",
    "# )\n",
    "\n",
    "# # setup Trainer\n",
    "# trainer = Trainer(precision=16,gpus=-1, num_nodes=1,  max_epochs=1, check_val_every_n_epoch=1,deterministic=True, logger=[wandb_logger] ,gradient_clip_val=0.5,enable_pl_optimizer=True,callbacks=[early_stop_callback],progress_bar_refresh_rate=0)\n",
    "# trainer.fit(model,dm) # traning and validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Configuration\n",
    "## Hyperparameter optimization with W&B sweeps\n",
    "\n",
    "So far, we chose our learning rate and number of layers arbitrarily. We can automate hyperparameter search with [W&B sweeps](https://docs.wandb.com/sweeps).\n",
    "\n",
    "## Defining sweep configuration\n",
    "\n",
    "Sweeps can be defined in multiple ways:\n",
    "* with a YAML file - best for distributed sweeps and runs from command line\n",
    "* with a Python object - best for notebooks\n",
    "\n",
    "\n",
    "The main items to be defined in a sweep are:\n",
    "*   **Metric** – This is the metric the sweeps are attempting to optimize. Metrics can take a `name` (this metric should be logged by your training script) and a `goal` (maximize or minimize). \n",
    "*   **Search Strategy** – We support several different search strategies with sweeps – `grid`, `random`, `bayes`.\n",
    "*   **Stopping Criteria** – The strategy for determining when to kill off poorly peforming runs, and try more combinations faster. We offer several custom scheduling algorithms like [HyperBand](https://arxiv.org/pdf/1603.06560.pdf) and Envelope.\n",
    "*   **Parameters** – A dictionary containing the hyperparameter names, and discreet values, max and min values or distributions from which to pull their values to sweep over.\n",
    "\n",
    "*Note: you can also use other search libraries such as [Ray-Tune](https://docs.wandb.com/sweeps/ray-tune) or even [create your own search algorithm](https://docs.wandb.com/sweeps/python-api#run-a-local-controller)*\n",
    "\n",
    "Here we just use a Python object. Refer to the [sweep configuration documentation](https://docs.wandb.com/sweeps/configuration) if you want to define more parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep_config = {\n",
    "#   \"method\": \"grid\",   # bayes search\n",
    "#   \"metric\": {           # We want to minimize validation loss\n",
    "#       \"name\": \"val_epoch_F1\",\n",
    "#       \"goal\": \"maximize\"\n",
    "#   },\n",
    "#   \"parameters\": {\n",
    "#         \"batch-size\": {\n",
    "#             # Choose from pre-defined values\n",
    "#             \"values\": [16]\n",
    "#         },\n",
    "#         \"seq-len\": {\n",
    "#             # Choose from pre-defined values\n",
    "#             \"values\": [700,600,400]\n",
    "#         },\n",
    "#         \"lr\": {\n",
    "#             \"values\":[0.0001,0.0002,0.0003]\n",
    "#             # log uniform distribution between exp(min) and exp(max)\n",
    "#             # \"distribution\": \"log_uniform\",\n",
    "#             # \"min\": -9.21,   # exp(-9.21) = 1e-4\n",
    "#             # \"max\": -4.61    # exp(-4.61) = 1e-2\n",
    "#         },\n",
    "#         \"dropout\":{\n",
    "#             \"values\": [0.1,0.15]\n",
    "#         },\n",
    "#         \"weight-decay\":{\n",
    "#             \"values\":[0.0003,0.0002,0.00001]\n",
    "#         },\n",
    "#         \"em-size\":{\n",
    "#             \"values\":[256,128,64]\n",
    "#         },\n",
    "#         \"nhid\":{\n",
    "#             \"values\":[128,64]\n",
    "#         },\n",
    "#         \"nlayers\":{\n",
    "#             \"values\":[4,3,2]\n",
    "#         },\n",
    "#         \"nhead\":{\n",
    "#             \"values\":[8,4,2]\n",
    "#         }\n",
    "\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an agent\n",
    "\n",
    "We now just need to run an agent against the sweep configuration. Sweeps can be run in multiple ways:\n",
    "* through a command line with `wandb sweep my_sweep_config.yaml`\n",
    "* directly within a script/notebook with the [Python sweep API](https://docs.wandb.com/sweeps/python-api)\n",
    "\n",
    "To use the Python sweep API, we define a function that the agent will run, refering to sweep parameters with `wandb.config.my_param`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def sweep_iteration():\n",
    "#     # set up W&B logger\n",
    "#     wandb.init()    # required to have access to `wandb.config`\n",
    "#     wandb_logger = WandbLogger()\n",
    "\n",
    "#     # setup data\n",
    "#     config_data = {\n",
    "#     'batch-size' :wandb.config[\"batch-size\"], # Batch Size \n",
    "#     'seq-len' : wandb.config['seq-len'], # Sequence length\n",
    "#     'filter-seq-len' :350 # remove sequence whose size is greater than this len\n",
    "#     }\n",
    "\n",
    "#     dm = MyDataModule(dir_path=dir_name,file_name=fname,config=config_data)\n",
    "\n",
    "\n",
    "#     config_model = {\n",
    "#         'lr' : wandb.config['lr'],\n",
    "#         'dropout' : wandb.config['dropout'],\n",
    "#         'weight-decay': wandb.config['weight-decay'],\n",
    "#         'em-size' :wandb.config['em-size'], # embedding dimension \n",
    "#         'nhid' : wandb.config['nhid'], # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "#         'nlayers' :wandb.config['nlayers'], # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "#         'nhead' : wandb.config['nhead'], # the number of heads in the multiheadattention models\n",
    "#         'seq-len': config_data['seq-len'], # dont use wandb config \n",
    "#         'vocab-size':len(dm.vocab.stoi) # the size of vocabulary /also called tokens\n",
    "#     }\n",
    "\n",
    "#     # setup model - note how we refer to sweep parameters with wandb.config\n",
    "#     model = TransformerModel(config=config_model)\n",
    "\n",
    "#     # setup Trainer\n",
    "#     trainer = Trainer(precision=16,gpus=-1, num_nodes=1,  max_epochs=1000, check_val_every_n_epoch=1,deterministic=True, logger=[wandb_logger] ,gradient_clip_val=0.5,enable_pl_optimizer=True,callbacks=[early_stop_callback],progress_bar_refresh_rate=0)\n",
    "#     trainer.fit(model,dm) # traning and validation\n",
    "\n",
    "#     return None\n",
    "\n",
    "# # sweep_id = wandb.sweep(sweep_config, project=\"Alarm-Transformers-Net\") # sweep id based on our configuration\n",
    "# # wandb.agent(sweep_id, function=sweep_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout,seq_len=seq_len)\n",
    "# trainer = Trainer(precision=16,gpus=1,max_epochs=400,check_val_every_n_epoch=4,deterministic=True, gradient_clip_val=0.5,logger=tb_logger,progress_bar_refresh_rate=50,auto_lr_find=0.002)\n",
    "# trainer.tune(model,dm) # finding the lr : first way\n",
    "# 2nd way\n",
    "# lr_finder = trainer.tuner.lr_find(model)\n",
    "# print(lr_finder.results)\n",
    "# fig = lr_finder.plot(suggest=True) # Plot with\n",
    "# fig.show()\n",
    "# new_lr = lr_finder.suggestion() # Pick point based on plot, or get suggestion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# config_data = {\n",
    "#     'batch-size' : 64, # Batch Size \n",
    "#     'seq-len' : 900, # Sequence length\n",
    "#     'filter-seq-len' :350 # remove sequence whose size is greater than this len\n",
    "# }\n",
    "\n",
    "# dm = MyDataModule(dir_path=dir_name,file_name=fname,config=config_data)\n",
    "\n",
    "# config_model = {\n",
    "#     'lr' : 0.0001,\n",
    "#     'dropout' : 0.0,\n",
    "#     'weight-decay': 0.0,\n",
    "#     'em-size' :256, # embedding dimension \n",
    "#     'nhid' : 256, # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "#     'nlayers' :2, # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "#     'nhead' : 2, # the number of heads in the multiheadattention models\n",
    "#     'seq-len': config_data['seq-len'],\n",
    "#     'vocab-size':len(dm.vocab.stoi) # the size of vocabulary /also called tokens\n",
    "# }\n",
    "\n",
    "# print(f\"> Vocab Size (Number of Unique Alarms): {config_model['vocab-size']}\")\n",
    "# model = TransformerModel(config=config_model)\n",
    "# print(f\"> Customised lr = {model.lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish() # When we want to close our W&B run, we can call wandb.finish() (mainly useful in notebooks, called automatically in scripts).\n",
    "# accelerator='dp'\n",
    "# progress_bar_refresh_rate=0 # set to zero to disable it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import tempfile\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "# from pytorch_lightning.utilities.cloud_io import load as pl_load\n",
    "# from ray import tune\n",
    "# from ray.tune import CLIReporter\n",
    "# from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n",
    "# from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Tune comes with ready-to-use PyTorch Lightning callbacks. To report metrics back to Tune after each validation epoch, we will use the TuneReportCallback:\n",
    "\n",
    "We are also able to specify the number of epochs to train each model, and the number of GPUs we want to use for training. We also create a TensorBoard logger that writes logfiles directly into Tune’s root trial directory - if we didn’t do that PyTorch Lightning would create subdirectories, and each trial would thus be shown twice in TensorBoard, one time for Tune’s logs, and another time for PyTorch Lightning’s logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_alarmnet_tune(config,num_epochs, dir_name, fname):\n",
    "#     early_stop_callback = EarlyStopping(\n",
    "#         monitor='val_epoch_loss',\n",
    "#         min_delta=0.00,\n",
    "#         patience=6,\n",
    "#         verbose=True,\n",
    "#         mode='min'\n",
    "#     )\n",
    "\n",
    "    \n",
    "#     tuner_callback = TuneReportCallback({\n",
    "#     \"val_epoch_loss\": \"val_epoch_loss\",\n",
    "#     \"val_epoch_F1\": \"val_epoch_F1\"\n",
    "#     }, on=\"validation_end\")\n",
    "\n",
    "#     # setup data\n",
    "#     config_data = {\n",
    "#     'batch-size' :config[\"batch-size\"], # Batch Size \n",
    "#     'seq-len' : config['seq-len'], # Sequence length\n",
    "#     'filter-seq-len':50 # remove sequence whose size is greater than this len\n",
    "#     }\n",
    "\n",
    "#     dm = MyDataModule(dir_path=dir_name,file_name=fname,config=config_data)\n",
    "\n",
    "\n",
    "#     config_model = {\n",
    "#         'lr' : config['lr'],\n",
    "#         'dropout' : config['dropout'],\n",
    "#         'weight-decay': config['weight-decay'],\n",
    "#         'em-size' :config['em-size'], # embedding dimension \n",
    "#         'nhid' : config['nhid'], # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "#         'nlayers' :config['nlayers'], # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "#         'nhead' : config['nhead'], # the number of heads in the multiheadattention models\n",
    "#         'seq-len': config_data['seq-len'], # dont use wandb config \n",
    "#         'vocab-size':len(dm.vocab.stoi), # the size of vocabulary /also called tokens\n",
    "#         'weight_per_class':ws\n",
    "#     }\n",
    "\n",
    "#     # setup model - note how we refer to sweep parameters with wandb.config\n",
    "#     model = TransformerModel(config=config_model)\n",
    "\n",
    "#     # setup Trainer\n",
    "#     # logger=[wandb_logger]\n",
    "#     tb_logger =  TensorBoardLogger(save_dir=tune.get_trial_dir(), name=\"\", version=\".\")\n",
    "#     trainer = Trainer(precision=16,gpus=-1, num_nodes=1,  max_epochs=100, check_val_every_n_epoch=1,deterministic=True, logger = [tb_logger],  gradient_clip_val=0.5,enable_pl_optimizer=True,callbacks=[early_stop_callback,tuner_callback],progress_bar_refresh_rate=0)\n",
    "#     trainer.fit(model,dm) # traning and validation\n",
    "#     return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use an Asynchronous Hyperband scheduler. This scheduler decides at each iteration which trials are likely to perform badly, and stops these trials. This way we don’t waste any resources on bad hyperparameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tune_alarmnet_asha(num_samples, num_epochs, gpus_per_trial=0):\n",
    "#     dir_name = \"/home/waris/Github/predict-future-alarms/.data/\"\n",
    "#     fname = 'seqs.tokens'\n",
    "#     config_tune = {\n",
    "#         \"batch-size\":  tune.choice([32]), \n",
    "#         \"seq-len\": tune.grid_search([16,32,64,128,256]),        \n",
    "#         \"lr\": tune.grid_search([0.0001,0.001]),\n",
    "#         \"dropout\": tune.grid_search ( [0.1,0.15]),\n",
    "#         \"weight-decay\": tune.grid_search([0.0003,0.0002]),\n",
    "#         \"em-size\":  tune.grid_search([256,128]),\n",
    "#         \"nhid\": tune.grid_search([128]),\n",
    "#         \"nlayers\": tune.grid_search([4,3]),\n",
    "#         \"nhead\": tune.grid_search([8,4])\n",
    "#         }\n",
    "\n",
    "#     scheduler = ASHAScheduler(max_t=num_epochs,grace_period=1,reduction_factor=2)\n",
    "\n",
    "#     # reporter = CLIReporter(\n",
    "#     #     parameter_columns=[\"seq-len\",\"lr\",\"dropout\",\"weight-decay\",\"em-size\",\"nhid\",\"nlayers\",\"nhead\"],\n",
    "#     #     metric_columns=[\"val_epoch_loss\",\"val_epoch_F1\",\"training_iteration\"]\n",
    "#     # )\n",
    "\n",
    "#     analysis = tune.run(\n",
    "#         tune.with_parameters(train_alarmnet_tune,num_epochs=num_epochs,dir_name=dir_name,fname=fname),\n",
    "#         resources_per_trial={\"cpu\": 4,\"gpu\":1},\n",
    "#         metric=\"val_epoch_loss\",\n",
    "#         mode=\"min\",\n",
    "#         config=config_tune,\n",
    "#         num_samples=num_samples,\n",
    "#         scheduler=scheduler,\n",
    "#         # progress_reporter=reporter,\n",
    "#         name=\"tune_alarmnet_asha\")\n",
    "\n",
    "#     print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "\n",
    "#     shutil.rmtree(dir_name+\"ray-tune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Tune will now proceed to sample 10 different parameter combinations randomly, train them, and compare their performance afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tune_alarmnet_asha(num_samples=10, num_epochs=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Population Based Training to find the best parameters (2nd way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_alarmnet_tune_checkpoint(config=None,num_epochs=None,checkpoint_dir=None,dir_name=None, fname=None):\n",
    "#     early_stop_callback = EarlyStopping(\n",
    "#         monitor='val_epoch_loss',\n",
    "#         min_delta=0.00,\n",
    "#         patience=6,\n",
    "#         verbose=True,\n",
    "#         mode='min'\n",
    "#     )\n",
    "\n",
    "    \n",
    "#     # tuner_callback = TuneReportCallback({\n",
    "#     # \"val_epoch_loss\": \"val_epoch_loss\",\n",
    "#     # \"val_epoch_F1\": \"val_epoch_F1\"\n",
    "#     # }, on=\"validation_end\")\n",
    "\n",
    "#     tuner_callback = TuneReportCheckpointCallback(\n",
    "#                 metrics={\n",
    "#                     \"val_epoch_loss\": \"val_epoch_loss\",\n",
    "#                     \"val_epoch_F1\": \"val_epoch_F1\"\n",
    "#                 },\n",
    "#                 filename=\"checkpoint\",\n",
    "#                 on=\"validation_end\")\n",
    "\n",
    "\n",
    "#     # setup data\n",
    "#     config_data = {\n",
    "#     'batch-size' :config[\"batch-size\"], # Batch Size \n",
    "#     'seq-len' : config['seq-len'], # Sequence length\n",
    "#     'filter-seq-len':350 # remove sequence whose size is greater than this len\n",
    "#     }\n",
    "\n",
    "#     dm = MyDataModule(dir_path=dir_name,file_name=fname,config=config_data)\n",
    "\n",
    "\n",
    "#     config_model = {\n",
    "#         'lr' : config['lr'],\n",
    "#         'dropout' : config['dropout'],\n",
    "#         'weight-decay': config['weight-decay'],\n",
    "#         'em-size' :config['em-size'], # embedding dimension \n",
    "#         'nhid' : config['nhid'], # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "#         'nlayers' :config['nlayers'], # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "#         'nhead' : config['nhead'], # the number of heads in the multiheadattention models\n",
    "#         'seq-len': config_data['seq-len'], # dont use wandb config \n",
    "#         'vocab-size':len(dm.vocab.stoi) # the size of vocabulary /also called tokens\n",
    "#     }\n",
    "\n",
    "#     model = None\n",
    "\n",
    "#     if checkpoint_dir:\n",
    "#         print(\"-----------------------------------------------------------------------------------------\")\n",
    "#         # Currently, this leads to errors:\n",
    "#         # model = LightningMNISTClassifier.load_from_checkpoint(\n",
    "#         #     os.path.join(checkpoint, \"checkpoint\"))\n",
    "#         # Workaround:\n",
    "#         ckpt = pl_load(os.path.join(checkpoint_dir, \"checkpoint\"),map_location=lambda storage, loc: storage)\n",
    "#         model = TransformerModel._load_model_state(ckpt, config=config_model)\n",
    "#         trainer.current_epoch = ckpt[\"epoch\"]\n",
    "#     else:\n",
    "#         model = TransformerModel(config=config_model)\n",
    "\n",
    "#     # setup model - note how we refer to sweep parameters with wandb.config\n",
    "#     # model = TransformerModel(config=config_model)\n",
    "\n",
    "#     # setup Trainer\n",
    "#     # logger=[wandb_logger]\n",
    "#     tb_logger =  TensorBoardLogger(save_dir=tune.get_trial_dir(), name=\"\", version=\".\")\n",
    "#     trainer = Trainer(precision=16,gpus=-1, num_nodes=1,  max_epochs=100, check_val_every_n_epoch=1,deterministic=True, logger = [tb_logger],  gradient_clip_val=0.5,enable_pl_optimizer=True,callbacks=[early_stop_callback,tuner_callback],progress_bar_refresh_rate=0)\n",
    "\n",
    "\n",
    "#     trainer.fit(model,dm) # traning and validation\n",
    "#     return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tune_alarmnet_pbt(num_samples, num_epochs, gpus_per_trial=0):\n",
    "#     dir_name = \"/home/waris/Github/predict-future-alarms/.data/\"\n",
    "#     fname = 'seqs.tokens'\n",
    "#     config_tune = {\n",
    "#         \"batch-size\": 16, \n",
    "#         \"seq-len\": tune.grid_search([700,600,400]),        \n",
    "#         \"lr\": 0.0001,\n",
    "#         \"dropout\": tune.grid_search ( [0.1,0.15]),\n",
    "#         \"weight-decay\": tune.grid_search([0.0003,0.0002]),\n",
    "#         \"em-size\":  tune.grid_search([256,512]),\n",
    "#         \"nhid\": tune.grid_search([128,64]),\n",
    "#         \"nlayers\": tune.grid_search([3,2]),\n",
    "#         \"nhead\": tune.grid_search([8,4])\n",
    "#         }\n",
    "\n",
    "#     # scheduler = ASHAScheduler(max_t=num_epochs,grace_period=1,reduction_factor=2)\n",
    "\n",
    "#     scheduler = PopulationBasedTraining(\n",
    "#         perturbation_interval=4,\n",
    "#         hyperparam_mutations={\n",
    "#             \"lr\": [0.0001,0.0002],\n",
    "#             \"batch-size\": [8,16]\n",
    "#         })\n",
    "\n",
    "#     # reporter = CLIReporter(\n",
    "#     #     parameter_columns=[\"seq-len\",\"lr\",\"dropout\",\"weight-decay\",\"em-size\",\"nhid\",\"nlayers\",\"nhead\"],\n",
    "#     #     metric_columns=[\"val_epoch_loss\",\"val_epoch_F1\",\"training_iteration\"]\n",
    "#     # )\n",
    "\n",
    "#     analysis = tune.run(\n",
    "#         tune.with_parameters(train_alarmnet_tune_checkpoint,num_epochs=num_epochs,dir_name=dir_name,fname=fname),\n",
    "#         resources_per_trial={\"cpu\": 4,\"gpu\":1},\n",
    "#         metric=\"val_epoch_F1\",\n",
    "#         mode=\"max\",\n",
    "#         config=config_tune,\n",
    "#         num_samples=num_samples,\n",
    "#         scheduler=scheduler,\n",
    "#         # progress_reporter=reporter,\n",
    "#         name=\"tune_alarmnet_pbt\")\n",
    "\n",
    "#     print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "\n",
    "#     shutil.rmtree(dir_name+\"ray-tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune_alarmnet_pbt(num_samples=2,num_epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('dl': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8076afad8c2f739e22f417bad77704dbad7b0389c4d6903b1ae4a1b7479f7ed3"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}