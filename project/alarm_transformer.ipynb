{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for logging \n",
    "import os\n",
    "# from pytorch_lightning import loggers as pl_loggers\n",
    "# from comet_ml import Experiment\n",
    "# from comet_ml import OfflineExperiment\n",
    "# from pytorch_lightning.loggers import CometLogger\n",
    "# from pytorch_lightning.loggers import MLFlowLogger\n",
    "\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import io\n",
    "import torchtext\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping # The EarlyStopping callback can be used to monitor a validation metric and stop the training when no improvement is observed.\n",
    "\"\"\"\n",
    "    To enable it:\n",
    "\n",
    "    Import EarlyStopping callback.\n",
    "\n",
    "    Log the metric you want to monitor using log() method.\n",
    "\n",
    "    Init the callback, and set monitor to the logged metric of your choice.\n",
    "\n",
    "    Pass the EarlyStopping callback to the Trainer callbacks flag.\n",
    "\"\"\"\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "# seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class AlarmDataset(Dataset):\n",
    "    def __init__(self,data,seq_len,batch_size):\n",
    "        self.length = len(data)//seq_len # how much data i have         \n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "       \n",
    "    def __getitem__(self, index: int):\n",
    "        x = self.data[index*self.seq_len:(index*self.seq_len)+seq_len]\n",
    "        y = self.data[1+index*self.seq_len:1+(index*self.seq_len)+seq_len]\n",
    "        return x,y\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, data_path:str, batch_size:int, seq_len:int):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.data_path = data_path\n",
    "\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.vocab = build_vocab_from_iterator(map(self.tokenizer,iter(io.open(self.data_path,encoding=\"utf8\"))))\n",
    "                \n",
    "        # url = data_path\n",
    "        # test_filepath, valid_filepath, train_filepath = extract_archive(download_from_url(url))\n",
    "        seqs = None\n",
    "        with open(self.data_path) as f:\n",
    "            seqs = f.readlines()\n",
    "        seqs = [seq for seq in seqs if len(seq.split())<=350]\n",
    "\n",
    "        print(f\"total seqs= {len(seqs)}\")\n",
    "        print(seqs[:4])\n",
    "        train, valid = train_test_split(seqs,test_size=0.30,shuffle=False)\n",
    "        valid, test = train_test_split(valid,test_size=0.30, shuffle=False)\n",
    "\n",
    "        with open(\"./.data/train.tokens\",\"w\") as f:\n",
    "            for seq in train:\n",
    "                f.write(seq)\n",
    "        \n",
    "        with open(\"./.data/val.tokens\",\"w\") as f:\n",
    "            for seq in valid:\n",
    "                f.write(seq)\n",
    "            \n",
    "        with open(\"./.data/test.tokens\",\"w\") as f:\n",
    "            for seq in test:\n",
    "                f.write(seq)\n",
    "\n",
    "        train_data = self.data_process(iter(io.open(\"./.data/train.tokens\", encoding=\"utf8\")))\n",
    "        val_data = self.data_process(iter(io.open(\"./.data/val.tokens\", encoding=\"utf8\")))\n",
    "        test_data = self.data_process(iter(io.open(\"./.data/test.tokens\", encoding=\"utf8\")))\n",
    "\n",
    "    \n",
    "        self.train_dataset = AlarmDataset(train_data, seq_len,self.batch_size)\n",
    "        self.valid_dataset = AlarmDataset(val_data,seq_len,self.batch_size)\n",
    "        self.test_dataset = AlarmDataset(test_data, seq_len,self.batch_size)\n",
    "\n",
    "    \n",
    "    def data_process(self, raw_text_iter):\n",
    "        data = [torch.tensor([self.vocab[token] for token in self.tokenizer(item)],dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "            Use this method to do things that might write to disk or that need to be done only from a single GPU in distributed settings.\n",
    "            e.g., download,tokenize,etc…\n",
    "        \"\"\" \n",
    "        return None\n",
    "\n",
    "\n",
    "    def setup(self, stage: None):\n",
    "        \"\"\"\n",
    "            There are also data operations you might want to perform on every GPU. Use setup to do things like:\n",
    "            count number of classes,build vocabulary,perform train/val/test splits,apply transforms (defined explicitly in your datamodule or assigned in init),etc…\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.valid_dataset, batch_size=self.batch_size, shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5, seq_len=None, lr=0.0013,weight_decay=0.0):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.ntoken = ntoken\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = torch.nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = torch.nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = torch.nn.Linear(ninp, ntoken)\n",
    "        self.src_mask = self.generate_square_subsequent_mask(seq_len)\n",
    "        self.seq_len = seq_len \n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src_mask = src_mask.to(self.device)\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        src_mask = src_mask.to(self.device)\n",
    "      \n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "   # The ReduceLROnPlateau scheduler requires a monitor\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr,weight_decay=self.weight_decay)\n",
    "        d = {\n",
    "       'optimizer': optimizer,\n",
    "       'lr_scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = \"min\", factor = 0.5, patience=6, verbose=True),\n",
    "       'monitor': 'val_epoch_loss',\n",
    "        'interval': 'epoch'\n",
    "        }\n",
    "        return d    \n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        x = x.T\n",
    "        y = y.T.reshape(-1)\n",
    "\n",
    "        if x.size(0) != self.seq_len:\n",
    "           self.src_mask =  self.generate_square_subsequent_mask(x.size(0))\n",
    "        \n",
    "        y_hat = self(x,self.src_mask)\n",
    "        loss = F.cross_entropy(y_hat.view(-1, self.ntoken),y)\n",
    "        self.log('train_loss', loss,logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch, batch_idx):\n",
    "        x,y = batch\n",
    "        x = x.T\n",
    "        y = y.T.reshape(-1)\n",
    "        # print(\"Validation Shape: \", x.size(),y.size())\n",
    "        \n",
    "        if x.size(0) != self.seq_len:\n",
    "           self.src_mask =  self.generate_square_subsequent_mask(x.size(0))\n",
    "        \n",
    "        y_hat = self(x,self.src_mask)\n",
    "        loss = F.cross_entropy(y_hat.view(-1, self.ntoken),y)\n",
    "        self.log('val_loss', loss,logger=True)\n",
    "        # self.logger.experiment.log({\"val_loss\":loss})\n",
    "        return {'val_loss':loss}\n",
    "    \n",
    "    def test_step(self,batch, batch_idx):\n",
    "        x,y = batch\n",
    "        x = x.T\n",
    "        y = y.T.reshape(-1)\n",
    "        if x.size(0) != self.seq_len:\n",
    "           self.src_mask =  self.generate_square_subsequent_mask(x.size(0))\n",
    "\n",
    "        y_hat = self(x,self.src_mask)\n",
    "        loss = F.cross_entropy(y_hat.view(-1, self.ntoken),y)\n",
    "        return {'test_loss':loss}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['loss']  for d in outputs]).mean()\n",
    "        print(f\">Epoch ={self.current_epoch}, Avg Training loss = {avg_loss}\")\n",
    "        self.log(\"train_epoch_loss\",avg_loss,logger=True)\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['val_loss'] for d in outputs]).mean()\n",
    "        print(f\"> Average Valid Loss = {avg_loss}\")\n",
    "        self.log(\"val_epoch_loss\",avg_loss,logger=True)\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['test_loss'] for d in outputs]).mean()\n",
    "        print(f\"> Average Test Loss = {avg_loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "30825lines [00:04, 6835.65lines/s]\n",
      "total seqs= 27975\n",
      "['A17 A75 A17 A57 A17 A99 A98 A56\\n', 'A245 A246 A50 A243\\n', 'A243 A9 A559 A1025\\n', 'A50 A59 A60 A64 A392 A726 A9 A725 A726 A725 A243 A725\\n']\n",
      "> Vocab Size (Number of Unique Alarms): 960\n",
      "> Customised lr = 0.0001\n"
     ]
    }
   ],
   "source": [
    "file_path = './.data/seqs.tokens'\n",
    "# tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
    "\n",
    "bsize = 256*(4)\n",
    "seq_len = 40\n",
    "\n",
    "dm = MyDataModule(file_path,bsize,seq_len)\n",
    "ntokens = len(dm.vocab.stoi) # the size of vocabulary\n",
    "print(f\"> Vocab Size (Number of Unique Alarms): {ntokens}\")\n",
    "\n",
    "emsize = 256 # embedding dimension\n",
    "nhid = 256 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers =8 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 16 # the number of heads in the multiheadattention models\n",
    "\n",
    "dropout = 0.0 # the dropout value\n",
    "weight_decay = 0.0\n",
    "lr = 0.0001\n",
    "\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout,seq_len=seq_len,lr=lr,weight_decay=weight_decay)\n",
    "print(f\"> Customised lr = {model.lr}\")"
   ]
  },
  {
   "source": [
    "# Trainer\n",
    "\n",
    "**Note: When monitoring any parameter after the validation epoch end then you should pass check_val_every_n_epoch=1  not to other. This is very important.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "och =236, Avg Training loss = 0.5199846625328064\n",
      "> Average Valid Loss = 0.8286483883857727\n",
      ">Epoch =237, Avg Training loss = 0.519305408000946\n",
      "> Average Valid Loss = 0.8294750452041626\n",
      ">Epoch =238, Avg Training loss = 0.5193446278572083\n",
      "> Average Valid Loss = 0.8291098475456238\n",
      ">Epoch =239, Avg Training loss = 0.518928050994873\n",
      "> Average Valid Loss = 0.829520046710968\n",
      ">Epoch =240, Avg Training loss = 0.5191930532455444\n",
      "> Average Valid Loss = 0.827997088432312\n",
      ">Epoch =241, Avg Training loss = 0.518527090549469\n",
      "> Average Valid Loss = 0.8288615345954895\n",
      ">Epoch =242, Avg Training loss = 0.5185628533363342\n",
      "> Average Valid Loss = 0.8284408450126648\n",
      ">Epoch =243, Avg Training loss = 0.5181592702865601\n",
      "> Average Valid Loss = 0.8289340138435364\n",
      ">Epoch =244, Avg Training loss = 0.5184192657470703\n",
      "> Average Valid Loss = 0.827372670173645\n",
      ">Epoch =245, Avg Training loss = 0.5177674889564514\n",
      "> Average Valid Loss = 0.8282879590988159\n",
      ">Epoch =246, Avg Training loss = 0.517802894115448\n",
      "> Average Valid Loss = 0.8277879357337952\n",
      ">Epoch =247, Avg Training loss = 0.5174105167388916\n",
      "> Average Valid Loss = 0.828399658203125\n",
      ">Epoch =248, Avg Training loss = 0.5176668763160706\n",
      "> Average Valid Loss = 0.8267305493354797\n",
      ">Epoch =249, Avg Training loss = 0.5170312523841858\n",
      "> Average Valid Loss = 0.827752947807312\n",
      ">Epoch =250, Avg Training loss = 0.5170645117759705\n",
      "> Average Valid Loss = 0.8271917700767517\n",
      ">Epoch =251, Avg Training loss = 0.5166811943054199\n",
      "> Average Valid Loss = 0.8278550505638123\n",
      ">Epoch =252, Avg Training loss = 0.5169357657432556\n",
      "> Average Valid Loss = 0.8261777758598328\n",
      ">Epoch =253, Avg Training loss = 0.5163124799728394\n",
      "> Average Valid Loss = 0.8272160291671753\n",
      ">Epoch =254, Avg Training loss = 0.5163449048995972\n",
      "> Average Valid Loss = 0.8266366720199585\n",
      ">Epoch =255, Avg Training loss = 0.515972375869751\n",
      "> Average Valid Loss = 0.8273371458053589\n",
      ">Epoch =256, Avg Training loss = 0.5162227153778076\n",
      "> Average Valid Loss = 0.8256397247314453\n",
      ">Epoch =257, Avg Training loss = 0.5156118869781494\n",
      "> Average Valid Loss = 0.8267181515693665\n",
      ">Epoch =258, Avg Training loss = 0.5156468152999878\n",
      "> Average Valid Loss = 0.8260815143585205\n",
      ">Epoch =259, Avg Training loss = 0.5152825713157654\n",
      "> Average Valid Loss = 0.8268428444862366\n",
      ">Epoch =260, Avg Training loss = 0.5155314803123474\n",
      "> Average Valid Loss = 0.8251011967658997\n",
      ">Epoch =261, Avg Training loss = 0.5149310827255249\n",
      "> Average Valid Loss = 0.826242983341217\n",
      ">Epoch =262, Avg Training loss = 0.5149661898612976\n",
      "> Average Valid Loss = 0.8255451917648315\n",
      ">Epoch =263, Avg Training loss = 0.5146118402481079\n",
      "> Average Valid Loss = 0.8263581395149231\n",
      ">Epoch =264, Avg Training loss = 0.5148587822914124\n",
      "> Average Valid Loss = 0.8245947957038879\n",
      ">Epoch =265, Avg Training loss = 0.5142679810523987\n",
      "> Average Valid Loss = 0.8257839679718018\n",
      ">Epoch =266, Avg Training loss = 0.514305055141449\n",
      "> Average Valid Loss = 0.8250394463539124\n",
      ">Epoch =267, Avg Training loss = 0.513955295085907\n",
      "> Average Valid Loss = 0.8259022831916809\n",
      ">Epoch =268, Avg Training loss = 0.5141988396644592\n",
      "> Average Valid Loss = 0.8241376876831055\n",
      ">Epoch =269, Avg Training loss = 0.5136204957962036\n",
      "> Average Valid Loss = 0.8253258466720581\n",
      ">Epoch =270, Avg Training loss = 0.513659656047821\n",
      "> Average Valid Loss = 0.8245803117752075\n",
      ">Epoch =271, Avg Training loss = 0.5133146643638611\n",
      "> Average Valid Loss = 0.8254483938217163\n",
      ">Epoch =272, Avg Training loss = 0.5135521292686462\n",
      "> Average Valid Loss = 0.8236938118934631\n",
      ">Epoch =273, Avg Training loss = 0.5129892230033875\n",
      "> Average Valid Loss = 0.8248893022537231\n",
      ">Epoch =274, Avg Training loss = 0.5130233764648438\n",
      "> Average Valid Loss = 0.8241249322891235\n",
      ">Epoch =275, Avg Training loss = 0.5126916766166687\n",
      "> Average Valid Loss = 0.8249993920326233\n",
      ">Epoch =276, Avg Training loss = 0.5129216909408569\n",
      "> Average Valid Loss = 0.8232659101486206\n",
      ">Epoch =277, Avg Training loss = 0.512371838092804\n",
      "> Average Valid Loss = 0.8244546055793762\n",
      ">Epoch =278, Avg Training loss = 0.5124040246009827\n",
      "> Average Valid Loss = 0.8237048983573914\n",
      ">Epoch =279, Avg Training loss = 0.5120798349380493\n",
      "> Average Valid Loss = 0.8245915174484253\n",
      ">Epoch =280, Avg Training loss = 0.5123070478439331\n",
      "> Average Valid Loss = 0.8228542804718018\n",
      ">Epoch =281, Avg Training loss = 0.5117668509483337\n",
      "> Average Valid Loss = 0.8240790367126465\n",
      ">Epoch =282, Avg Training loss = 0.5118008852005005\n",
      "> Average Valid Loss = 0.8232778906822205\n",
      ">Epoch =283, Avg Training loss = 0.5114824175834656\n",
      "> Average Valid Loss = 0.8242136836051941\n",
      ">Epoch =284, Avg Training loss = 0.5117086172103882\n",
      "> Average Valid Loss = 0.82245934009552\n",
      ">Epoch =285, Avg Training loss = 0.5111761093139648\n",
      "> Average Valid Loss = 0.823681652545929\n",
      ">Epoch =286, Avg Training loss = 0.5112114548683167\n",
      "> Average Valid Loss = 0.8228979706764221\n",
      ">Epoch =287, Avg Training loss = 0.5108973979949951\n",
      "> Average Valid Loss = 0.8238555788993835\n",
      ">Epoch =288, Avg Training loss = 0.5111268162727356\n",
      "> Average Valid Loss = 0.822083592414856\n",
      ">Epoch =289, Avg Training loss = 0.5105961561203003\n",
      "> Average Valid Loss = 0.823320209980011\n",
      ">Epoch =290, Avg Training loss = 0.5106309652328491\n",
      "> Average Valid Loss = 0.8225463032722473\n",
      ">Epoch =291, Avg Training loss = 0.510325014591217\n",
      "> Average Valid Loss = 0.8235061168670654\n",
      ">Epoch =292, Avg Training loss = 0.5105542540550232\n",
      "> Average Valid Loss = 0.8217445611953735\n",
      ">Epoch =293, Avg Training loss = 0.510028600692749\n",
      "> Average Valid Loss = 0.8229772448539734\n",
      ">Epoch =294, Avg Training loss = 0.5100619196891785\n",
      "> Average Valid Loss = 0.8222035765647888\n",
      ">Epoch =295, Avg Training loss = 0.5097641944885254\n",
      "> Average Valid Loss = 0.823185920715332\n",
      ">Epoch =296, Avg Training loss = 0.5099928379058838\n",
      "> Average Valid Loss = 0.8214095830917358\n",
      ">Epoch =297, Avg Training loss = 0.5094739198684692\n",
      "> Average Valid Loss = 0.8226614594459534\n",
      ">Epoch =298, Avg Training loss = 0.5095085501670837\n",
      "> Average Valid Loss = 0.8218766450881958\n",
      ">Epoch =299, Avg Training loss = 0.5092135071754456\n",
      "> Average Valid Loss = 0.8228764533996582\n",
      ">Epoch =300, Avg Training loss = 0.5094444155693054\n",
      "> Average Valid Loss = 0.82109534740448\n",
      ">Epoch =301, Avg Training loss = 0.5089287161827087\n",
      "> Average Valid Loss = 0.8223548531532288\n",
      ">Epoch =302, Avg Training loss = 0.5089680552482605\n",
      "> Average Valid Loss = 0.8215457797050476\n",
      ">Epoch =303, Avg Training loss = 0.5086726546287537\n",
      "> Average Valid Loss = 0.8225600719451904\n",
      ">Epoch =304, Avg Training loss = 0.5089043378829956\n",
      "> Average Valid Loss = 0.8207800388336182\n",
      ">Epoch =305, Avg Training loss = 0.5083948969841003\n",
      "> Average Valid Loss = 0.8220350742340088\n",
      ">Epoch =306, Avg Training loss = 0.5084362626075745\n",
      "> Average Valid Loss = 0.8212385773658752\n",
      ">Epoch =307, Avg Training loss = 0.5081441402435303\n",
      "> Average Valid Loss = 0.8222590684890747\n",
      ">Epoch =308, Avg Training loss = 0.5083771347999573\n",
      "> Average Valid Loss = 0.8204910755157471\n",
      ">Epoch =309, Avg Training loss = 0.5078713297843933\n",
      "> Average Valid Loss = 0.8217502236366272\n",
      ">Epoch =310, Avg Training loss = 0.5079142451286316\n",
      "> Average Valid Loss = 0.8209607005119324\n",
      ">Epoch =311, Avg Training loss = 0.5076252818107605\n",
      "> Average Valid Loss = 0.8219620585441589\n",
      ">Epoch =312, Avg Training loss = 0.5078595280647278\n",
      "> Average Valid Loss = 0.820212185382843\n",
      ">Epoch =313, Avg Training loss = 0.5073570013046265\n",
      "> Average Valid Loss = 0.8214739561080933\n",
      ">Epoch =314, Avg Training loss = 0.5074034929275513\n",
      "> Average Valid Loss = 0.8206683397293091\n",
      ">Epoch =315, Avg Training loss = 0.5071159601211548\n",
      "> Average Valid Loss = 0.821689784526825\n",
      ">Epoch =316, Avg Training loss = 0.5073493123054504\n",
      "> Average Valid Loss = 0.8199345469474792\n",
      ">Epoch =317, Avg Training loss = 0.5068520307540894\n",
      "> Average Valid Loss = 0.821201503276825\n",
      ">Epoch =318, Avg Training loss = 0.506902813911438\n",
      "> Average Valid Loss = 0.8203950524330139\n",
      ">Epoch =319, Avg Training loss = 0.5066138505935669\n",
      "> Average Valid Loss = 0.8214253783226013\n",
      ">Epoch =320, Avg Training loss = 0.5068495273590088\n",
      "> Average Valid Loss = 0.8196802139282227\n",
      ">Epoch =321, Avg Training loss = 0.506355345249176\n",
      "> Average Valid Loss = 0.8209417462348938\n",
      ">Epoch =322, Avg Training loss = 0.5064087510108948\n",
      "> Average Valid Loss = 0.8201295137405396\n",
      ">Epoch =323, Avg Training loss = 0.5061236023902893\n",
      "> Average Valid Loss = 0.8211644291877747\n",
      ">Epoch =324, Avg Training loss = 0.5063583254814148\n",
      "> Average Valid Loss = 0.8194247484207153\n",
      ">Epoch =325, Avg Training loss = 0.5058687329292297\n",
      "> Average Valid Loss = 0.8206759095191956\n",
      ">Epoch =326, Avg Training loss = 0.5059225559234619\n",
      "> Average Valid Loss = 0.8198899030685425\n",
      ">Epoch =327, Avg Training loss = 0.5056408643722534\n",
      "> Average Valid Loss = 0.8209044337272644\n",
      ">Epoch =328, Avg Training loss = 0.5058754682540894\n",
      "> Average Valid Loss = 0.8191664814949036\n",
      ">Epoch =329, Avg Training loss = 0.5053895115852356\n",
      "> Average Valid Loss = 0.8204371929168701\n",
      ">Epoch =330, Avg Training loss = 0.5054459571838379\n",
      "> Average Valid Loss = 0.8196482062339783\n",
      ">Epoch =331, Avg Training loss = 0.5051650404930115\n",
      "> Average Valid Loss = 0.8206824660301208\n",
      ">Epoch =332, Avg Training loss = 0.5054011940956116\n",
      "> Average Valid Loss = 0.8189480900764465\n",
      ">Epoch =333, Avg Training loss = 0.5049176812171936\n",
      "> Average Valid Loss = 0.8202201128005981\n",
      ">Epoch =334, Avg Training loss = 0.5049758553504944\n",
      "> Average Valid Loss = 0.8194140791893005\n",
      ">Epoch =335, Avg Training loss = 0.5046972632408142\n",
      "> Average Valid Loss = 0.8204460740089417\n",
      ">Epoch =336, Avg Training loss = 0.5049359202384949\n",
      "> Average Valid Loss = 0.8187292218208313\n",
      ">Epoch =337, Avg Training loss = 0.5044523477554321\n",
      "> Average Valid Loss = 0.8200075626373291\n",
      ">Epoch =338, Avg Training loss = 0.5045163631439209\n",
      "> Average Valid Loss = 0.8192028403282166\n",
      ">Epoch =339, Avg Training loss = 0.5042361617088318\n",
      "> Average Valid Loss = 0.8202390670776367\n",
      ">Epoch =340, Avg Training loss = 0.5044776797294617\n",
      "> Average Valid Loss = 0.8185395002365112\n",
      ">Epoch =341, Avg Training loss = 0.5039960145950317\n",
      "> Average Valid Loss = 0.8198111653327942\n",
      ">Epoch =342, Avg Training loss = 0.5040645003318787\n",
      "> Average Valid Loss = 0.8190144896507263\n",
      ">Epoch =343, Avg Training loss = 0.5037826895713806\n",
      "> Average Valid Loss = 0.8200564980506897\n",
      ">Epoch =344, Avg Training loss = 0.5040284395217896\n",
      "> Average Valid Loss = 0.8183627724647522\n",
      ">Epoch =345, Avg Training loss = 0.5035454034805298\n",
      "> Average Valid Loss = 0.8196280002593994\n",
      ">Epoch =346, Avg Training loss = 0.5036214590072632\n",
      "> Average Valid Loss = 0.8188207745552063\n",
      ">Epoch =347, Avg Training loss = 0.5033362507820129\n",
      "> Average Valid Loss = 0.8198756575584412\n",
      ">Epoch =348, Avg Training loss = 0.5035804510116577\n",
      "> Average Valid Loss = 0.8181928992271423\n",
      ">Epoch =349, Avg Training loss = 0.5031038522720337\n",
      "> Average Valid Loss = 0.8194674253463745\n",
      ">Epoch =350, Avg Training loss = 0.5031837224960327\n",
      "> Average Valid Loss = 0.8186472654342651\n",
      ">Epoch =351, Avg Training loss = 0.5028973817825317\n",
      "> Average Valid Loss = 0.8197131752967834\n",
      ">Epoch =352, Avg Training loss = 0.5031415820121765\n",
      "> Average Valid Loss = 0.8180328011512756\n",
      ">Epoch =353, Avg Training loss = 0.5026690363883972\n",
      "> Average Valid Loss = 0.8193113803863525\n",
      ">Epoch =354, Avg Training loss = 0.5027527809143066\n",
      "> Average Valid Loss = 0.8184791207313538\n",
      ">Epoch =355, Avg Training loss = 0.5024673938751221\n",
      "> Average Valid Loss = 0.8195508122444153\n",
      ">Epoch =356, Avg Training loss = 0.5027111768722534\n",
      "> Average Valid Loss = 0.8178681135177612\n",
      ">Epoch =357, Avg Training loss = 0.5022420287132263\n",
      "> Average Valid Loss = 0.8191635608673096\n",
      ">Epoch =358, Avg Training loss = 0.5023316740989685\n",
      "> Average Valid Loss = 0.8182937502861023\n",
      ">Epoch =359, Avg Training loss = 0.5020421743392944\n",
      "> Average Valid Loss = 0.8194015622138977\n",
      ">Epoch =360, Avg Training loss = 0.5022876262664795\n",
      "> Average Valid Loss = 0.817710280418396\n",
      ">Epoch =361, Avg Training loss = 0.5018213987350464\n",
      "> Average Valid Loss = 0.8190167546272278\n",
      ">Epoch =362, Avg Training loss = 0.5019159317016602\n",
      "> Average Valid Loss = 0.8181263208389282\n",
      ">Epoch =363, Avg Training loss = 0.5016244053840637\n",
      "> Average Valid Loss = 0.8192409873008728\n",
      ">Epoch =364, Avg Training loss = 0.5018708109855652\n",
      "> Average Valid Loss = 0.8175765872001648\n",
      ">Epoch =365, Avg Training loss = 0.5014052391052246\n",
      "> Average Valid Loss = 0.8188658952713013\n",
      ">Epoch =366, Avg Training loss = 0.5015064477920532\n",
      "> Average Valid Loss = 0.8179858922958374\n",
      ">Epoch =367, Avg Training loss = 0.5012117028236389\n",
      "> Average Valid Loss = 0.8190690875053406\n",
      ">Epoch =368, Avg Training loss = 0.501460611820221\n",
      "> Average Valid Loss = 0.8174467086791992\n",
      ">Epoch =369, Avg Training loss = 0.5009986162185669\n",
      "> Average Valid Loss = 0.8187100887298584\n",
      ">Epoch =370, Avg Training loss = 0.5011031627655029\n",
      "> Average Valid Loss = 0.8178476691246033\n",
      ">Epoch =371, Avg Training loss = 0.5008083581924438\n",
      "> Average Valid Loss = 0.818926215171814\n",
      ">Epoch =372, Avg Training loss = 0.5010591149330139\n",
      "> Average Valid Loss = 0.8173196911811829\n",
      ">Epoch =373, Avg Training loss = 0.5005995035171509\n",
      "> Average Valid Loss = 0.8185761570930481\n",
      ">Epoch =374, Avg Training loss = 0.5007047653198242\n",
      "> Average Valid Loss = 0.8177202343940735\n",
      ">Epoch =375, Avg Training loss = 0.5004118084907532\n",
      "> Average Valid Loss = 0.8187769055366516\n",
      ">Epoch =376, Avg Training loss = 0.5006599426269531\n",
      "> Average Valid Loss = 0.8171917200088501\n",
      ">Epoch =377, Avg Training loss = 0.5002037286758423\n",
      "> Average Valid Loss = 0.8184346556663513\n",
      ">Epoch =378, Avg Training loss = 0.5003132224082947\n",
      "> Average Valid Loss = 0.8175972700119019\n",
      ">Epoch =379, Avg Training loss = 0.500020444393158\n",
      "> Average Valid Loss = 0.8186306357383728\n",
      ">Epoch =380, Avg Training loss = 0.5002696514129639\n",
      "> Average Valid Loss = 0.8170765042304993\n",
      ">Epoch =381, Avg Training loss = 0.49981629848480225\n",
      "> Average Valid Loss = 0.8182953596115112\n",
      ">Epoch =382, Avg Training loss = 0.4999280273914337\n",
      "> Average Valid Loss = 0.8174803853034973\n",
      ">Epoch =383, Avg Training loss = 0.4996371567249298\n",
      "> Average Valid Loss = 0.8184916973114014\n",
      ">Epoch =384, Avg Training loss = 0.4998854100704193\n",
      "> Average Valid Loss = 0.8169602155685425\n",
      ">Epoch =385, Avg Training loss = 0.499435693025589\n",
      "> Average Valid Loss = 0.8181626796722412\n",
      ">Epoch =386, Avg Training loss = 0.4995516538619995\n",
      "> Average Valid Loss = 0.8173545002937317\n",
      ">Epoch =387, Avg Training loss = 0.4992576837539673\n",
      "> Average Valid Loss = 0.8183626532554626\n",
      ">Epoch =388, Avg Training loss = 0.49950677156448364\n",
      "> Average Valid Loss = 0.816848874092102\n",
      ">Epoch =389, Avg Training loss = 0.49906086921691895\n",
      "> Average Valid Loss = 0.8180355429649353\n",
      ">Epoch =390, Avg Training loss = 0.49917903542518616\n",
      "> Average Valid Loss = 0.817225992679596\n",
      ">Epoch =391, Avg Training loss = 0.49888408184051514\n",
      "> Average Valid Loss = 0.8182340264320374\n",
      ">Epoch =392, Avg Training loss = 0.49913474917411804\n",
      "> Average Valid Loss = 0.8167517781257629\n",
      ">Epoch =393, Avg Training loss = 0.4986902177333832\n",
      "> Average Valid Loss = 0.81794673204422\n",
      ">Epoch =394, Avg Training loss = 0.49881500005722046\n",
      "> Average Valid Loss = 0.817104160785675\n",
      ">Epoch =395, Avg Training loss = 0.49851876497268677\n",
      "> Average Valid Loss = 0.8181217908859253\n",
      ">Epoch =396, Avg Training loss = 0.4987694025039673\n",
      "> Average Valid Loss = 0.8166399002075195\n",
      ">Epoch =397, Avg Training loss = 0.4983276426792145\n",
      "> Average Valid Loss = 0.8178424835205078\n",
      ">Epoch =398, Avg Training loss = 0.49845731258392334\n",
      "> Average Valid Loss = 0.8169876337051392\n",
      ">Epoch =399, Avg Training loss = 0.49815794825553894\n",
      "> Average Valid Loss = 0.81800377368927\n",
      ">Epoch =400, Avg Training loss = 0.4984090328216553\n",
      "> Average Valid Loss = 0.8165481686592102\n",
      ">Epoch =401, Avg Training loss = 0.49796947836875916\n",
      "> Average Valid Loss = 0.817735493183136\n",
      ">Epoch =402, Avg Training loss = 0.4981040954589844\n",
      "> Average Valid Loss = 0.8168867826461792\n",
      ">Epoch =403, Avg Training loss = 0.49780169129371643\n",
      "> Average Valid Loss = 0.8178933262825012\n",
      ">Epoch =404, Avg Training loss = 0.4980538785457611\n",
      "> Average Valid Loss = 0.8164684772491455\n",
      ">Epoch =405, Avg Training loss = 0.49761709570884705\n",
      "Epoch   406: reducing learning rate of group 0 to 5.0000e-06.\n",
      "> Average Valid Loss = 0.8142646551132202\n",
      ">Epoch =406, Avg Training loss = 0.496886283159256\n",
      "> Average Valid Loss = 0.8137379288673401\n",
      ">Epoch =407, Avg Training loss = 0.4962967336177826\n",
      "> Average Valid Loss = 0.8136420249938965\n",
      ">Epoch =408, Avg Training loss = 0.4961574375629425\n",
      "> Average Valid Loss = 0.8137727379798889\n",
      ">Epoch =409, Avg Training loss = 0.49606478214263916\n",
      "> Average Valid Loss = 0.8137600421905518\n",
      ">Epoch =410, Avg Training loss = 0.49597975611686707\n",
      "> Average Valid Loss = 0.8139044642448425\n",
      ">Epoch =411, Avg Training loss = 0.4959063231945038\n",
      "> Average Valid Loss = 0.8138009905815125\n",
      ">Epoch =412, Avg Training loss = 0.49585390090942383\n",
      "Epoch   413: reducing learning rate of group 0 to 2.5000e-06.\n",
      "> Average Valid Loss = 0.8135796785354614\n",
      ">Epoch =413, Avg Training loss = 0.4954806864261627\n",
      "> Average Valid Loss = 0.8138387799263\n",
      ">Epoch =414, Avg Training loss = 0.4955112934112549\n",
      "> Average Valid Loss = 0.813909649848938\n",
      ">Epoch =415, Avg Training loss = 0.49542948603630066\n",
      "> Average Valid Loss = 0.8140057325363159\n",
      ">Epoch =416, Avg Training loss = 0.49534836411476135\n",
      "Epoch   417: reducing learning rate of group 0 to 1.2500e-06.\n",
      "> Average Valid Loss = 0.8153036832809448\n",
      ">Epoch =417, Avg Training loss = 0.4951230585575104\n",
      "> Average Valid Loss = 0.81532222032547\n",
      ">Epoch =418, Avg Training loss = 0.49497750401496887\n",
      "> Average Valid Loss = 0.815285861492157\n",
      ">Epoch =419, Avg Training loss = 0.4949038326740265\n",
      "> Average Valid Loss = 0.8153206706047058\n",
      ">Epoch =420, Avg Training loss = 0.4948650002479553\n",
      "Epoch   421: reducing learning rate of group 0 to 6.2500e-07.\n",
      "> Average Valid Loss = 0.8158292770385742\n",
      ">Epoch =421, Avg Training loss = 0.4946976602077484\n",
      "> Average Valid Loss = 0.8158295750617981\n",
      ">Epoch =422, Avg Training loss = 0.49465951323509216\n",
      "> Average Valid Loss = 0.815894365310669\n",
      ">Epoch =423, Avg Training loss = 0.4946463108062744\n",
      "> Average Valid Loss = 0.8159376978874207\n",
      ">Epoch =424, Avg Training loss = 0.4946337044239044\n",
      "Epoch   425: reducing learning rate of group 0 to 3.1250e-07.\n",
      "> Average Valid Loss = 0.8160751461982727\n",
      ">Epoch =425, Avg Training loss = 0.4945504069328308\n",
      "> Average Valid Loss = 0.8161362409591675\n",
      ">Epoch =426, Avg Training loss = 0.4945411682128906\n",
      "> Average Valid Loss = 0.8161681294441223\n",
      ">Epoch =427, Avg Training loss = 0.49453452229499817\n",
      "> Average Valid Loss = 0.816196620464325\n",
      ">Epoch =428, Avg Training loss = 0.49452871084213257\n",
      "Epoch   429: reducing learning rate of group 0 to 1.5625e-07.\n",
      "> Average Valid Loss = 0.8162382245063782\n",
      ">Epoch =429, Avg Training loss = 0.4944870173931122\n",
      "> Average Valid Loss = 0.8162730932235718\n",
      ">Epoch =430, Avg Training loss = 0.4944837987422943\n",
      "> Average Valid Loss = 0.8162965774536133\n",
      ">Epoch =431, Avg Training loss = 0.4944807291030884\n",
      "> Average Valid Loss = 0.8163151144981384\n",
      ">Epoch =432, Avg Training loss = 0.4944778084754944\n",
      "Epoch   433: reducing learning rate of group 0 to 7.8125e-08.\n",
      "> Average Valid Loss = 0.8163302540779114\n",
      ">Epoch =433, Avg Training loss = 0.4944568872451782\n",
      "COMET INFO: No Comet API Key was found, creating an OfflineExperiment. Set up your API Key to get the full Comet experience https://www.comet.ml/docs/python-sdk/advanced/#python-configuration\n",
      "COMET INFO: No Comet API Key was found, creating an OfflineExperiment. Set up your API Key to get the full Comet experience https://www.comet.ml/docs/python-sdk/advanced/#python-configuration\n",
      "run_id: 760dcd04a887443cb7cb7b4c810da9f5\n",
      "artifacts: ['model/MLmodel', 'model/conda.yaml', 'model/data']\n",
      "params: {'amsgrad': 'False', 'monitor': 'val_epoch_loss', 'stopped_epoch': '0', 'weight_decay': '0.0', 'lr': '1e-05', 'optimizer_name': 'AdamW', 'betas': '(0.9, 0.999)', 'eps': '1e-08', 'epochs': '1200', 'min_delta': '-0.0', 'mode': 'min', 'patience': '20'}\n",
      "metrics: {'val_epoch_loss': 0.8163302540779114, 'best_score': 0.8135796785354614, 'train_loss': 0.4281667172908783, 'stopped_epoch': 433.0, 'train_epoch_loss': 0.4944778084754944, 'val_loss': 0.8520005345344543, 'wait_count': 20.0, 'restored_epoch': 413.0}\n",
      "tags: {'Mode': 'training'}\n"
     ]
    }
   ],
   "source": [
    "import mlflow.pytorch\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_epoch_loss',\n",
    "   min_delta=0.00,\n",
    "   patience=20,\n",
    "   verbose=True,\n",
    "   mode='min'\n",
    ")\n",
    "\n",
    "\n",
    "# mlf_logger = MLFlowLogger(\n",
    "#     experiment_name=\"default\",\n",
    "#     tracking_uri=\"file:./ml-runs\"\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(precision=16,gpus=-1, num_nodes=1, accelerator='dp', max_epochs=1200, check_val_every_n_epoch=1,deterministic=True, gradient_clip_val=0.5,enable_pl_optimizer=True,callbacks=[early_stop_callback], progress_bar_refresh_rate=0)\n",
    "\n",
    "\n",
    "def print_auto_logged_info(r):\n",
    "    tags = {k: v for k, v in r.data.tags.items() if not k.startswith(\"mlflow.\")}\n",
    "    artifacts = [f.path for f in MlflowClient().list_artifacts(r.info.run_id, \"model\")]\n",
    "    print(\"run_id: {}\".format(r.info.run_id))\n",
    "    print(\"artifacts: {}\".format(artifacts))\n",
    "    print(\"params: {}\".format(r.data.params))\n",
    "    print(\"metrics: {}\".format(r.data.metrics))\n",
    "    print(\"tags: {}\".format(tags))\n",
    "\n",
    "# Auto log all MLflow entities\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "# Train the model\n",
    "with mlflow.start_run() as run:\n",
    "    trainer.fit(model,dm) # traning and validation\n",
    "\n",
    "# fetch the auto logged parameters and metrics\n",
    "print_auto_logged_info(mlflow.get_run(run_id=run.info.run_id))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(precision=16,gpus=1,max_epochs=1200,check_val_every_n_epoch=4,deterministic=True, gradient_clip_val=0.5,logger=tb_logger)\n",
    "# trainer.fit(model,dm) # traning and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout,seq_len=seq_len)\n",
    "# trainer = Trainer(precision=16,gpus=1,max_epochs=400,check_val_every_n_epoch=4,deterministic=True, gradient_clip_val=0.5,logger=tb_logger,progress_bar_refresh_rate=50,auto_lr_find=0.002)\n",
    "# trainer.tune(model,dm) # finding the lr : first way\n",
    "# 2nd way\n",
    "# lr_finder = trainer.tuner.lr_find(model)\n",
    "# print(lr_finder.results)\n",
    "# fig = lr_finder.plot(suggest=True) # Plot with\n",
    "# fig.show()\n",
    "# new_lr = lr_finder.suggestion() # Pick point based on plot, or get suggestion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout,seq_len=seq_len)\n",
    "# trainer = Trainer(precision=16,gpus=1,max_epochs=1,check_val_every_n_epoch=4,deterministic=True, gradient_clip_val=0.5,logger=tb_logger,progress_bar_refresh_rate=10)\n",
    "# trainer.fit(model,dm) # traning and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test(datamodule=dm) # testing\n",
    "# # %%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}