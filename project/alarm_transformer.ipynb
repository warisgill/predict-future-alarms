{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('dl': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8076afad8c2f739e22f417bad77704dbad7b0389c4d6903b1ae4a1b7479f7ed3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import io\n",
    "import torchtext\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "30551lines [00:00, 53066.75lines/s]\n",
      "['A17 A17 A17 A75 A17 A57 A17 A17 A17 A98 A99 A56\\n', 'A245 A246 A50 A243\\n', 'A50 A59 A60 A64 A392 A726 A726 A726 A9 A725 A726 A726 A725 A725 A725 A243 A725\\n', 'A746 A17 A266 A563 A204 A613 A367 A1094\\n']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class AlarmDataset(Dataset):\n",
    "    def __init__(self,data,seq_len,batch_size):\n",
    "        self.length = len(data)//seq_len # how much data i have         \n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "       \n",
    "    def __getitem__(self, index: int):\n",
    "        x = self.data[index*self.seq_len:(index*self.seq_len)+seq_len]\n",
    "        y = self.data[1+index*self.seq_len:1+(index*self.seq_len)+seq_len]\n",
    "        return x,y\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, data_path:str, batch_size:int, seq_len:int):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.vocab = build_vocab_from_iterator(map(self.tokenizer,iter(io.open(data_path,encoding=\"utf8\"))))\n",
    "                \n",
    "        # url = data_path\n",
    "        # test_filepath, valid_filepath, train_filepath = extract_archive(download_from_url(url))\n",
    "        seqs = None\n",
    "        with open(data_path) as f:\n",
    "            seqs = f.readlines()\n",
    "        print(seqs[:4])\n",
    "        train, valid = train_test_split(seqs,test_size=0.30,shuffle=False)\n",
    "        valid, test = train_test_split(valid,test_size=0.30, shuffle=False)\n",
    "\n",
    "        \n",
    "\n",
    "        with open(\"../.data/train.tokens\",\"w\") as f:\n",
    "            for seq in train:\n",
    "                f.write(seq)\n",
    "        \n",
    "        with open(\"../.data/val.tokens\",\"w\") as f:\n",
    "            for seq in valid:\n",
    "                f.write(seq)\n",
    "            \n",
    "        with open(\"../.data/test.tokens\",\"w\") as f:\n",
    "            for seq in test:\n",
    "                f.write(seq)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        train_data = self.data_process(iter(io.open(\"../.data/train.tokens\", encoding=\"utf8\")))\n",
    "        val_data = self.data_process(iter(io.open(\"../.data/val.tokens\", encoding=\"utf8\")))\n",
    "        test_data = self.data_process(iter(io.open(\"../.data/test.tokens\", encoding=\"utf8\")))\n",
    "\n",
    "    \n",
    "        self.train_dataset = AlarmDataset(train_data, seq_len,self.batch_size)\n",
    "        self.valid_dataset = AlarmDataset(val_data,seq_len,self.batch_size)\n",
    "        self.test_dataset = AlarmDataset(test_data, seq_len,self.batch_size)\n",
    "    \n",
    "    def data_process(self, raw_text_iter):\n",
    "        data = [torch.tensor([self.vocab[token] for token in self.tokenizer(item)],dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "\n",
    "    def setup(self, stage: None):\n",
    "        return None\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.valid_dataset, batch_size=self.batch_size, shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "\n",
    "file_path = '../.data/seqs.tokens'\n",
    "\n",
    "bsize = 20\n",
    "seq_len = 35\n",
    "dm = MyDataModule(file_path,bsize,seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5, seq_len=None, lr=0.0013):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.lr = lr\n",
    "        self.ntoken = ntoken\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = torch.nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = torch.nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = torch.nn.Linear(ninp, ntoken)\n",
    "        self.src_mask = self.generate_square_subsequent_mask(seq_len)\n",
    "        self.seq_len = seq_len \n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src_mask = src_mask.to(self.device)\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        src_mask = src_mask.to(self.device)\n",
    "      \n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # , weight_decay=0.0000001\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        x = x.T\n",
    "        y = y.T.reshape(-1)\n",
    "\n",
    "        # print(\"Training Shape: \", x.size(),y.size())\n",
    "        \n",
    "        if x.size(0) != self.seq_len:\n",
    "           self.src_mask =  self.generate_square_subsequent_mask(x.size(0))\n",
    "        \n",
    "        y_hat = self(x,self.src_mask)\n",
    "\n",
    "        loss = F.cross_entropy(y_hat.view(-1, self.ntoken),y)\n",
    "        self.log('train_loss', loss,on_step=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train_ppl\",math.exp(loss.item()),on_step=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch, batch_idx):\n",
    "        x,y = batch\n",
    "        x = x.T\n",
    "        y = y.T.reshape(-1)\n",
    "\n",
    "        # print(\"Validation Shape: \", x.size(),y.size())\n",
    "\n",
    "        if x.size(0) != self.seq_len:\n",
    "           self.src_mask =  self.generate_square_subsequent_mask(x.size(0))\n",
    "        \n",
    "        y_hat = self(x,self.src_mask)\n",
    "        # print(\"> y-hat\",y_hat.size())\n",
    "        loss = F.cross_entropy(y_hat.view(-1, self.ntoken),y)\n",
    "        self.log('val_loss', loss, on_step=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_ppl\",math.exp(loss.item()),on_step=True, prog_bar=True, logger=True)\n",
    "        return {'val_loss':loss}\n",
    "    \n",
    "    def test_step(self,batch, batch_idx):\n",
    "        x,y = batch\n",
    "        x = x.T\n",
    "        y = y.T.reshape(-1)\n",
    "\n",
    "        # print(\"Validation Shape: \", x.size(),y.size())\n",
    "\n",
    "        if x.size(0) != self.seq_len:\n",
    "           self.src_mask =  self.generate_square_subsequent_mask(x.size(0))\n",
    "        \n",
    "        y_hat = self(x,self.src_mask)\n",
    "        # print(\"> y-hat\",y_hat.size())\n",
    "        loss = F.cross_entropy(y_hat.view(-1, self.ntoken),y)\n",
    "        self.log('test_loss', loss, on_step=True, prog_bar=True, logger=True)\n",
    "        self.log(\"test_ppl\",math.exp(loss.item()),on_step=True, prog_bar=True, logger=True)\n",
    "        return {'test_loss':loss}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['loss']  for d in outputs]).mean()\n",
    "        print(f\"> Avg Training loss = {avg_loss}\")\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # print(outputs)\n",
    "        avg_loss = torch.stack([d['val_loss'] for d in outputs]).mean()\n",
    "        print(f\"> Average Valid Loss = {avg_loss}\")\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['test_loss'] for d in outputs]).mean()\n",
    "        print(f\"> Average Test Loss = {avg_loss}\")\n",
    "    \n",
    "    \n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../.data/seqs.tokens'\n",
    "tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
    "\n",
    "bsize = 2048\n",
    "seq_len = 90\n",
    "dm = MyDataModule(file_path,bsize,seq_len)\n",
    "\n",
    "ntokens = len(dm.vocab.stoi) # the size of vocabulary\n",
    "emsize = 256 # embedding dimension\n",
    "nhid = 258 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 4 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8 # the number of heads in the multiheadattention models\n",
    "dropout = 0 # the dropout value\n",
    "\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout,seq_len=seq_len)\n",
    "trainer = Trainer(precision=16,gpus=1,max_epochs=400,check_val_every_n_epoch=4,deterministic=True, gradient_clip_val=0.5,logger=tb_logger)\n",
    "trainer.fit(model,dm) # traning and validation"
   ]
  },
  {
   "source": [
    "### Learning Rate Finder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout,seq_len=seq_len)\n",
    "# trainer = Trainer(precision=16,gpus=1,max_epochs=400,check_val_every_n_epoch=4,deterministic=True, gradient_clip_val=0.5,logger=tb_logger,progress_bar_refresh_rate=50,auto_lr_find=0.002)\n",
    "# trainer.tune(model,dm) # finding the lr : first way\n",
    "# 2nd way\n",
    "# lr_finder = trainer.tuner.lr_find(model)\n",
    "# print(lr_finder.results)\n",
    "# fig = lr_finder.plot(suggest=True) # Plot with\n",
    "# fig.show()\n",
    "# new_lr = lr_finder.suggestion() # Pick point based on plot, or get suggestion\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Main trainer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout,seq_len=seq_len)\n",
    "# trainer = Trainer(precision=16,gpus=1,max_epochs=1,check_val_every_n_epoch=4,deterministic=True, gradient_clip_val=0.5,logger=tb_logger,progress_bar_refresh_rate=10)\n",
    "# trainer.fit(model,dm) # traning and validation"
   ]
  },
  {
   "source": [
    "# Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test(datamodule=dm) # testing\n",
    "# # %%"
   ]
  }
 ]
}