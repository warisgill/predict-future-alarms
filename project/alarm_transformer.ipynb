{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for logging \n",
    "import os\n",
    "from comet_ml import OfflineExperiment\n",
    "\n",
    "from pytorch_lightning import metrics\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import io\n",
    "import torchtext\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping # The EarlyStopping callback can be used to monitor a validation metric and stop the training when no improvement is observed.\n",
    "\"\"\"\n",
    "    To enable it:\n",
    "\n",
    "    Import EarlyStopping callback.\n",
    "\n",
    "    Log the metric you want to monitor using log() method.\n",
    "\n",
    "    Init the callback, and set monitor to the logged metric of your choice.\n",
    "\n",
    "    Pass the EarlyStopping callback to the Trainer callbacks flag.\n",
    "\"\"\"\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "# seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class AlarmDataset(Dataset):\n",
    "    def __init__(self,data,seq_len,batch_size):\n",
    "        self.length = len(data)//seq_len # how much data i have         \n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "       \n",
    "    def __getitem__(self, index: int):\n",
    "        x = self.data[index*self.seq_len:(index*self.seq_len)+seq_len]\n",
    "        y = self.data[1+index*self.seq_len:1+(index*self.seq_len)+seq_len]\n",
    "        return x,y\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, dir_path:str, file_name:str, batch_size:int=64, seq_len:int=8, filter_seq=350):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        # self.data_path = data_path\n",
    "\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.vocab = build_vocab_from_iterator(map(self.tokenizer,iter(io.open(dir_path+file_name,encoding=\"utf8\"))))\n",
    "                \n",
    "        # url = data_path\n",
    "        # test_filepath, valid_filepath, train_filepath = extract_archive(download_from_url(url))\n",
    "        seqs = None\n",
    "        with open(dir_path+file_name) as f:\n",
    "            seqs = f.readlines()\n",
    "        seqs = [seq for seq in seqs if len(seq.split())<=filter_seq]\n",
    "\n",
    "        print(f\"total seqs= {len(seqs)}\")\n",
    "        print(seqs[:4])\n",
    "        train, valid = train_test_split(seqs,test_size=0.30,shuffle=False)\n",
    "        valid, test = train_test_split(valid,test_size=0.30, shuffle=False)\n",
    "\n",
    "        with open(dir_path +\"train.tokens\",\"w\") as f:\n",
    "            for seq in train:\n",
    "                f.write(seq)\n",
    "        \n",
    "        with open(dir_path +\"val.tokens\",\"w\") as f:\n",
    "            for seq in valid:\n",
    "                f.write(seq)\n",
    "            \n",
    "        with open(dir_path +\"test.tokens\",\"w\") as f:\n",
    "            for seq in test:\n",
    "                f.write(seq)\n",
    "\n",
    "        train_data = self.data_process(iter(io.open(dir_path +\"train.tokens\", encoding=\"utf8\")))\n",
    "        val_data = self.data_process(iter(io.open(dir_path +\"val.tokens\", encoding=\"utf8\")))\n",
    "        test_data = self.data_process(iter(io.open(dir_path +\"test.tokens\", encoding=\"utf8\")))\n",
    "\n",
    "    \n",
    "        self.train_dataset = AlarmDataset(train_data, seq_len,self.batch_size)\n",
    "        self.valid_dataset = AlarmDataset(val_data,seq_len,self.batch_size)\n",
    "        self.test_dataset = AlarmDataset(test_data, seq_len,self.batch_size)\n",
    "\n",
    "    \n",
    "    def data_process(self, raw_text_iter):\n",
    "        data = [torch.tensor([self.vocab[token] for token in self.tokenizer(item)],dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "            Use this method to do things that might write to disk or that need to be done only from a single GPU in distributed settings.\n",
    "            e.g., download,tokenize,etc…\n",
    "        \"\"\" \n",
    "        return None\n",
    "\n",
    "\n",
    "    def setup(self, stage: None):\n",
    "        \"\"\"\n",
    "            There are also data operations you might want to perform on every GPU. Use setup to do things like:\n",
    "            count number of classes,build vocabulary,perform train/val/test splits,apply transforms (defined explicitly in your datamodule or assigned in init),etc…\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.valid_dataset, batch_size=self.batch_size, shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0, seq_len=None, lr=0.0013,weight_decay=0.0):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.ntoken = ntoken\n",
    "        self.seq_len = seq_len \n",
    "        self.ninp = ninp\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = torch.nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = torch.nn.Embedding(ntoken, ninp)\n",
    "        self.decoder = torch.nn.Linear(ninp, ntoken)\n",
    "        self.src_mask = self.generate_square_subsequent_mask(seq_len)\n",
    "        self.init_weights()\n",
    "\n",
    "        self.train_F1 = metrics.classification.F1(num_classes=self.ntoken)\n",
    "        self.val_F1 = metrics.classification.F1(num_classes=self.ntoken)\n",
    "        self.test_F1 = metrics.classification.F1(num_classes=self.ntoken)\n",
    "\n",
    "\n",
    "        self.log(\"seq_len\",self.seq_len)\n",
    "        self.log(\"lr\",lr)\n",
    "        self.log(\"# of tokens {unique alarms}\",self.ntoken)\n",
    "        self.log(\"weight_decay\",self.weight_decay)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src_mask = src_mask.to(self.device)\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        src_mask = src_mask.to(self.device)\n",
    "      \n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "   # The ReduceLROnPlateau scheduler requires a monitor\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr,weight_decay=self.weight_decay)\n",
    "        d = {\n",
    "       'optimizer': optimizer,\n",
    "       'lr_scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = \"min\", factor = 0.5, patience=8, verbose=True),\n",
    "       'monitor': 'val_epoch_loss',\n",
    "        'interval': 'epoch'\n",
    "        }\n",
    "        return d    \n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        x = x.T\n",
    "        y = y.T.reshape(-1)\n",
    "\n",
    "        if x.size(0) != self.seq_len:\n",
    "           self.src_mask =  self.generate_square_subsequent_mask(x.size(0))\n",
    "        \n",
    "        y_hat = self(x,self.src_mask)\n",
    "        y_hat =  y_hat.view(-1, self.ntoken)\n",
    "        loss = F.cross_entropy(y_hat,y) # cross entropy itself compute softmax \n",
    "        \n",
    "        self.log('train_loss',loss,logger=True)\n",
    "        self.log('train_F1',self.train_F1(F.softmax(y_hat),y),logger=True, on_step=True,prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch, batch_idx):\n",
    "        x,y = batch\n",
    "        x = x.T\n",
    "        y = y.T.reshape(-1)\n",
    "        \n",
    "        if x.size(0) != self.seq_len:\n",
    "           self.src_mask =  self.generate_square_subsequent_mask(x.size(0))\n",
    "        \n",
    "        y_hat = self(x,self.src_mask)\n",
    "        y_hat =  y_hat.view(-1, self.ntoken)\n",
    "        loss = F.cross_entropy(y_hat,y)\n",
    "\n",
    "        self.log('val_loss',loss,logger=True)\n",
    "        self.log('val_F1',self.val_F1(F.softmax(y_hat) ,y),logger=True, on_step=True,prog_bar=True)\n",
    "        return {'val_loss':loss}\n",
    "    \n",
    "    def test_step(self,batch, batch_idx):\n",
    "        x,y = batch\n",
    "        x = x.T\n",
    "        y = y.T.reshape(-1)\n",
    "        if x.size(0) != self.seq_len:\n",
    "           self.src_mask =  self.generate_square_subsequent_mask(x.size(0))\n",
    "\n",
    "        y_hat = self(x,self.src_mask)\n",
    "        y_hat =  y_hat.view(-1, self.ntoken)\n",
    "        loss = F.cross_entropy(y_hat,y)\n",
    "\n",
    "        self.test_F1(F.softmax(y_hat) ,y)\n",
    "        \n",
    "        self.log('test_loss',loss,logger=True)\n",
    "        self.log('test_F1', self.test_F1(F.softmax(y_hat) ,y),logger=True)\n",
    "        return {'test_loss':loss}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['loss']  for d in outputs]).mean()\n",
    "        f1 = self.train_F1.compute()\n",
    "        print(f\">Epoch ={self.current_epoch}, Avg Training loss = {avg_loss}, F1 = {f1}\")\n",
    "        self.log(\"train_epoch_loss\",avg_loss,logger=True,prog_bar=True)\n",
    "        self.log(\"train_epoch_F1\", f1, logger=True,prog_bar=True)\n",
    "  \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['val_loss'] for d in outputs]).mean()\n",
    "        f1 = self.val_F1.compute()\n",
    "        print(f\">== Average Valid Loss = {avg_loss}, F1 = {f1}\")\n",
    "        self.log(\"val_epoch_loss\",avg_loss,logger=True)\n",
    "        self.log(\"val_epoch_F1\",f1,logger=True,prog_bar=True)\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['test_loss'] for d in outputs]).mean()\n",
    "        f1 = self.test_F1.compute()\n",
    "        print(f\">Average Test Loss = {avg_loss}, f1= {f1}\")\n",
    "        self.log(\"test_epoch_loss\",avg_loss, logger = True)\n",
    "        self.log(\"test_epoch_F1\",f1, logger=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "30825lines [00:01, 15493.72lines/s]\n",
      "total seqs= 27975\n",
      "['A17 A75 A17 A57 A17 A99 A98 A56\\n', 'A245 A246 A50 A243\\n', 'A243 A9 A559 A1025\\n', 'A50 A59 A60 A64 A392 A726 A9 A725 A726 A725 A243 A725\\n']\n",
      "> Vocab Size (Number of Unique Alarms): 960\n",
      "> Customised lr = 0.0001\n"
     ]
    }
   ],
   "source": [
    "dir_name = \"../.data/\"\n",
    "fname = 'seqs.tokens'\n",
    "# tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
    "\n",
    "bsize = 64\n",
    "seq_len = 128\n",
    "filter_seq = 350\n",
    "\n",
    "dm = MyDataModule(dir_path=dir_name,file_name=fname,batch_size=bsize,seq_len=seq_len,filter_seq=filter_seq)\n",
    "ntokens = len(dm.vocab.stoi) # the size of vocabulary\n",
    "print(f\"> Vocab Size (Number of Unique Alarms): {ntokens}\")\n",
    "\n",
    "emsize = 256 # embedding dimension\n",
    "nhid = 256 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers =2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "\n",
    "dropout = 0.0 # the dropout value\n",
    "weight_decay = 0.000\n",
    "lr = 0.0001\n",
    "\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout,seq_len=seq_len,lr=lr,weight_decay=weight_decay)\n",
    "print(f\"> Customised lr = {model.lr}\")"
   ]
  },
  {
   "source": [
    "# Trainer\n",
    "\n",
    "**Note: When monitoring any parameter after the validation epoch end then you should pass check_val_every_n_epoch=1  not to other. This is very important.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n",
      "COMET INFO: No Comet API Key was found, creating an OfflineExperiment. Set up your API Key to get the full Comet experience https://www.comet.ml/docs/python-sdk/advanced/#python-configuration\n",
      "2021/01/16 06:26:39 INFO mlflow.utils.autologging_utils: pytorch autologging will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pytorch workflow to the MLflow run with ID 'fdb13fd9cda94cc78af9a9c831c1d1be'\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | pos_encoder         | PositionalEncoding | 0     \n",
      "1 | transformer_encoder | TransformerEncoder | 791 K \n",
      "2 | encoder             | Embedding          | 245 K \n",
      "3 | decoder             | Linear             | 246 K \n",
      "4 | train_F1            | F1                 | 0     \n",
      "5 | val_F1              | F1                 | 0     \n",
      "6 | test_F1             | F1                 | 0     \n",
      "-----------------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "INFO:lightning:\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | pos_encoder         | PositionalEncoding | 0     \n",
      "1 | transformer_encoder | TransformerEncoder | 791 K \n",
      "2 | encoder             | Embedding          | 245 K \n",
      "3 | decoder             | Linear             | 246 K \n",
      "4 | train_F1            | F1                 | 0     \n",
      "5 | val_F1              | F1                 | 0     \n",
      "6 | test_F1             | F1                 | 0     \n",
      "-----------------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96dddaa149c24cd187f89a2ba200b3d3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">== Average Valid Loss = 7.471125602722168, F1 = 6.103515625e-05\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9bb4fbfda32444439ddd88c8d8849c9f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28fd7c70a1b84ff097f48ad1ef4b2bfc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">== Average Valid Loss = 0.9445329308509827, F1 = 0.815323531627655\n>Epoch =0, Avg Training loss = 1.00809645652771, F1 = 0.8267511129379272\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0c92c1bce624305bbe6c7ef72c9f7ca"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">== Average Valid Loss = 0.9541664123535156, F1 = 0.8113608956336975\n>Epoch =1, Avg Training loss = 0.6413501501083374, F1 = 0.852878212928772\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30785bcc0a53494dafc87fe5e51a73c0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">== Average Valid Loss = 0.9257034659385681, F1 = 0.8092100620269775\n>Epoch =2, Avg Training loss = 0.5997321009635925, F1 = 0.854438304901123\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a4bd38b3aa24f0882b5726044036f9c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">== Average Valid Loss = 0.9060231447219849, F1 = 0.8113204836845398\n>Epoch =3, Avg Training loss = 0.5760810375213623, F1 = 0.8577659130096436\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79e949c47eca4270b763f612d9992ea9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">== Average Valid Loss = 0.8975859880447388, F1 = 0.8075355887413025\n>Epoch =4, Avg Training loss = 0.5647791624069214, F1 = 0.8590824007987976\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e96a90a9aaf6416c9458f4633a65c21e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">== Average Valid Loss = 0.9236698150634766, F1 = 0.7823100686073303\n>Epoch =5, Avg Training loss = 0.5564301609992981, F1 = 0.860651433467865\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4141964574d24748af9f283c36b3c5c9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">== Average Valid Loss = 0.8929443359375, F1 = 0.7981109619140625\n>Epoch =6, Avg Training loss = 0.5451700091362, F1 = 0.8629342317581177\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8ab27e4ebc04b5d8237512bd49f8b28"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">== Average Valid Loss = 0.9004809260368347, F1 = 0.7930722236633301\n>Epoch =7, Avg Training loss = 0.5399174094200134, F1 = 0.8641495704650879\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0bb6505ef8f945b78947d43f3dd99e25"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">== Average Valid Loss = 0.8831403255462646, F1 = 0.8035324811935425\n>Epoch =8, Avg Training loss = 0.5312461853027344, F1 = 0.8664150834083557\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64e9217dc09f4152ba362ebf8bbe6cda"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">== Average Valid Loss = 0.8726280927658081, F1 = 0.8082341551780701\n>Epoch =9, Avg Training loss = 0.527281641960144, F1 = 0.8678411841392517\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19a05490993c49c2b0ec0c69c9292798"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">== Average Valid Loss = 0.863749623298645, F1 = 0.8139999508857727\n>Epoch =10, Avg Training loss = 0.5215423703193665, F1 = 0.8686859011650085\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3c2603727154a18985cac3da5fbc13e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">== Average Valid Loss = 0.8608298301696777, F1 = 0.8181198835372925\n>Epoch =11, Avg Training loss = 0.517568826675415, F1 = 0.8698300123214722\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bfca9a97b62b45368d5c8dcb395f80dc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">== Average Valid Loss = 0.8736092448234558, F1 = 0.806412398815155\n>Epoch =12, Avg Training loss = 0.5128885507583618, F1 = 0.8701900243759155\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b34f1321c39f494aa0b4e994afa8803b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">== Average Valid Loss = 0.8557664752006531, F1 = 0.8194440603256226\n>Epoch =13, Avg Training loss = 0.5097476840019226, F1 = 0.8712642788887024\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef90a5b29afc49e69241c9e0616946b3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">== Average Valid Loss = 0.9176872968673706, F1 = 0.7712515592575073\n>Epoch =14, Avg Training loss = 0.5052396655082703, F1 = 0.8720402717590332\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e4e364992644a7985c3019b5cd05269"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">== Average Valid Loss = 0.9014582633972168, F1 = 0.7768309116363525\n",
      ">Epoch =15, Avg Training loss = 0.503762423992157, F1 = 0.8728628158569336\n",
      "2021/01/16 06:30:21 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: A GradScaler instance may only be pickled at the beginning of an iteration, or at the end after scaler.update().\n",
      "COMET INFO: ----------------------------------\n",
      "COMET INFO: Comet.ml OfflineExperiment Summary\n",
      "COMET INFO: ----------------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : [OfflineExperiment will get URL after upload]\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     loss [761] : (0.15430860221385956, 7.46098518371582)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Created from       : MLFlow auto-logger\n",
      "COMET INFO:     Mode               : training\n",
      "COMET INFO:     offline_experiment : True\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     amsgrad        : 1\n",
      "COMET INFO:     betas          : (0.9, 0.999)\n",
      "COMET INFO:     epochs         : 20\n",
      "COMET INFO:     eps            : 1e-08\n",
      "COMET INFO:     lr             : 0.0001\n",
      "COMET INFO:     min_delta      : 1\n",
      "COMET INFO:     mode           : min\n",
      "COMET INFO:     monitor        : val_epoch_loss\n",
      "COMET INFO:     optimizer_name : AdamW\n",
      "COMET INFO:     patience       : 20\n",
      "COMET INFO:     stopped_epoch  : 1\n",
      "COMET INFO:     weight_decay   : 1\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     asset                    : 1\n",
      "COMET INFO:     code                     : 1 (12 KB)\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (51 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO: ----------------------------------\n",
      "\n",
      "COMET INFO: Saving offline stats to disk before program termination (may take several seconds)\n",
      "COMET INFO: Starting saving the offline archive\n",
      "COMET INFO: To upload this offline experiment, run:\n",
      "    comet upload /tmp/tmpidzbnins/82b8eb6e2ff940ac94131ca9a9ab3458.zip\n",
      "run_id: fdb13fd9cda94cc78af9a9c831c1d1be\n",
      "artifacts: []\n",
      "params: {'weight_decay': '0.0', 'lr': '0.0001', 'optimizer_name': 'AdamW', 'min_delta': '-0.0', 'mode': 'min', 'monitor': 'val_epoch_loss', 'stopped_epoch': '0', 'eps': '1e-08', 'amsgrad': 'False', 'patience': '20', 'epochs': '20', 'betas': '(0.9, 0.999)'}\n",
      "metrics: {'train_epoch_loss': 0.5052396655082703, 'val_loss': 0.9014581441879272, 'val_F1': 0.83349609375, 'train_F1': 0.841552734375, 'val_epoch_F1': 0.7768309116363525, 'val_F1_epoch': 0.7768309116363525, 'train_loss': 0.62191241979599, 'train_epoch_F1': 0.8720402717590332, 'val_epoch_loss': 0.9014582633972168}\n",
      "tags: {'Mode': 'training'}\n"
     ]
    }
   ],
   "source": [
    "import mlflow.pytorch\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_epoch_loss',\n",
    "   min_delta=0.00,\n",
    "   patience=20,\n",
    "   verbose=True,\n",
    "   mode='min'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(precision=16,gpus=-1, num_nodes=1,  max_epochs=1200, check_val_every_n_epoch=1,deterministic=True, gradient_clip_val=0.5,enable_pl_optimizer=True,callbacks=[early_stop_callback])\n",
    "# accelerator='dp'\n",
    "progress_bar_refresh_rate=0 # set to zero to disable it\n",
    "\n",
    "\n",
    "def print_auto_logged_info(r):\n",
    "    tags = {k: v for k, v in r.data.tags.items() if not k.startswith(\"mlflow.\")}\n",
    "    artifacts = [f.path for f in MlflowClient().list_artifacts(r.info.run_id, \"model\")]\n",
    "    print(\"run_id: {}\".format(r.info.run_id))\n",
    "    print(\"artifacts: {}\".format(artifacts))\n",
    "    print(\"params: {}\".format(r.data.params))\n",
    "    print(\"metrics: {}\".format(r.data.metrics))\n",
    "    print(\"tags: {}\".format(tags))\n",
    "\n",
    "# Auto log all MLflow entities\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "# Train the model\n",
    "with mlflow.start_run() as run:\n",
    "    trainer.fit(model,dm) # traning and validation\n",
    "\n",
    "# fetch the auto logged parameters and metrics\n",
    "print_auto_logged_info(mlflow.get_run(run_id=run.info.run_id))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(precision=16,gpus=1,max_epochs=1200,check_val_every_n_epoch=4,deterministic=True, gradient_clip_val=0.5,logger=tb_logger)\n",
    "# trainer.fit(model,dm) # traning and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout,seq_len=seq_len)\n",
    "# trainer = Trainer(precision=16,gpus=1,max_epochs=400,check_val_every_n_epoch=4,deterministic=True, gradient_clip_val=0.5,logger=tb_logger,progress_bar_refresh_rate=50,auto_lr_find=0.002)\n",
    "# trainer.tune(model,dm) # finding the lr : first way\n",
    "# 2nd way\n",
    "# lr_finder = trainer.tuner.lr_find(model)\n",
    "# print(lr_finder.results)\n",
    "# fig = lr_finder.plot(suggest=True) # Plot with\n",
    "# fig.show()\n",
    "# new_lr = lr_finder.suggestion() # Pick point based on plot, or get suggestion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout,seq_len=seq_len)\n",
    "# trainer = Trainer(precision=16,gpus=1,max_epochs=1,check_val_every_n_epoch=4,deterministic=True, gradient_clip_val=0.5,logger=tb_logger,progress_bar_refresh_rate=10)\n",
    "# trainer.fit(model,dm) # traning and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021/01/16 06:22:16 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '99b0d1fa7fc3453ba397207d640591de', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pytorch workflow\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06b9895993084bf0acf1d14f754e98ab"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "COMET INFO: No Comet API Key was found, creating an OfflineExperiment. Set up your API Key to get the full Comet experience https://www.comet.ml/docs/python-sdk/advanced/#python-configuration\n",
      "COMET INFO: ----------------------------------\n",
      "COMET INFO: Comet.ml OfflineExperiment Summary\n",
      "COMET INFO: ----------------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : [OfflineExperiment will get URL after upload]\n",
      "COMET INFO:   Metrics [count]:\n",
      "COMET INFO:     test_F1 [2]          : 0.8368929624557495\n",
      "COMET INFO:     test_epoch_F1 [2]    : 0.8368929028511047\n",
      "COMET INFO:     test_epoch_loss [2]  : 0.7977280020713806\n",
      "COMET INFO:     test_loss [2]        : 0.7977280020713806\n",
      "COMET INFO:     train_F1 [2]         : 0.8369140625\n",
      "COMET INFO:     train_epoch_F1 [2]   : 0.8753800392150879\n",
      "COMET INFO:     train_epoch_loss [2] : 0.4936947524547577\n",
      "COMET INFO:     train_loss [2]       : 0.6517497897148132\n",
      "COMET INFO:     val_F1 [2]           : 0.8263682723045349\n",
      "COMET INFO:     val_epoch_F1 [2]     : 0.8263682723045349\n",
      "COMET INFO:     val_epoch_loss [2]   : 0.844127357006073\n",
      "COMET INFO:     val_loss [2]         : 0.8441272974014282\n",
      "COMET INFO:   Others [count]:\n",
      "COMET INFO:     Created from       : MLFlow auto-logger\n",
      "COMET INFO:     Mode [2]           : testing\n",
      "COMET INFO:     offline_experiment : True\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     code                     : 1 (13 KB)\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (66 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO: ----------------------------------\n",
      ">Average Test Loss = 0.7977280020713806, f1= 0.8368929028511047\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_F1': tensor(0.8369, device='cuda:0'),\n",
      " 'test_epoch_F1': tensor(0.8369, device='cuda:0'),\n",
      " 'test_epoch_loss': tensor(0.7977, device='cuda:0'),\n",
      " 'test_loss': tensor(0.7977, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "COMET INFO: Starting saving the offline archive\n",
      "COMET INFO: To upload this offline experiment, run:\n",
      "    comet upload /tmp/tmpig0k_n4b/3bf07f7030cf427f8c3850d2b72fd0c4.zip\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'test_loss': 0.7977280020713806,\n",
       "  'test_F1': 0.8368929624557495,\n",
       "  'test_epoch_loss': 0.7977280020713806,\n",
       "  'test_epoch_F1': 0.8368929028511047}]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "trainer.test(datamodule=dm) # testing\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('dl': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8076afad8c2f739e22f417bad77704dbad7b0389c4d6903b1ae4a1b7479f7ed3"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}