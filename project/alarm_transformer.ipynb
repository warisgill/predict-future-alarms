{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "# # for logging \n",
    "\n",
    "from comet_ml import Experiment\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.loggers import TestTubeLogger\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "# For metrics\n",
    "from pytorch_lightning import metrics\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import io\n",
    "import torchtext\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping # The EarlyStopping callback can be used to monitor a validation metric and stop the training when no improvement is observed.\n",
    "\"\"\"\n",
    "    To enable it:\n",
    "\n",
    "    Import EarlyStopping callback.\n",
    "\n",
    "    Log the metric you want to monitor using log() method.\n",
    "\n",
    "    Init the callback, and set monitor to the logged metric of your choice.\n",
    "\n",
    "    Pass the EarlyStopping callback to the Trainer callbacks flag.\n",
    "\"\"\"\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class AlarmDataset(Dataset):\n",
    "    def __init__(self,data,seq_len,batch_size):\n",
    "        self.length = len(data)//seq_len # how much data i have         \n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "       \n",
    "    def __getitem__(self, index: int):\n",
    "        x = self.data[index*self.seq_len:(index*self.seq_len)+self.seq_len]\n",
    "        y = self.data[1+index*self.seq_len:1+(index*self.seq_len)+self.seq_len]\n",
    "        return x,y\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, dir_path:str, file_name:str, config):\n",
    "        super().__init__()\n",
    "        # self.batch_size = batch_size\n",
    "        # self.data_path = data_path\n",
    "        self.config = config\n",
    "\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.vocab = build_vocab_from_iterator(map(self.tokenizer,iter(io.open(dir_path+file_name,encoding=\"utf8\"))))\n",
    "                \n",
    "        # url = data_path\n",
    "        # test_filepath, valid_filepath, train_filepath = extract_archive(download_from_url(url))\n",
    "        seqs = None\n",
    "        with open(dir_path+file_name) as f:\n",
    "            seqs = f.readlines()\n",
    "        seqs = [seq for seq in seqs if len(seq.split())<=self.config['filter-seq-len']]\n",
    "\n",
    "        print(f\"total seqs= {len(seqs)}\")\n",
    "        print(seqs[:4])\n",
    "        train, valid = train_test_split(seqs,test_size=0.30,shuffle=False)\n",
    "        valid, test = train_test_split(valid,test_size=0.30, shuffle=False)\n",
    "\n",
    "        with open(dir_path +\"train.tokens\",\"w\") as f:\n",
    "            for seq in train:\n",
    "                f.write(seq)\n",
    "        \n",
    "        with open(dir_path +\"val.tokens\",\"w\") as f:\n",
    "            for seq in valid:\n",
    "                f.write(seq)\n",
    "            \n",
    "        with open(dir_path +\"test.tokens\",\"w\") as f:\n",
    "            for seq in test:\n",
    "                f.write(seq)\n",
    "\n",
    "        train_data = self.data_process(iter(io.open(dir_path +\"train.tokens\", encoding=\"utf8\")))\n",
    "        val_data = self.data_process(iter(io.open(dir_path +\"val.tokens\", encoding=\"utf8\")))\n",
    "        test_data = self.data_process(iter(io.open(dir_path +\"test.tokens\", encoding=\"utf8\")))\n",
    "\n",
    "    \n",
    "        self.train_dataset = AlarmDataset(train_data, self.config['seq-len'], self.config['batch-size'])\n",
    "        self.valid_dataset = AlarmDataset(val_data,self.config['seq-len'], self.config['batch-size'])\n",
    "        self.test_dataset = AlarmDataset(test_data, self.config['seq-len'], self.config['batch-size'])\n",
    "\n",
    "    \n",
    "    def data_process(self, raw_text_iter):\n",
    "        data = [torch.tensor([self.vocab[token] for token in self.tokenizer(item)],dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "            Use this method to do things that might write to disk or that need to be done only from a single GPU in distributed settings.\n",
    "            e.g., download,tokenize,etc…\n",
    "        \"\"\" \n",
    "        return None\n",
    "\n",
    "\n",
    "    def setup(self, stage: None):\n",
    "        \"\"\"\n",
    "            There are also data operations you might want to perform on every GPU. Use setup to do things like:\n",
    "            count number of classes,build vocabulary,perform train/val/test splits,apply transforms (defined explicitly in your datamodule or assigned in init),etc…\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.config['batch-size'], shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.valid_dataset, batch_size=self.config['batch-size'], shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.config['batch-size'], shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.config = config        \n",
    "        self.lr = self.config[\"lr\"]\n",
    "        self.weight_decay = self.config[\"weight-decay\"]\n",
    "    \n",
    "        self.pos_encoder = PositionalEncoding(self.config['em-size'], self.config['dropout'])\n",
    "        encoder_layers = torch.nn.TransformerEncoderLayer(self.config['em-size'], self.config['nhead'], self.config['nhid'], self.config[\"dropout\"])\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layers, self.config['nlayers'])\n",
    "        self.encoder = torch.nn.Embedding(self.config[\"vocab-size\"], self.config['em-size'])\n",
    "        self.decoder = torch.nn.Linear(self.config['em-size'], self.config[\"vocab-size\"])\n",
    "        self.src_mask = self.generate_square_subsequent_mask(self.config['seq-len'])\n",
    "        self.init_weights()\n",
    "\n",
    "        self.train_F1 = metrics.classification.F1(num_classes=self.config[\"vocab-size\"])\n",
    "        self.val_F1 = metrics.classification.F1(num_classes=self.config[\"vocab-size\"])\n",
    "        self.test_F1 = metrics.classification.F1(num_classes=self.config[\"vocab-size\"])\n",
    "\n",
    "\n",
    "        self.log(\"Sequence length\",self.config['seq-len'])\n",
    "        self.log(\"lr\",self.lr)\n",
    "        self.log(\"# of tokens/vocab_size (unique alarms)\",self.config['vocab-size'])\n",
    "        self.log(\"weight_decay\",self.weight_decay)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src_mask = src_mask.to(self.device)\n",
    "        src = self.encoder(src) * math.sqrt(self.config['em-size'])\n",
    "        src = self.pos_encoder(src)\n",
    "        src_mask = src_mask.to(self.device)\n",
    "      \n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "   # The ReduceLROnPlateau scheduler requires a monitor\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr,weight_decay=self.weight_decay)\n",
    "        d = {\n",
    "       'optimizer': optimizer,\n",
    "       'lr_scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = \"min\", factor = 0.5, patience=4, verbose=True),\n",
    "       'monitor': 'val_epoch_loss',\n",
    "        'interval': 'epoch'\n",
    "        }\n",
    "        return d    \n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        x = x.T\n",
    "        y = y.T.reshape(-1)\n",
    "\n",
    "        if x.size(0) != self.config['seq-len']:\n",
    "           self.src_mask =  self.generate_square_subsequent_mask(x.size(0))\n",
    "        \n",
    "        y_hat = self(x,self.src_mask)\n",
    "        y_hat =  y_hat.view(-1, self.config['vocab-size'])\n",
    "        loss = F.cross_entropy(y_hat,y) # cross entropy itself compute softmax \n",
    "        \n",
    "        self.log('train_loss',loss,logger=True)\n",
    "        self.log('train_F1',self.train_F1(F.softmax(y_hat),y),logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch, batch_idx):\n",
    "        x,y = batch\n",
    "        x = x.T\n",
    "        y = y.T.reshape(-1)\n",
    "        \n",
    "        if x.size(0) != self.config['seq-len']:\n",
    "           self.src_mask =  self.generate_square_subsequent_mask(x.size(0))\n",
    "        \n",
    "        y_hat = self(x,self.src_mask)\n",
    "        y_hat =  y_hat.view(-1, self.config['vocab-size'])\n",
    "        loss = F.cross_entropy(y_hat,y)\n",
    "\n",
    "        self.log('val_loss',loss,logger=True)\n",
    "        self.log('val_F1',self.val_F1(F.softmax(y_hat) ,y),logger=True)\n",
    "        return {'val_loss':loss}\n",
    "    \n",
    "    def test_step(self,batch, batch_idx):\n",
    "        x,y = batch\n",
    "        x = x.T\n",
    "        y = y.T.reshape(-1)\n",
    "        if x.size(0) != self.config['seq-len']:\n",
    "           self.src_mask =  self.generate_square_subsequent_mask(x.size(0))\n",
    "\n",
    "        y_hat = self(x,self.src_mask)\n",
    "        y_hat =  y_hat.view(-1,  self.config['vocab-size'])\n",
    "        loss = F.cross_entropy(y_hat,y)\n",
    "\n",
    "        self.test_F1(F.softmax(y_hat) ,y)\n",
    "        \n",
    "        self.log('test_loss',loss,logger=True)\n",
    "        self.log('test_F1', self.test_F1(F.softmax(y_hat) ,y),logger=True)\n",
    "        return {'test_loss':loss}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['loss']  for d in outputs]).mean()\n",
    "        f1 = self.train_F1.compute()\n",
    "        print(f\">Epoch ={self.current_epoch}, Avg Training loss = {avg_loss}, F1 = {f1}\")\n",
    "        self.log(\"train_epoch_loss\",avg_loss,logger=True,prog_bar=True)\n",
    "        self.log(\"train_epoch_F1\", f1, logger=True,prog_bar=True)\n",
    "  \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['val_loss'] for d in outputs]).mean()\n",
    "        f1 = self.val_F1.compute()\n",
    "        print(f\">== Average Valid Loss = {avg_loss}, F1 = {f1}\")\n",
    "        self.log(\"val_epoch_loss\",avg_loss,logger=True)\n",
    "        self.log(\"val_epoch_F1\",f1,logger=True,prog_bar=True)\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['test_loss'] for d in outputs]).mean()\n",
    "        f1 = self.test_F1.compute()\n",
    "        print(f\">Average Test Loss = {avg_loss}, f1= {f1}\")\n",
    "        self.log(\"test_epoch_loss\",avg_loss, logger = True)\n",
    "        self.log(\"test_epoch_F1\",f1, logger=True)\n",
    "        self.save_hyperparameters()\n",
    "    "
   ]
  },
  {
   "source": [
    "# Trainning\n",
    "\n",
    "**Note: When monitoring any parameter after the validation epoch end then you should pass check_val_every_n_epoch=1  not to other. This is very important.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Loggers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(project=\"Alarm-Transformers-Net\")\n",
    "# comet_logger = CometLogger(\n",
    "#     api_key=\"YZWScOiWdE8FwQSUj725dRmor\",\n",
    "#     project_name=\"Alarm-Transformers-Net\" # Optional\n",
    "# )\n",
    "# test_tube_logger = TestTubeLogger('tb_logs', name='Alarm-Transformers-Net')"
   ]
  },
  {
   "source": [
    "# Hyperparameters Configuration\n",
    "## Hyperparameter optimization with W&B sweeps\n",
    "\n",
    "So far, we chose our learning rate and number of layers arbitrarily. We can automate hyperparameter search with [W&B sweeps](https://docs.wandb.com/sweeps).\n",
    "\n",
    "## Defining sweep configuration\n",
    "\n",
    "Sweeps can be defined in multiple ways:\n",
    "* with a YAML file - best for distributed sweeps and runs from command line\n",
    "* with a Python object - best for notebooks\n",
    "\n",
    "\n",
    "The main items to be defined in a sweep are:\n",
    "*   **Metric** – This is the metric the sweeps are attempting to optimize. Metrics can take a `name` (this metric should be logged by your training script) and a `goal` (maximize or minimize). \n",
    "*   **Search Strategy** – We support several different search strategies with sweeps – `grid`, `random`, `bayes`.\n",
    "*   **Stopping Criteria** – The strategy for determining when to kill off poorly peforming runs, and try more combinations faster. We offer several custom scheduling algorithms like [HyperBand](https://arxiv.org/pdf/1603.06560.pdf) and Envelope.\n",
    "*   **Parameters** – A dictionary containing the hyperparameter names, and discreet values, max and min values or distributions from which to pull their values to sweep over.\n",
    "\n",
    "*Note: you can also use other search libraries such as [Ray-Tune](https://docs.wandb.com/sweeps/ray-tune) or even [create your own search algorithm](https://docs.wandb.com/sweeps/python-api#run-a-local-controller)*\n",
    "\n",
    "Here we just use a Python object. Refer to the [sweep configuration documentation](https://docs.wandb.com/sweeps/configuration) if you want to define more parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "  \"method\": \"grid\",   # bayes search\n",
    "  \"metric\": {           # We want to minimize validation loss\n",
    "      \"name\": \"val_epoch_F1\",\n",
    "      \"goal\": \"maximize\"\n",
    "  },\n",
    "  \"parameters\": {\n",
    "        \"batch-size\": {\n",
    "            # Choose from pre-defined values\n",
    "            \"values\": [16]\n",
    "        },\n",
    "        \"seq-len\": {\n",
    "            # Choose from pre-defined values\n",
    "            \"values\": [700,600,400]\n",
    "        },\n",
    "        \"lr\": {\n",
    "            \"values\":[0.0001,0.0002,0.0003]\n",
    "            # log uniform distribution between exp(min) and exp(max)\n",
    "            # \"distribution\": \"log_uniform\",\n",
    "            # \"min\": -9.21,   # exp(-9.21) = 1e-4\n",
    "            # \"max\": -4.61    # exp(-4.61) = 1e-2\n",
    "        },\n",
    "        \"dropout\":{\n",
    "            \"values\": [0.1,0.15]\n",
    "        },\n",
    "        \"weight-decay\":{\n",
    "            \"values\":[0.0003,0.0002,0.00001]\n",
    "        },\n",
    "        \"em-size\":{\n",
    "            \"values\":[256,128,64]\n",
    "        },\n",
    "        \"nhid\":{\n",
    "            \"values\":[128,64]\n",
    "        },\n",
    "        \"nlayers\":{\n",
    "            \"values\":[4,3,2]\n",
    "        },\n",
    "        \"nhead\":{\n",
    "            \"values\":[8,4,2]\n",
    "        }\n",
    "\n",
    "    }\n",
    "}"
   ]
  },
  {
   "source": [
    "## Running an agent\n",
    "\n",
    "We now just need to run an agent against the sweep configuration. Sweeps can be run in multiple ways:\n",
    "* through a command line with `wandb sweep my_sweep_config.yaml`\n",
    "* directly within a script/notebook with the [Python sweep API](https://docs.wandb.com/sweeps/python-api)\n",
    "\n",
    "To use the Python sweep API, we define a function that the agent will run, refering to sweep parameters with `wandb.config.my_param`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sweep_iteration():\n",
    "    # set up W&B logger\n",
    "    wandb.init()    # required to have access to `wandb.config`\n",
    "    wandb_logger = WandbLogger()\n",
    "\n",
    "    # setup data\n",
    "    config_data = {\n",
    "    'batch-size' :wandb.config[\"batch-size\"], # Batch Size \n",
    "    'seq-len' : wandb.config['seq-len'], # Sequence length\n",
    "    'filter-seq-len' :350 # remove sequence whose size is greater than this len\n",
    "    }\n",
    "\n",
    "    dm = MyDataModule(dir_path=dir_name,file_name=fname,config=config_data)\n",
    "\n",
    "\n",
    "    config_model = {\n",
    "        'lr' : wandb.config['lr'],\n",
    "        'dropout' : wandb.config['dropout'],\n",
    "        'weight-decay': wandb.config['weight-decay'],\n",
    "        'em-size' :wandb.config['em-size'], # embedding dimension \n",
    "        'nhid' : wandb.config['nhid'], # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "        'nlayers' :wandb.config['nlayers'], # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "        'nhead' : wandb.config['nhead'], # the number of heads in the multiheadattention models\n",
    "        'seq-len': config_data['seq-len'], # dont use wandb config \n",
    "        'vocab-size':len(dm.vocab.stoi) # the size of vocabulary /also called tokens\n",
    "    }\n",
    "\n",
    "    # setup model - note how we refer to sweep parameters with wandb.config\n",
    "    model = TransformerModel(config=config_model)\n",
    "\n",
    "    # setup Trainer\n",
    "    trainer = Trainer(precision=16,gpus=-1, num_nodes=1,  max_epochs=1000, check_val_every_n_epoch=1,deterministic=True, logger=[wandb_logger] ,gradient_clip_val=0.5,enable_pl_optimizer=True,callbacks=[early_stop_callback],progress_bar_refresh_rate=0)\n",
    "    trainer.fit(model,dm) # traning and validation\n",
    "\n",
    "    return None\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"Alarm-Transformers-Net\") # sweep id based on our configuration\n",
    "# wandb.agent(sweep_id, function=sweep_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout,seq_len=seq_len)\n",
    "# trainer = Trainer(precision=16,gpus=1,max_epochs=400,check_val_every_n_epoch=4,deterministic=True, gradient_clip_val=0.5,logger=tb_logger,progress_bar_refresh_rate=50,auto_lr_find=0.002)\n",
    "# trainer.tune(model,dm) # finding the lr : first way\n",
    "# 2nd way\n",
    "# lr_finder = trainer.tuner.lr_find(model)\n",
    "# print(lr_finder.results)\n",
    "# fig = lr_finder.plot(suggest=True) # Plot with\n",
    "# fig.show()\n",
    "# new_lr = lr_finder.suggestion() # Pick point based on plot, or get suggestion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# config_data = {\n",
    "#     'batch-size' : 64, # Batch Size \n",
    "#     'seq-len' : 900, # Sequence length\n",
    "#     'filter-seq-len' :350 # remove sequence whose size is greater than this len\n",
    "# }\n",
    "\n",
    "# dm = MyDataModule(dir_path=dir_name,file_name=fname,config=config_data)\n",
    "\n",
    "# config_model = {\n",
    "#     'lr' : 0.0001,\n",
    "#     'dropout' : 0.0,\n",
    "#     'weight-decay': 0.0,\n",
    "#     'em-size' :256, # embedding dimension \n",
    "#     'nhid' : 256, # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "#     'nlayers' :2, # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "#     'nhead' : 2, # the number of heads in the multiheadattention models\n",
    "#     'seq-len': config_data['seq-len'],\n",
    "#     'vocab-size':len(dm.vocab.stoi) # the size of vocabulary /also called tokens\n",
    "# }\n",
    "\n",
    "# print(f\"> Vocab Size (Number of Unique Alarms): {config_model['vocab-size']}\")\n",
    "# model = TransformerModel(config=config_model)\n",
    "# print(f\"> Customised lr = {model.lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test(datamodule=dm) # testing"
   ]
  },
  {
   "source": [
    "# accelerator='dp'\n",
    "# progress_bar_refresh_rate=0 # set to zero to disable it"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish() # When we want to close our W&B run, we can call wandb.finish() (mainly useful in notebooks, called automatically in scripts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import tempfile\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.utilities.cloud_io import load as pl_load\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback\n"
   ]
  },
  {
   "source": [
    "Ray Tune comes with ready-to-use PyTorch Lightning callbacks. To report metrics back to Tune after each validation epoch, we will use the TuneReportCallback:\n",
    "\n",
    "We are also able to specify the number of epochs to train each model, and the number of GPUs we want to use for training. We also create a TensorBoard logger that writes logfiles directly into Tune’s root trial directory - if we didn’t do that PyTorch Lightning would create subdirectories, and each trial would thus be shown twice in TensorBoard, one time for Tune’s logs, and another time for PyTorch Lightning’s logs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_alarmnet_tune(config,num_epochs, dir_name, fname):\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_epoch_loss',\n",
    "        min_delta=0.00,\n",
    "        patience=6,\n",
    "        verbose=True,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    \n",
    "    tuner_callback = TuneReportCallback({\n",
    "    \"val_epoch_loss\": \"val_epoch_loss\",\n",
    "    \"val_epoch_F1\": \"val_epoch_F1\"\n",
    "    }, on=\"validation_end\")\n",
    "\n",
    "    # setup data\n",
    "    config_data = {\n",
    "    'batch-size' :config[\"batch-size\"], # Batch Size \n",
    "    'seq-len' : config['seq-len'], # Sequence length\n",
    "    'filter-seq-len':350 # remove sequence whose size is greater than this len\n",
    "    }\n",
    "\n",
    "    dm = MyDataModule(dir_path=dir_name,file_name=fname,config=config_data)\n",
    "\n",
    "\n",
    "    config_model = {\n",
    "        'lr' : config['lr'],\n",
    "        'dropout' : config['dropout'],\n",
    "        'weight-decay': config['weight-decay'],\n",
    "        'em-size' :config['em-size'], # embedding dimension \n",
    "        'nhid' : config['nhid'], # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "        'nlayers' :config['nlayers'], # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "        'nhead' : config['nhead'], # the number of heads in the multiheadattention models\n",
    "        'seq-len': config_data['seq-len'], # dont use wandb config \n",
    "        'vocab-size':len(dm.vocab.stoi) # the size of vocabulary /also called tokens\n",
    "    }\n",
    "\n",
    "    # setup model - note how we refer to sweep parameters with wandb.config\n",
    "    model = TransformerModel(config=config_model)\n",
    "\n",
    "    # setup Trainer\n",
    "    # logger=[wandb_logger]\n",
    "    tb_logger =  TensorBoardLogger(save_dir=tune.get_trial_dir(), name=\"\", version=\".\")\n",
    "    trainer = Trainer(precision=16,gpus=-1, num_nodes=1,  max_epochs=100, check_val_every_n_epoch=1,deterministic=True, logger = [tb_logger],  gradient_clip_val=0.5,enable_pl_optimizer=True,callbacks=[early_stop_callback,tuner_callback],progress_bar_refresh_rate=0)\n",
    "    trainer.fit(model,dm) # traning and validation\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "source": [
    "In this example, we use an Asynchronous Hyperband scheduler. This scheduler decides at each iteration which trials are likely to perform badly, and stops these trials. This way we don’t waste any resources on bad hyperparameter configurations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_alarmnet_asha(num_samples, num_epochs, gpus_per_trial=0):\n",
    "    dir_name = \"/home/waris/Github/predict-future-alarms/.data/\"\n",
    "    fname = 'seqs.tokens'\n",
    "    config_tune = {\n",
    "        \"batch-size\":  tune.choice([16]), \n",
    "        \"seq-len\": tune.grid_search([700]),        \n",
    "        \"lr\": tune.grid_search([0.0001,0.0002]),\n",
    "        \"dropout\": tune.grid_search ( [0.1,0.15]),\n",
    "        \"weight-decay\": tune.grid_search([0.0003,0.0002]),\n",
    "        \"em-size\":  tune.grid_search([256,512]),\n",
    "        \"nhid\": tune.grid_search([266,128,64]),\n",
    "        \"nlayers\": tune.grid_search([4,3,2]),\n",
    "        \"nhead\": tune.grid_search([8,4])\n",
    "        }\n",
    "\n",
    "    scheduler = ASHAScheduler(max_t=num_epochs,grace_period=1,reduction_factor=2)\n",
    "\n",
    "    # reporter = CLIReporter(\n",
    "    #     parameter_columns=[\"seq-len\",\"lr\",\"dropout\",\"weight-decay\",\"em-size\",\"nhid\",\"nlayers\",\"nhead\"],\n",
    "    #     metric_columns=[\"val_epoch_loss\",\"val_epoch_F1\",\"training_iteration\"]\n",
    "    # )\n",
    "\n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(train_alarmnet_tune,num_epochs=num_epochs,dir_name=dir_name,fname=fname),\n",
    "        resources_per_trial={\"cpu\": 4,\"gpu\":1},\n",
    "        metric=\"val_epoch_F1\",\n",
    "        mode=\"max\",\n",
    "        config=config_tune,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        # progress_reporter=reporter,\n",
    "        name=\"tune_alarmnet_asha\")\n",
    "\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "\n",
    "    shutil.rmtree(dir_name+\"ray-tune\")"
   ]
  },
  {
   "source": [
    "Ray Tune will now proceed to sample 10 different parameter combinations randomly, train them, and compare their performance afterwards."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-01-22 05:57:25,195\tINFO services.py:1171 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n",
      "2021-01-22 05:57:25,845\tWARNING function_runner.py:539 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 6.0/15.5 GiB<br>Using AsyncHyperBand: num_stopped=0\nBracket: Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Resources requested: 4/8 CPUs, 1/1 GPUs, 0.0/7.42 GiB heap, 0.0/2.54 GiB objects (0/1.0 accelerator_type:RTX)<br>Result logdir: /home/waris/ray_results/tune_alarmnet_asha<br>Number of trials: 1/3840 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name        </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  batch-size</th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">  em-size</th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  nhead</th><th style=\"text-align: right;\">  nhid</th><th style=\"text-align: right;\">  nlayers</th><th style=\"text-align: right;\">  seq-len</th><th style=\"text-align: right;\">  weight-decay</th></tr>\n</thead>\n<tbody>\n<tr><td>_inner_8e95b_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      0.1</td><td style=\"text-align: right;\">      256</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">        3</td><td style=\"text-align: right;\">      700</td><td style=\"text-align: right;\">        0.0003</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0lines [00:00, ?lines/s]\n",
      "4082lines [00:00, 40802.92lines/s]\n",
      "6106lines [00:00, 31267.11lines/s]\n",
      "7967lines [00:00, 25968.36lines/s]\n",
      "9661lines [00:00, 22387.28lines/s]\n",
      "11370lines [00:00, 20481.01lines/s]\n",
      "13014lines [00:00, 16809.44lines/s]\n",
      "14521lines [00:00, 15782.06lines/s]\n",
      "15990lines [00:00, 15256.60lines/s]\n",
      "17443lines [00:00, 14605.68lines/s]\n",
      "18859lines [00:01, 13773.59lines/s]\n",
      "20215lines [00:01, 13543.11lines/s]\n",
      "21555lines [00:01, 13190.76lines/s]\n",
      "22874lines [00:01, 13189.87lines/s]\n",
      "24226lines [00:01, 13285.82lines/s]\n",
      "25551lines [00:01, 12942.75lines/s]\n",
      "26889lines [00:01, 13069.79lines/s]\n",
      "28207lines [00:01, 13102.53lines/s]\n",
      "29518lines [00:01, 12998.55lines/s]\n",
      "30825lines [00:02, 15381.18lines/s]\n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m total seqs= 27975\n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m ['A17 A75 A17 A57 A17 A99 A98 A56\\n', 'A245 A246 A50 A243\\n', 'A243 A9 A559 A1025\\n', 'A50 A59 A60 A64 A392 A726 A9 A725 A726 A725 A243 A725\\n']\n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m GPU available: True, used: True\n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m TPU available: None, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m Using native 16bit precision.\n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m 2021-01-22 05:57:33.864712: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m   | Name                | Type               | Params\n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m -----------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m 0 | pos_encoder         | PositionalEncoding | 0     \n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m 1 | transformer_encoder | TransformerEncoder | 990 K \n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m 2 | encoder             | Embedding          | 245 K \n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m 3 | decoder             | Linear             | 246 K \n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m 4 | train_F1            | F1                 | 0     \n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m 5 | val_F1              | F1                 | 0     \n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m 6 | test_F1             | F1                 | 0     \n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m -----------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m 1.5 M     Trainable params\n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m 1.5 M     Total params\n",
      "\u001b[2m\u001b[36m(pid=89603)\u001b[0m >== Average Valid Loss = 7.167915344238281, F1 = 0.0\n"
     ]
    }
   ],
   "source": [
    "tune_alarmnet_asha(num_samples=10, num_epochs=100)\n"
   ]
  },
  {
   "source": [
    "# Using Population Based Training to find the best parameters (2nd way)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_alarmnet_tune_checkpoint(config=None,num_epochs=None,checkpoint_dir=None,dir_name=None, fname=None):\n",
    "#     early_stop_callback = EarlyStopping(\n",
    "#         monitor='val_epoch_loss',\n",
    "#         min_delta=0.00,\n",
    "#         patience=6,\n",
    "#         verbose=True,\n",
    "#         mode='min'\n",
    "#     )\n",
    "\n",
    "    \n",
    "#     # tuner_callback = TuneReportCallback({\n",
    "#     # \"val_epoch_loss\": \"val_epoch_loss\",\n",
    "#     # \"val_epoch_F1\": \"val_epoch_F1\"\n",
    "#     # }, on=\"validation_end\")\n",
    "\n",
    "#     tuner_callback = TuneReportCheckpointCallback(\n",
    "#                 metrics={\n",
    "#                     \"val_epoch_loss\": \"val_epoch_loss\",\n",
    "#                     \"val_epoch_F1\": \"val_epoch_F1\"\n",
    "#                 },\n",
    "#                 filename=\"checkpoint\",\n",
    "#                 on=\"validation_end\")\n",
    "\n",
    "\n",
    "#     # setup data\n",
    "#     config_data = {\n",
    "#     'batch-size' :config[\"batch-size\"], # Batch Size \n",
    "#     'seq-len' : config['seq-len'], # Sequence length\n",
    "#     'filter-seq-len':350 # remove sequence whose size is greater than this len\n",
    "#     }\n",
    "\n",
    "#     dm = MyDataModule(dir_path=dir_name,file_name=fname,config=config_data)\n",
    "\n",
    "\n",
    "#     config_model = {\n",
    "#         'lr' : config['lr'],\n",
    "#         'dropout' : config['dropout'],\n",
    "#         'weight-decay': config['weight-decay'],\n",
    "#         'em-size' :config['em-size'], # embedding dimension \n",
    "#         'nhid' : config['nhid'], # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "#         'nlayers' :config['nlayers'], # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "#         'nhead' : config['nhead'], # the number of heads in the multiheadattention models\n",
    "#         'seq-len': config_data['seq-len'], # dont use wandb config \n",
    "#         'vocab-size':len(dm.vocab.stoi) # the size of vocabulary /also called tokens\n",
    "#     }\n",
    "\n",
    "#     model = None\n",
    "\n",
    "#     if checkpoint_dir:\n",
    "#         print(\"-----------------------------------------------------------------------------------------\")\n",
    "#         # Currently, this leads to errors:\n",
    "#         # model = LightningMNISTClassifier.load_from_checkpoint(\n",
    "#         #     os.path.join(checkpoint, \"checkpoint\"))\n",
    "#         # Workaround:\n",
    "#         ckpt = pl_load(os.path.join(checkpoint_dir, \"checkpoint\"),map_location=lambda storage, loc: storage)\n",
    "#         model = TransformerModel._load_model_state(ckpt, config=config_model)\n",
    "#         trainer.current_epoch = ckpt[\"epoch\"]\n",
    "#     else:\n",
    "#         model = TransformerModel(config=config_model)\n",
    "\n",
    "#     # setup model - note how we refer to sweep parameters with wandb.config\n",
    "#     # model = TransformerModel(config=config_model)\n",
    "\n",
    "#     # setup Trainer\n",
    "#     # logger=[wandb_logger]\n",
    "#     tb_logger =  TensorBoardLogger(save_dir=tune.get_trial_dir(), name=\"\", version=\".\")\n",
    "#     trainer = Trainer(precision=16,gpus=-1, num_nodes=1,  max_epochs=100, check_val_every_n_epoch=1,deterministic=True, logger = [tb_logger],  gradient_clip_val=0.5,enable_pl_optimizer=True,callbacks=[early_stop_callback,tuner_callback],progress_bar_refresh_rate=0)\n",
    "\n",
    "\n",
    "#     trainer.fit(model,dm) # traning and validation\n",
    "#     return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tune_alarmnet_pbt(num_samples, num_epochs, gpus_per_trial=0):\n",
    "#     dir_name = \"/home/waris/Github/predict-future-alarms/.data/\"\n",
    "#     fname = 'seqs.tokens'\n",
    "#     config_tune = {\n",
    "#         \"batch-size\": 16, \n",
    "#         \"seq-len\": tune.grid_search([700,600,400]),        \n",
    "#         \"lr\": 0.0001,\n",
    "#         \"dropout\": tune.grid_search ( [0.1,0.15]),\n",
    "#         \"weight-decay\": tune.grid_search([0.0003,0.0002]),\n",
    "#         \"em-size\":  tune.grid_search([256,512]),\n",
    "#         \"nhid\": tune.grid_search([128,64]),\n",
    "#         \"nlayers\": tune.grid_search([3,2]),\n",
    "#         \"nhead\": tune.grid_search([8,4])\n",
    "#         }\n",
    "\n",
    "#     # scheduler = ASHAScheduler(max_t=num_epochs,grace_period=1,reduction_factor=2)\n",
    "\n",
    "#     scheduler = PopulationBasedTraining(\n",
    "#         perturbation_interval=4,\n",
    "#         hyperparam_mutations={\n",
    "#             \"lr\": [0.0001,0.0002],\n",
    "#             \"batch-size\": [8,16]\n",
    "#         })\n",
    "\n",
    "#     # reporter = CLIReporter(\n",
    "#     #     parameter_columns=[\"seq-len\",\"lr\",\"dropout\",\"weight-decay\",\"em-size\",\"nhid\",\"nlayers\",\"nhead\"],\n",
    "#     #     metric_columns=[\"val_epoch_loss\",\"val_epoch_F1\",\"training_iteration\"]\n",
    "#     # )\n",
    "\n",
    "#     analysis = tune.run(\n",
    "#         tune.with_parameters(train_alarmnet_tune_checkpoint,num_epochs=num_epochs,dir_name=dir_name,fname=fname),\n",
    "#         resources_per_trial={\"cpu\": 4,\"gpu\":1},\n",
    "#         metric=\"val_epoch_F1\",\n",
    "#         mode=\"max\",\n",
    "#         config=config_tune,\n",
    "#         num_samples=num_samples,\n",
    "#         scheduler=scheduler,\n",
    "#         # progress_reporter=reporter,\n",
    "#         name=\"tune_alarmnet_pbt\")\n",
    "\n",
    "#     print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "\n",
    "#     shutil.rmtree(dir_name+\"ray-tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune_alarmnet_pbt(num_samples=2,num_epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('dl': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8076afad8c2f739e22f417bad77704dbad7b0389c4d6903b1ae4a1b7479f7ed3"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}