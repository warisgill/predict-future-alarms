{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from functools import partial\n",
    "import itertools\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCol2Count(df,col):\n",
    "    return dict(df[col].value_counts())\n",
    "    \n",
    "def __removeChatteringAlarmsHelper(alarms,chattering_timedelta_threshold, chattering_count_threshold):\n",
    "        \"\"\"Find the chatterings in an alarms list from the same source.  \n",
    "        \"\"\"\n",
    "\n",
    "        alarms_without_chattering = []\n",
    "        alarms = [alarm for alarm in sorted(alarms, key=lambda arg: arg[\"StartTime\"], reverse=False)]\n",
    "        i = 0\n",
    "        j = 0\n",
    "\n",
    "        while i < (len(alarms)):\n",
    "            alarms_without_chattering.append(alarms[i])\n",
    "            prev_start = alarms[i][\"StartTime\"]\n",
    "            prev_end = alarms[i][\"EndTime\"]\n",
    "            count_alarms = 0\n",
    "            j = i + 1\n",
    "            while j < len(alarms):\n",
    "                next_start = alarms[j][\"StartTime\"]\n",
    "                next_end = alarms[j][\"EndTime\"]\n",
    "\n",
    "                # this assert is very important: the prev alarm has to turn off before the start of\n",
    "                # the next one\n",
    "                assert(prev_start <= next_start)\n",
    "                assert(prev_end <= next_start)\n",
    "                assert(prev_end <= next_end)\n",
    "\n",
    "                delta = timedelta.total_seconds(next_start - prev_start)\n",
    "                assert (delta >= 0)\n",
    "                \n",
    "                if delta > chattering_timedelta_threshold:\n",
    "                    break\n",
    "                count_alarms += 1\n",
    "                \n",
    "                j += 1\n",
    "            \n",
    "            if count_alarms >= chattering_count_threshold:\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        return alarms_without_chattering\n",
    "    \n",
    "def tempFun(df,chat_delta,chat_count,sname):\n",
    "\n",
    "    alarms_without_chatterings = []\n",
    "    df_source = df.loc[df['SourceName'].isin([sname])]\n",
    "\n",
    "    for condition in df_source[\"Condition\"].unique():\n",
    "        df_condition = df_source.loc[df_source['Condition'].isin([condition])]\n",
    "        alarms = __removeChatteringAlarmsHelper(df_condition.to_dict(orient=\"records\"),chattering_timedelta_threshold=chat_delta,chattering_count_threshold=chat_count)\n",
    "        alarms_without_chatterings = alarms_without_chatterings + alarms\n",
    "    \n",
    "    return alarms_without_chatterings\n",
    "\n",
    "def removeChatteringAlarms(df,chattering_timedelta_threshold=None,chat_count=None):\n",
    "\n",
    "    # for sname in df[\"SourceName\"].unique():\n",
    "\n",
    "    alarms_without_chatterings = [] \n",
    "    sources=[sname for sname in df[\"SourceName\"].unique()] \n",
    "\n",
    "    myFun =  partial(tempFun,df,chattering_timedelta_threshold,chat_count)\n",
    "    \n",
    "\n",
    "    with Pool(3) as p:\n",
    "        alarms_without_chatterings = p.map(myFun, sources)\n",
    "\n",
    "    alarms_without_chatterings = list(itertools.chain.from_iterable(alarms_without_chatterings))\n",
    "    return pd.DataFrame(alarms_without_chatterings)\n",
    "\n",
    "\n",
    "\n",
    "class AlarmsProcessing:\n",
    "    def __init__(self,config) -> None:\n",
    "        self.config = config\n",
    "        self.alias2name = None\n",
    "        self.name2alias = None\n",
    "        self.df = pd.read_csv(self.config['file-path'], low_memory=False, usecols=self.config['usecols'],parse_dates=self.config['date-cols'])\n",
    "        if self.config['alias']:\n",
    "            self.convertSourceNamesToAlias()\n",
    "        \n",
    "        for m in self.df[\"Year-Month\"].unique():\n",
    "            \n",
    "            month_df = self.df[self.df[\"Year-Month\"].isin([m])].sort_values(by='StartTime', ascending=True).reset_index(drop=True)\n",
    "\n",
    "            days = sorted(list(month_df['Day'].unique()))\n",
    "            print(f\"[{m}],Days:{days}\")\n",
    "    \n",
    "    def convertSourceNamesToAlias(self):\n",
    "        alias2name = {f\"A{k}\": v for k, v in enumerate(self.df[\"SourceName\"].unique())}\n",
    "        name2alias = {v: k for k, v in alias2name.items()}\n",
    "        self.df[\"SourceName\"] = self.df[\"SourceName\"].apply(lambda sname: name2alias[sname])\n",
    "        self.alias2name = alias2name\n",
    "        self.name2alias = name2alias\n",
    "        # return name2alias, alias2name\n",
    "\n",
    "    def removeChatteringAlarms(self,df,chattering_timedelta_threshold=60,chattering_count_threshold=2):\n",
    "        return removeChatteringAlarms(df=df,chattering_timedelta_threshold=chattering_timedelta_threshold,chat_count=chattering_count_threshold)\n",
    "\n",
    "    def removeMomentaryAlarms(self,df,monmentarly_filter=None):\n",
    "        df = df[(df[\"TimeDelta\"] > monmentarly_filter)].reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def removeStalingAlarms(self,df,staling_filter=None):\n",
    "        df =  df[(df[\"TimeDelta\"] < staling_filter)].reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def removeConditionsAlarms(self,df,conditions_filter):\n",
    "        df = df[~df[\"Condition\"].isin(conditions_filter)].reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def removeSources(self,df, sources_filter):\n",
    "        df = df[(~df[\"SourceName\"].isin(sources_filter))].reset_index(drop=True)\n",
    "        return df\n",
    "    \n",
    "    def removeSourcesBasedOnMinCount(self,df,min_alarms_per_source_filter):\n",
    "        source2count = dict(df[\"SourceName\"].value_counts())\n",
    "        select_sources = [k for k, v in source2count.items() if v >= min_alarms_per_source_filter]\n",
    "        df = df[df[\"SourceName\"].isin(select_sources)]\n",
    "        return df\n",
    "\n",
    "    def getDFWithCommonSourcesInAllMonths(self,df):\n",
    "        each_month_source_names = [[sname for sname in df[df[\"Year-Month\"]==month][\"SourceName\"].unique()] for month in df[\"Year-Month\"].unique()]\n",
    "        common_sourcenames_in_all_months = set.intersection(*[set(l) for l in each_month_source_names])\n",
    "        df = df[df[\"SourceName\"].isin(common_sourcenames_in_all_months)]\n",
    "        return df\n",
    "      \n",
    "    def updatSourceNamewithCondition(self,df):\n",
    "        def _concatenateSourceNameAndCondition(sname, condition):\n",
    "            return sname+\"-\"+condition\n",
    "        df[\"SourceName\"] = df[[\"SourceName\", \"Condition\"]].apply(\n",
    "            lambda arg: _concatenateSourceNameAndCondition(*arg), axis=1)\n",
    "        return df\n",
    "    \n",
    "    def getCondition2Sources(self,df,condition):\n",
    "        return df[df[\"Condition\"]==condition][\"SourceName\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataset:\n",
    "    def __init__(self,config) -> None:\n",
    "        self.config = config\n",
    "\n",
    "    def getSeqsFromAlarmsDF(self,df,seq_duration_gap,filter_short_seq):\n",
    "        print(f\">> Duration to next seq: {seq_duration_gap}, ignore seq len: {filter_short_seq}\")\n",
    "\n",
    "        list_of_sequences = []    \n",
    "        alarms= df.sort_values(by='StartTime', ascending=True).reset_index(drop=True).to_dict(orient=\"records\")\n",
    "        alarms = [alarm for alarm in sorted(alarms, key=lambda arg: arg[\"StartTime\"], reverse=False)] # sorting\n",
    "\n",
    "        # print('check',len(alarms))\n",
    "        assert alarms[0]['StartTime'] < alarms[-1]['StartTime']\n",
    "        i =0\n",
    "        j= 0\n",
    "\n",
    "        max_seq_len = -1\n",
    "        while i <len(alarms):\n",
    "            prev_start = alarms[i][\"StartTime\"]\n",
    "            seq = []\n",
    "            seq.append(alarms[i])\n",
    "            j = i+1\n",
    "            while j < len(alarms):    \n",
    "                next_start = alarms[j][\"StartTime\"]\n",
    "                delta = timedelta.total_seconds(next_start - prev_start)\n",
    "                # print(delta)\n",
    "                assert delta >= 0\n",
    "                if delta >= seq_duration_gap:\n",
    "                    break\n",
    "\n",
    "                seq.append(alarms[j])\n",
    "                j += 1\n",
    "            i = j\n",
    "\n",
    "            if len(seq) > max_seq_len:\n",
    "                max_seq_len = len(seq)\n",
    "            \n",
    "            if len(seq)>=filter_short_seq:\n",
    "                seq = [alarm for alarm in sorted(seq, key=lambda arg: arg[\"StartTime\"], reverse=False)]\n",
    "                seq = [alarm[\"SourceName\"] for alarm in seq]\n",
    "                list_of_sequences.append(seq)\n",
    "        \n",
    "        \n",
    "        return list_of_sequences, max_seq_len\n",
    "\n",
    "\n",
    "    def splitDFtoTrainValidDfsPerMonthByRows(self,df,p=0.2):    \n",
    "        tarain_dfs = []\n",
    "        valid_dfs = []\n",
    "        for m in df[\"Year-Month\"].unique():\n",
    "            \n",
    "            month_df = df[(df[\"Year-Month\"].isin([m]))].sort_values(by='StartTime', ascending=True).reset_index(drop=True)\n",
    "\n",
    "            train_df = month_df[0:int(len(month_df)*(1-p))]\n",
    "            train_df = train_df.reset_index(drop=True)\n",
    "            valid_df = month_df[int(len(month_df)*(1-p)):]\n",
    "            valid_df = valid_df.reset_index(drop=True)\n",
    "            assert len(train_df)+len(valid_df) == len(month_df)\n",
    "            tarain_dfs.append(train_df)\n",
    "            valid_dfs.append(valid_df)\n",
    "        \n",
    "        t_df = pd.concat(tarain_dfs, ignore_index=True).sort_values(by='StartTime', ascending=True).reset_index(drop=True)\n",
    "        v_df = pd.concat(valid_dfs, ignore_index=True).sort_values(by='StartTime', ascending=True).reset_index(drop=True)\n",
    "\n",
    "        assert len(t_df)+len(v_df) == len(df) \n",
    "\n",
    "        return t_df, v_df\n",
    "    \n",
    "    def splitDFtoTrainValidDfsPerMonthByDays(self,df,p=0.2):    \n",
    "        tarain_dfs = []\n",
    "        valid_dfs = []\n",
    "        for m in df[\"Year-Month\"].unique():\n",
    "            \n",
    "            month_df = df[(df[\"Year-Month\"].isin([m]))].sort_values(by='StartTime', ascending=True).reset_index(drop=True)\n",
    "\n",
    "            days = sorted(list(month_df['Day'].unique()))\n",
    "            # if len(days)<5:\n",
    "            #     print(f\"Skipping : [{m}],Days:{days}\")\n",
    "            #     continue\n",
    "            \n",
    "            print(f\"[{m}]: Days: {days}\")\n",
    "\n",
    "            train_days = days[0:len(days)-int(len(days)*p)]\n",
    "            valid_days = days[len(days)-int(len(days)*p):len(days)]\n",
    "            # print(f\"[{m}]Train Days: {train_days}, val days = {valid_days} \")\n",
    "\n",
    "            train_df = month_df[month_df['Day'].isin(train_days)].reset_index(drop=True)\n",
    "            valid_df = month_df[month_df['Day'].isin(valid_days)].reset_index(drop=True)\n",
    "\n",
    "            assert len(train_df)+len(valid_df) == len(month_df)\n",
    "            tarain_dfs.append(train_df)\n",
    "            valid_dfs.append(valid_df)\n",
    "        \n",
    "        t_df = pd.concat(tarain_dfs, ignore_index=True).sort_values(by='StartTime', ascending=True).reset_index(drop=True)\n",
    "        v_df = pd.concat(valid_dfs, ignore_index=True).sort_values(by='StartTime', ascending=True).reset_index(drop=True)\n",
    "\n",
    "        # assert len(t_df)+len(v_df) == len(df) \n",
    "\n",
    "        return t_df, v_df\n",
    "    \n",
    "    def writeSequeces2TokenFile(self,file_path,li_of_seqs):\n",
    "        with open(file_path,\"w\") as f:\n",
    "            for seq in li_of_seqs:\n",
    "                f.write(f\"{' '.join(seq)}\\n\")\n",
    "        \n",
    "        print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../.data/all-months-alarms-2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cf6b6cde6cdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0malarm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlarmsProcessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0f2e6ab0f7b9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias2name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname2alias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file-path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date-cols'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alias'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvertSourceNamesToAlias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../.data/all-months-alarms-2.csv'"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'file-path': \"../.data/all-months-alarms-2.csv\",\n",
    "    'usecols':[\"SourceName\", \"Condition\", \"StartTime\",\"EndTime\",\"TimeDelta\",\"Year-Month\",'Day'],\n",
    "    'date-cols':   [\"StartTime\", \"EndTime\"],\n",
    "    'alias': False\n",
    "}\n",
    "\n",
    "alarm = AlarmsProcessing(config=config)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "print(\"Total Time to load the data \", time.time()-start)\n",
    "\n",
    "alarm.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con2count=getCol2Count(df=alarm.df,col='Condition')\n",
    "con2count\n",
    "\n",
    "#Important  Trip, HHH, HTRP, LTRP, LLL, -> most imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove communication alarms\n",
    "df = alarm.removeConditionsAlarms(df=alarm.df,conditions_filter=[\"IOP\", \"IOP-\",'VEL-','VEL+'])\n",
    "con2count=getCol2Count(df=df,col='Condition')\n",
    "con2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = alarm.removeChatteringAlarms(df=df,chattering_timedelta_threshold=60,chattering_count_threshold=3)\n",
    "con2count=getCol2Count(df=df,col='Condition')\n",
    "con2count\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = alarm.removeConditionsAlarms(df=alarm.df,conditions_filter=[\"IOP\", \"IOP-\",'VEL+','VEL-'])\n",
    "# con2count=getCol2Count(df=df3,col='Condition')\n",
    "# print(con2count)\n",
    "# df4 = removeChatteringAlarms(df=df3,chattering_timedelta_threshold=60, chat_count =2)\n",
    "# con2count=getCol2Count(df=df4,col='Condition')\n",
    "# con2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total = 0\n",
    "# for cond,count in getCol2Count(df=df,col='Condition').items(): \n",
    "#     l = df[df['Condition']==cond][\"SourceName\"].unique()\n",
    "#     total += len(l)\n",
    "\n",
    "#     print(f\">>{cond},{count},{len(l)}\")\n",
    "\n",
    "\n",
    "# print(len(getCol2Count(df,col='SourceName')),total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = alarm.removeSourcesBasedOnMinCount(df,min_alarms_per_source_filter=70)\n",
    "total = 0\n",
    "for cond,count in getCol2Count(df=df2,col='Condition').items(): \n",
    "    l = df2[df2['Condition']==cond][\"SourceName\"].unique()\n",
    "    total += len(l)\n",
    "\n",
    "    print(f\">>{cond},{count},{len(l)}\")\n",
    "\n",
    "\n",
    "print(len(getCol2Count(df2,col='SourceName')),total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_sources = []\n",
    "\n",
    "# for s in df[\"SourceName\"].unique():\n",
    "#     # print(len(df2[df2['SourceName']==s]['Condition'].unique()))\n",
    "#     if len(df[df['SourceName']==s]['Condition'].unique())==1:\n",
    "#         temp_sources.append(s)\n",
    "\n",
    "\n",
    "# print(temp_sources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FINAL_DF = df2\n",
    "\n",
    "\n",
    "alarm_dataset = PrepareDataset({})\n",
    "train_df, valid_df = alarm_dataset.splitDFtoTrainValidDfsPerMonthByDays(FINAL_DF,p=0.3)\n",
    "valid_df, test_df = alarm_dataset.splitDFtoTrainValidDfsPerMonthByDays(valid_df,p=0.2)\n",
    "\n",
    "seqs_train, _  = alarm_dataset.getSeqsFromAlarmsDF(train_df,seq_duration_gap=60*15,filter_short_seq=0)\n",
    "seqs_valid, _  = alarm_dataset.getSeqsFromAlarmsDF(valid_df,seq_duration_gap=60*15,filter_short_seq=0)\n",
    "seqs_test, _  = alarm_dataset.getSeqsFromAlarmsDF(test_df,seq_duration_gap=60*15,filter_short_seq=0)\n",
    "\n",
    "alarm_dataset.writeSequeces2TokenFile(file_path=\"../.data/train.tokens\",li_of_seqs=seqs_train)\n",
    "alarm_dataset.writeSequeces2TokenFile(file_path=\"../.data/val.tokens\",li_of_seqs=seqs_valid)\n",
    "alarm_dataset.writeSequeces2TokenFile(file_path=\"../.data/test.tokens\",li_of_seqs=seqs_test)\n",
    "\n",
    "print(\"DataSet is complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration_from_1_seq_to_next = 60*15 # duration in seconds\n",
    "# filter_short_seq = 3 # remove the sequence whose size is less than 4\n",
    "# li_of_seqs,max_seq_len = getSequenceOfWholeData(df_rnn,duration_from_1_seq_to_next,filter_short_seq)\n",
    "# print(len(li_of_seqs))\n",
    "# print(li_of_seqs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_len_2_count ={}\n",
    "# for seq in li_of_seqs:\n",
    "#     # seq = removeSameAlarms(seq)\n",
    "#     l = len(seq)\n",
    "#     seq_len_2_count[l] = 1+seq_len_2_count.get(l,0)\n",
    "\n",
    "# seq_len_2_count = {k:v for k,v in sorted(seq_len_2_count.items(), key=lambda t: t[1] )}\n",
    "# seq_len_2_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def removeSameAlarms(seq):\n",
    "#     new_seq = []\n",
    "\n",
    "#     new_seq.append(seq[0])\n",
    "\n",
    "#     for a in seq:\n",
    "\n",
    "#         if a == new_seq[-1]:\n",
    "#             continue\n",
    "#         new_seq.append(a)\n",
    "    \n",
    "#     return new_seq\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(seq_len_2_count.keys())/len(seq_len_2_count.keys())\n",
    "# l = 5*['5']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}