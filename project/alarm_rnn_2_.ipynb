{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # for logging \n",
    "\n",
    "from comet_ml import Experiment\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.loggers import TestTubeLogger\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "# For metrics\n",
    "from pytorch_lightning import metrics\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import io\n",
    "import torchtext\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "\n",
    "\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping # The EarlyStopping callback can be used to monitor a validation metric and stop the training when no improvement is observed.\n",
    "\"\"\"\n",
    "    To enable it:\n",
    "\n",
    "    Import EarlyStopping callback.\n",
    "\n",
    "    Log the metric you want to monitor using log() method.\n",
    "\n",
    "    Init the callback, and set monitor to the logged metric of your choice.\n",
    "\n",
    "    Pass the EarlyStopping callback to the Trainer callbacks flag.\n",
    "\"\"\"\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.metrics import Metric\n",
    "from pytorch_lightning.metrics.utils import _input_format_classification\n",
    "from sklearn.metrics import classification_report\n",
    "class MyClassificationReport(Metric):\n",
    "    def __init__(self,threshold: float = 0.5,compute_on_step: bool = True,dist_sync_on_step: bool = False):\n",
    "        super().__init__(\n",
    "            compute_on_step=compute_on_step,\n",
    "            dist_sync_on_step=dist_sync_on_step,\n",
    "        )\n",
    "\n",
    "        self.threshold = threshold\n",
    "        self.add_state(\"preds\", default=[], dist_reduce_fx=None)\n",
    "        self.add_state(\"target\", default=[], dist_reduce_fx=None)\n",
    "\n",
    "        # rank_zero_warn(\n",
    "        #     'Metric `MyClassificationReport` will save all targets and predictions in buffer.'\n",
    "        #     ' For large datasets this may lead to large memory footprint.'\n",
    "        # )\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        preds = preds.cpu()\n",
    "        target = target.cpu()\n",
    "        y_hat, y = preds.max(1).indices, target\n",
    "        assert y_hat.shape == y.shape\n",
    "        self.preds.append(y_hat)\n",
    "        self.target.append(y)\n",
    "\n",
    "    def compute(self):\n",
    "        preds = torch.cat(self.preds, dim=0)\n",
    "        target = torch.cat(self.target, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class AlarmDataset(Dataset):\n",
    "    def __init__(self,data,seq_len,batch_size):\n",
    "        self.length = len(data)//seq_len # how much data i have         \n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "       \n",
    "    def __getitem__(self, index: int):\n",
    "        x = self.data[index*self.seq_len:(index*self.seq_len)+self.seq_len]\n",
    "        y = self.data[1+index*self.seq_len:1+(index*self.seq_len)+self.seq_len]\n",
    "        return x,y\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        dir_path = self.config['dir-path']\n",
    "        file_name = 'train.tokens'\n",
    "\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.vocab = build_vocab_from_iterator(map(self.tokenizer,iter(io.open(dir_path+file_name,encoding=\"utf8\"))))\n",
    "    \n",
    "\n",
    "        train_data = self.data_process(iter(io.open(dir_path +\"train.tokens\", encoding=\"utf8\")))\n",
    "        val_data = self.data_process(iter(io.open(dir_path +\"val.tokens\", encoding=\"utf8\")))\n",
    "        test_data = self.data_process(iter(io.open(dir_path +\"test.tokens\", encoding=\"utf8\")))\n",
    "\n",
    "    \n",
    "        self.train_dataset = AlarmDataset(train_data, self.config['seq-len'], self.config['batch-size'])\n",
    "        self.valid_dataset = AlarmDataset(val_data,self.config['seq-len'], self.config['batch-size'])\n",
    "        self.test_dataset = AlarmDataset(test_data, self.config['seq-len'], self.config['batch-size'])\n",
    "\n",
    "    \n",
    "    def data_process(self, raw_text_iter):\n",
    "        data = [torch.tensor([self.vocab[token] for token in self.tokenizer(item)],dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "    \n",
    "    def get_weight_per_class(self):\n",
    "        def lambdaFun(total,v,num_classes):\n",
    "            if v>0:\n",
    "                return total/(v*num_classes) \n",
    "            return 0\n",
    "        \n",
    "        index_2_count = {self.vocab.stoi[k]:self.vocab.freqs[k]  for k in list(self.vocab.stoi)}\n",
    "        total = sum(index_2_count.values())\n",
    "        index_2_ws = {k:lambdaFun(total,v,len(index_2_count)) for k,v in index_2_count.items()}\n",
    "        index_2_ws[1] = 0.0 # MANUALLY Setting the weights to zero for the padding\n",
    "        # index_2_ws[0] = 0.0 # MANUALLY Setting the weights to zero for the padding\n",
    "        ws = torch.tensor([index_2_ws[i] for i in range(len(index_2_ws))])\n",
    "\n",
    "        return ws\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "            Use this method to do things that might write to disk or that need to be done only from a single GPU in distributed settings.\n",
    "            e.g., download,tokenize,etc…\n",
    "        \"\"\" \n",
    "        return None\n",
    "\n",
    "\n",
    "    def setup(self, stage: None):\n",
    "        \"\"\"\n",
    "            There are also data operations you might want to perform on every GPU. Use setup to do things like:\n",
    "            count number of classes,build vocabulary,perform train/val/test splits,apply transforms (defined explicitly in your datamodule or assigned in init),etc…\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.config['batch-size'], shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.valid_dataset, batch_size=self.config['batch-size'], shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.config['batch-size'], shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlarmGRU(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self,config):\n",
    "        # super().__init__()\n",
    "        super(AlarmGRU,self).__init__()\n",
    "        self.config =config\n",
    "        self.lr = self.config['lr']\n",
    "        \n",
    "\n",
    "        self.val_CM_normalized = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"],normalize ='true')\n",
    "        self.val_CM_raw = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"])\n",
    "\n",
    "        self.train_CM_normalized = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"],normalize ='true')\n",
    "        self.train_CM_raw = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"])\n",
    "\n",
    "        self.test_CM = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"],normalize ='true')\n",
    "\n",
    "        self.val_MCR = MyClassificationReport()\n",
    "        self.test_MCR = MyClassificationReport()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ## TODO: define the layers of the model\n",
    "        self.h = None\n",
    "        self.embedding = torch.nn.Embedding(self.config['vocab-size'],self.config['em-size'])\n",
    "        self.gru = torch.nn.GRU(input_size=self.config['em-size'], hidden_size=self.config['nhid'], num_layers=self.config['nlayers'],dropout=self.config['dropout'], batch_first=True)\n",
    "        # self.droput = torch.nn.Dropout(p=self.drop_prob)\n",
    "        self.fc3 = torch.nn.Linear(in_features=self.config['nhid'], out_features=self.config['vocab-size'])\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)  \n",
    "    \n",
    "    def __init_hidden(self):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of GRU\n",
    "        device = None \n",
    "        if (torch.cuda.is_available()):\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\") \n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.config['nlayers'], self.config['batch-size'], self.config['nhid']).zero_().to(device)\n",
    "        return hidden\n",
    "\n",
    "    def initialize_hidden(self):\n",
    "        self.h = self.__init_hidden()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        out, hidden = self.gru(embeds,hidden)\n",
    "        # Contiguous variables: If you are stacking up multiple LSTM outputs, it may be necessary to use .contiguous() to reshape the output.\n",
    "        out = out.contiguous().view(-1,self.config['nhid']) \n",
    "        out = self.fc3(out)\n",
    "        out = self.softmax(out)\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr,weight_decay=self.config['weight-decay'])\n",
    "        d = {\n",
    "       'optimizer': optimizer,\n",
    "       'lr_scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = \"min\", factor = 0.5, patience=10, verbose=True),\n",
    "       'monitor': 'val_epoch_loss',\n",
    "        'interval': 'epoch'\n",
    "        }\n",
    "        return d\n",
    "\n",
    "    # def loss_function(self,logits,y):\n",
    "    #     return F.cross_entropy(logits,y,weight= self.class_weight,ignore_index=1) \n",
    "\n",
    "    def myPrintToFile(self,cm_normal,cm_raw,f):\n",
    "        cm_normal = cm_normal.cpu()\n",
    "        cm_raw = cm_raw.cpu()\n",
    "        \n",
    "\n",
    "        sum_of_each_class = cm_raw.sum(axis=1) # sum along the columns\n",
    "        print(f\"        ------ Epoch {self.current_epoch} ---------\",file=f)\n",
    "        print(f\"Total={[v.item() for v in sum_of_each_class]}\",file=f)\n",
    "        print(f\"Corret={[v.item() for v in torch.diagonal(cm_raw,0)]}\",file=f)\n",
    "        print(f\"Accuracy={[round(v.item(),3) for v in (torch.diagonal(cm_raw,0)/sum_of_each_class)]}\",file=f)\n",
    "\n",
    "        accs = [round(v.item(),3)  for v in torch.diagonal(cm_normal,0)]\n",
    "\n",
    "        source2acc = {self.config['vocab'].itos[i]:accs[i] for i in range(len(accs))}\n",
    "\n",
    "        source2_acc50 = {self.config['vocab'].itos[i]:accs[i] for i in range(len(accs)) if accs[i]>=0.5}\n",
    "\n",
    "        print(f\"Acc2={accs}\",file=f)\n",
    "        print(f\"source2_acc= {source2acc}\",file=f)\n",
    "        print(f\"source2_acc50= {source2_acc50}\",file=f)\n",
    "\n",
    "        a_50 = len([a for a in accs if a>=0.5])\n",
    "        a_30 = len([a for a in accs if a>=0.3])\n",
    "        out_str = f\"acc>0.5= {a_50}, acc>=0.3= {a_30}, Total={len(accs)}\"\n",
    "        print(out_str,file=f)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # if temp> self.accuraccy_50_count and train=:\n",
    "        #     self.accuraccy_50_count = temp\n",
    "        print(out_str,end=\" \")\n",
    "    \n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        y =  y.view(self.config['batch-size']*self.config['seq-len']).long()\n",
    "\n",
    "        self.h = self.h.data # for GRU\n",
    "        y_hat, self.h = self(x,self.h)\n",
    "        #  ignore_index=self.char2int[\"NoName\"]\n",
    "        loss = F.nll_loss(y_hat,y)\n",
    "        \n",
    "        self.train_CM_normalized(y_hat,y)\n",
    "        self.train_CM_raw(y_hat,y)    \n",
    "        # result = pl.TrainResult(loss) # logging\n",
    "        self.log('train_loss',loss,logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch, batch_idx):\n",
    "        x,y = batch\n",
    "        y =  y.view(self.config['batch-size']*self.config['seq-len']).long()\n",
    "\n",
    "        self.h = self.h.data # for GRU\n",
    "        y_hat, self.h = self(x,self.h)\n",
    "        #  ignore_index=self.char2int[\"NoName\"]\n",
    "        loss = F.nll_loss(y_hat,y)\n",
    "        \n",
    "        self.val_CM_normalized(y_hat,y)\n",
    "        self.val_CM_raw(y_hat,y)\n",
    "        self.log('val_loss',loss,logger=True)\n",
    "        return {'val_loss':loss}\n",
    "    \n",
    "    def test_step(self,batch, batch_idx):\n",
    "        x,y = batch\n",
    "        y =  y.view(self.config['batch-size']*self.config['seq-len']).long()\n",
    "\n",
    "        self.h = self.h.data # for GRU\n",
    "        y_hat, self.h = self(x,self.h)\n",
    "        #  ignore_index=self.char2int[\"NoName\"]\n",
    "        loss = F.nll_loss(y_hat,y)\n",
    "        \n",
    "        self.val_CM_normalized(y_hat,y)\n",
    "        self.val_CM_raw(y_hat,y)\n",
    "        self.log('test_loss',loss,logger=True)\n",
    "        return {'test_loss':loss}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        avg_loss = torch.stack([d['loss']  for d in outputs]).mean()\n",
    "        # f1 = self.train_F1.compute()\n",
    "        print(f\"[{self.current_epoch}]E, Avg Training loss = {round(avg_loss.item(),4)}\",end=\" \")\n",
    "        \n",
    "        with open(self.config[\"train-file\"],'a') as f:\n",
    "            self.myPrintToFile(self.train_CM_normalized.compute(),self.train_CM_raw.compute(),f)\n",
    "        self.log(\"train_epoch_loss\",avg_loss,logger=True,prog_bar=True)\n",
    "        # self.log(\"train_epoch_F1\", f1, logger=True,prog_bar=True)\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['val_loss'] for d in outputs]).mean()\n",
    "        # f1 = self.val_F1.compute()\n",
    "        # print(self.val_MCR.compute(),file=open(\"val-out.txt\",'w'))\n",
    "        # print(self.val_CM.compute(),file=open(\"val-cm-out.txt\",'w'))\n",
    "\n",
    "        # if self.current_epoch%4==0 and self.current_epoch>0:\n",
    "        # self.myPrintToFile(self.val_CM_normalized.compute(),self.val_CM_raw.compute())\n",
    "\n",
    "\n",
    "        print(f\"::Val Loss = {round(avg_loss.item(),4) }\",end=\" \")\n",
    "\n",
    "        with open(self.config[\"val-file\"],'a') as f:\n",
    "            self.myPrintToFile(self.val_CM_normalized.compute(),self.val_CM_raw.compute(),f)\n",
    "        \n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "        self.log(\"val_epoch_loss\",avg_loss,logger=True)\n",
    "        # self.log(\"val_epoch_F1\",f1,logger=True,prog_bar=True)\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['test_loss'] for d in outputs]).mean()\n",
    "        # f1 = self.test_F1.compute()\n",
    "        # print(self.test_MCR.compute(),file=open(\"test-out.txt\",'w'))\n",
    "        print(f\">Average Test Loss = {avg_loss.item()}\")\n",
    "        self.log(\"test_epoch_loss\",avg_loss, logger = True)\n",
    "        # self.log(\"test_epoch_F1\",f1, logger=True)\n",
    "    # def validation_epoch_end(self, outputs):\n",
    "    #     avg_loss = torch.stack([d['val_loss'] for d in outputs]).mean()\n",
    "        \n",
    "\n",
    "    #     # logValidationResults(net=self, predictions=y_hats, targets=ys,ignore=self.char2int[\"NoName\"] )\n",
    "    #     print(f\"> Average Valid Loss= {avg_loss}\")\n",
    "\n",
    "    #     self.log(\"val_epoch_loss\",avg_loss,logger=True)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning\n",
    "\n",
    "**Note: When monitoring any parameter after the validation epoch end then you should pass check_val_every_n_epoch=1  not to other. This is very important.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48652lines [00:00, 82947.49lines/s]\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n",
      "\n",
      "   | Name                | Type                   | Params\n",
      "----------------------------------------------------------------\n",
      "0  | val_CM_normalized   | ConfusionMatrix        | 0     \n",
      "1  | val_CM_raw          | ConfusionMatrix        | 0     \n",
      "2  | train_CM_normalized | ConfusionMatrix        | 0     \n",
      "3  | train_CM_raw        | ConfusionMatrix        | 0     \n",
      "4  | test_CM             | ConfusionMatrix        | 0     \n",
      "5  | val_MCR             | MyClassificationReport | 0     \n",
      "6  | test_MCR            | MyClassificationReport | 0     \n",
      "7  | embedding           | Embedding              | 182 K \n",
      "8  | gru                 | GRU                    | 4.3 M \n",
      "9  | fc3                 | Linear                 | 366 K \n",
      "10 | softmax             | LogSoftmax             | 0     \n",
      "----------------------------------------------------------------\n",
      "4.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.9 M     Total params\n",
      "LR finder stopped early due to diverging loss.\n",
      "Restored states from the checkpoint file at /home/waris/Github/predict-future-alarms/project/lr_find_temp_model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before [0.0, 0.0, 0.008, 0.009, 0.059, 0.061, 0.063, 0.075, 0.075, 0.075, 0.076, 0.078, 0.083, 0.098, 0.101, 0.11, 0.143, 0.157, 0.158, 0.172, 0.176, 0.18, 0.186, 0.187, 0.251, 0.269, 0.31, 0.327, 0.337, 0.344, 0.35, 0.363, 0.371, 0.377, 0.377, 0.389, 0.39, 0.404, 0.418, 0.419, 0.426, 0.428, 0.428, 0.467, 0.47, 0.476, 0.48, 0.48, 0.486, 0.492, 0.495, 0.495, 0.513, 0.531, 0.537, 0.539, 0.546, 0.576, 0.58, 0.601, 0.624, 0.629, 0.63, 0.644, 0.644, 0.654, 0.66, 0.662, 0.667, 0.673, 0.673, 0.687, 0.706, 0.707, 0.708, 0.735, 0.746, 0.749, 0.764, 0.785, 0.802, 0.803, 0.807, 0.827, 0.848, 0.904, 0.905, 0.905, 0.909, 0.911, 0.919, 0.935, 0.966, 0.976, 0.978, 0.982, 0.983, 1.004, 1.013, 1.03, 1.039, 1.05, 1.051, 1.054, 1.081, 1.082, 1.096, 1.109, 1.115, 1.124, 1.14, 1.141, 1.155, 1.16, 1.173, 1.177, 1.188, 1.198, 1.2, 1.201, 1.215, 1.235, 1.24, 1.25, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.264, 1.279, 1.283, 1.284, 1.303, 1.304, 1.316, 1.333, 1.338, 1.392, 1.398, 1.447, 1.496, 1.502, 1.508, 1.513, 1.516, 1.527, 1.547, 1.571, 1.588, 1.597, 1.645, 1.666, 1.691, 1.703, 1.703, 1.718, 1.736, 1.738, 1.747, 1.749, 1.758, 1.762, 1.809, 1.827, 1.839, 1.865, 1.867, 1.867, 1.89, 1.89, 1.895, 1.941, 1.959, 1.97, 2.023, 2.049, 2.052, 2.061, 2.109, 2.112, 2.115, 2.118, 2.173, 2.176, 2.241, 2.259, 2.259, 2.269, 2.269, 2.295, 2.302, 2.321, 2.336, 2.355, 2.355, 2.39, 2.443, 2.452, 2.538, 2.538, 2.538, 2.57, 2.57, 2.598, 2.636, 2.656, 2.656, 2.711, 2.711, 2.721, 2.727, 2.742, 2.779, 2.807, 2.807, 2.818, 2.823, 2.829, 2.829, 2.846, 2.851, 2.851, 2.857, 2.863, 2.874, 2.88, 2.963, 2.994, 3.0, 3.007, 3.007, 3.051, 3.058, 3.084, 3.111, 3.118, 3.125, 3.223, 3.223, 3.237, 3.245, 3.245, 3.274, 3.282, 3.312, 3.328, 3.335, 3.415, 3.423, 3.448, 3.464, 3.55, 3.577, 3.622, 3.687, 3.696, 3.725, 3.745, 3.804, 3.835, 3.835, 3.897, 3.908, 3.919, 3.919, 3.94, 3.951, 3.973, 3.995, 3.995, 4.04, 4.074, 4.074, 4.098, 4.098, 4.109, 4.145, 4.157, 4.169, 4.181, 4.194, 4.194, 4.194, 4.218, 4.294, 4.333, 4.413, 4.427, 4.567, 4.612, 4.641, 4.672, 4.687, 4.733, 4.797, 4.813, 4.829, 4.862, 4.928, 4.963, 4.98, 5.015, 5.122, 5.14, 5.178, 5.196, 5.234, 5.253, 5.253, 5.273, 5.273, 5.351, 5.392, 5.412, 5.433, 5.433, 5.474, 5.516, 5.537, 5.537, 5.602, 5.624, 5.624, 5.646, 5.669, 5.669, 5.691, 5.691, 5.691, 5.737, 5.737, 5.737, 5.783, 5.806, 5.83, 5.83, 5.854, 5.878, 5.926, 5.951, 5.951, 5.976, 6.001, 6.001, 6.051, 6.103, 6.103, 6.103, 6.129, 6.182, 6.209, 6.318, 6.318, 6.374, 6.431, 6.431, 6.46, 6.49, 6.549, 6.579, 6.671, 6.862, 6.895, 6.962, 6.962, 7.03, 7.065, 7.135, 7.171, 7.28, 7.317, 7.355, 7.355, 7.393, 7.47, 7.509, 7.548, 7.669, 7.669, 7.711, 7.752, 7.752, 7.794, 7.794, 7.794, 7.794, 7.837, 7.837, 7.837, 7.88, 7.88, 7.968, 7.968, 7.968, 7.968, 8.012, 8.012, 8.012, 8.057, 8.103, 8.149, 8.149, 8.195, 8.195, 8.242, 8.387, 8.387, 8.436, 8.486, 8.486, 8.486, 8.64, 8.745, 8.853, 8.908, 8.964, 9.194, 9.253, 9.253, 9.374, 9.435, 9.435, 9.435, 9.435, 9.498, 9.498, 9.498, 9.561, 9.69, 9.69, 9.69, 9.823, 9.823, 10.1, 10.1, 10.1, 10.172, 10.393, 10.393, 10.393, 10.469, 10.469, 10.545, 10.703, 10.703, 10.783, 10.865, 11.032, 11.032, 11.118, 11.205, 11.205, 11.205, 11.293, 11.382, 11.474, 11.474, 11.566, 11.66, 11.66, 11.66, 11.756, 11.756, 11.853, 11.853, 11.853, 11.952, 11.952, 11.952, 12.052, 12.154, 12.154, 12.258, 12.364, 12.364, 12.364, 12.471, 12.581, 12.581, 12.581, 12.581, 12.692, 12.692, 12.692, 12.805, 12.921, 12.921, 12.921, 13.158, 13.28, 13.28, 13.404, 13.404, 13.404, 13.404, 13.659, 13.659, 13.659, 13.659, 13.659, 13.79, 13.79, 13.79, 14.061, 14.061, 14.2, 14.2, 14.2, 14.2, 14.2, 14.2, 14.2, 14.342, 14.342, 14.342, 14.342, 14.342, 14.487, 14.487, 14.487, 14.487, 14.635, 14.635, 14.785, 14.785, 14.785, 14.785, 14.939, 14.939, 14.939, 14.939, 14.939, 14.939, 15.097, 15.257, 15.257, 15.257, 15.257, 15.421, 15.421, 15.421, 15.589, 15.589, 15.589, 15.76, 15.76, 15.76, 15.935, 16.114, 16.298, 16.298, 16.298, 16.298, 16.298, 16.485, 16.485, 16.485, 16.485, 16.485, 16.677, 16.677, 16.677, 16.677, 16.677, 17.074, 17.074, 17.074, 17.279, 17.49, 17.49, 17.706, 17.706, 17.927, 17.927, 18.154, 18.154, 18.154, 18.387, 18.387, 18.387, 18.387, 18.626, 18.626, 18.626, 18.626, 18.871, 19.123, 19.123, 19.123, 19.381, 19.381, 19.381, 19.381, 19.381, 19.646, 19.646, 19.646, 19.646, 19.646, 19.919, 19.919, 19.919, 19.919, 19.919, 19.919, 19.919, 20.2, 20.2, 20.2, 20.2, 20.2, 20.488, 20.488, 20.488, 20.785, 21.091, 21.091, 21.091, 21.091, 21.406, 21.406, 21.406, 21.73, 21.73, 21.73, 21.73, 21.73, 22.064, 22.064, 22.064, 22.064, 22.064, 22.064, 22.064, 22.064, 22.409, 22.409, 22.409, 22.409, 22.409, 22.409, 22.409, 22.765, 23.132, 23.132, 23.132, 23.132, 23.511, 23.511, 23.511, 23.903, 23.903, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.727, 24.727, 24.727, 24.727, 25.161, 25.161, 25.161, 25.61, 25.61, 25.61, 25.61, 25.61, 26.076, 26.076, 26.076, 26.076, 27.06, 27.06, 27.06, 27.06, 27.581, 27.581, 27.581, 27.581, 28.684, 29.269, 29.879, 30.515, 30.515, 31.178, 31.178, 31.178, 31.178, 31.871, 34.147, 34.98, 34.98, 35.855, 36.774, 36.774, 36.774, 47.806, 49.455, 95.613, 143.419]\n",
      "After [0.0, 0.0, 0.008, 0.009, 0.059, 0.061, 0.063, 0.075, 0.075, 0.075, 0.076, 0.078, 0.083, 0.098, 0.101, 0.11, 0.143, 0.157, 0.158, 0.172, 0.176, 0.18, 0.186, 0.187, 0.251, 0.269, 0.31, 0.327, 0.337, 0.344, 0.35, 0.363, 0.371, 0.377, 0.377, 0.389, 0.39, 0.404, 0.418, 0.419, 0.426, 0.428, 0.428, 0.467, 0.47, 0.476, 0.48, 0.48, 0.486, 0.492, 0.495, 0.495, 0.513, 0.531, 0.537, 0.539, 0.546, 0.576, 0.58, 0.601, 0.624, 0.629, 0.63, 0.644, 0.644, 0.654, 0.66, 0.662, 0.667, 0.673, 0.673, 0.687, 0.706, 0.707, 0.708, 0.735, 0.746, 0.749, 0.764, 0.785, 0.802, 0.803, 0.807, 0.827, 0.848, 0.904, 0.905, 0.905, 0.909, 0.911, 0.919, 0.935, 0.966, 0.976, 0.978, 0.982, 0.983, 1.004, 1.013, 1.03, 1.039, 1.05, 1.051, 1.054, 1.081, 1.082, 1.096, 1.109, 1.115, 1.124, 1.14, 1.141, 1.155, 1.16, 1.173, 1.177, 1.188, 1.198, 1.2, 1.201, 1.215, 1.235, 1.24, 1.25, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.264, 1.279, 1.283, 1.284, 1.303, 1.304, 1.316, 1.333, 1.338, 1.392, 1.398, 1.447, 1.496, 1.502, 1.508, 1.513, 1.516, 1.527, 1.547, 1.571, 1.588, 1.597, 1.645, 1.666, 1.691, 1.703, 1.703, 1.718, 1.736, 1.738, 1.747, 1.749, 1.758, 1.762, 1.809, 1.827, 1.839, 1.865, 1.867, 1.867, 1.89, 1.89, 1.895, 1.941, 1.959, 1.97, 2.023, 2.049, 2.052, 2.061, 2.109, 2.112, 2.115, 2.118, 2.173, 2.176, 2.241, 2.259, 2.259, 2.269, 2.269, 2.295, 2.302, 2.321, 2.336, 2.355, 2.355, 2.39, 2.443, 2.452, 2.538, 2.538, 2.538, 2.57, 2.57, 2.598, 2.636, 2.656, 2.656, 2.711, 2.711, 2.721, 2.727, 2.742, 2.779, 2.807, 2.807, 2.818, 2.823, 2.829, 2.829, 2.846, 2.851, 2.851, 2.857, 2.863, 2.874, 2.88, 2.963, 2.994, 3.0, 3.007, 3.007, 3.051, 3.058, 3.084, 3.111, 3.118, 3.125, 3.223, 3.223, 3.237, 3.245, 3.245, 3.274, 3.282, 3.312, 3.328, 3.335, 3.415, 3.423, 3.448, 3.464, 3.55, 3.577, 3.622, 3.687, 3.696, 3.725, 3.745, 3.804, 3.835, 3.835, 3.897, 3.908, 3.919, 3.919, 3.94, 3.951, 3.973, 3.995, 3.995, 4.04, 4.074, 4.074, 4.098, 4.098, 4.109, 4.145, 4.157, 4.169, 4.181, 4.194, 4.194, 4.194, 4.218, 4.294, 4.333, 4.413, 4.427, 4.567, 4.612, 4.641, 4.672, 4.687, 4.733, 4.797, 4.813, 4.829, 4.862, 4.928, 4.963, 4.98, 5.015, 5.122, 5.14, 5.178, 5.196, 5.234, 5.253, 5.253, 5.273, 5.273, 5.351, 5.392, 5.412, 5.433, 5.433, 5.474, 5.516, 5.537, 5.537, 5.602, 5.624, 5.624, 5.646, 5.669, 5.669, 5.691, 5.691, 5.691, 5.737, 5.737, 5.737, 5.783, 5.806, 5.83, 5.83, 5.854, 5.878, 5.926, 5.951, 5.951, 5.976, 6.001, 6.001, 6.051, 6.103, 6.103, 6.103, 6.129, 6.182, 6.209, 6.318, 6.318, 6.374, 6.431, 6.431, 6.46, 6.49, 6.549, 6.579, 6.671, 6.862, 6.895, 6.962, 6.962, 7.03, 7.065, 7.135, 7.171, 7.28, 7.317, 7.355, 7.355, 7.393, 7.47, 7.509, 7.548, 7.669, 7.669, 7.711, 7.752, 7.752, 7.794, 7.794, 7.794, 7.794, 7.837, 7.837, 7.837, 7.88, 7.88, 7.968, 7.968, 7.968, 7.968, 8.012, 8.012, 8.012, 8.057, 8.103, 8.149, 8.149, 8.195, 8.195, 8.242, 8.387, 8.387, 8.436, 8.486, 8.486, 8.486, 8.64, 8.745, 8.853, 8.908, 8.964, 9.194, 9.253, 9.253, 9.374, 9.435, 9.435, 9.435, 9.435, 9.498, 9.498, 9.498, 9.561, 9.69, 9.69, 9.69, 9.823, 9.823, 10.1, 10.1, 10.1, 10.172, 10.393, 10.393, 10.393, 10.469, 10.469, 10.545, 10.703, 10.703, 10.783, 10.865, 11.032, 11.032, 11.118, 11.205, 11.205, 11.205, 11.293, 11.382, 11.474, 11.474, 11.566, 11.66, 11.66, 11.66, 11.756, 11.756, 11.853, 11.853, 11.853, 11.952, 11.952, 11.952, 12.052, 12.154, 12.154, 12.258, 12.364, 12.364, 12.364, 12.471, 12.581, 12.581, 12.581, 12.581, 12.692, 12.692, 12.692, 12.805, 12.921, 12.921, 12.921, 13.158, 13.28, 13.28, 13.404, 13.404, 13.404, 13.404, 13.659, 13.659, 13.659, 13.659, 13.659, 13.79, 13.79, 13.79, 14.061, 14.061, 14.2, 14.2, 14.2, 14.2, 14.2, 14.2, 14.2, 14.342, 14.342, 14.342, 14.342, 14.342, 14.487, 14.487, 14.487, 14.487, 14.635, 14.635, 14.785, 14.785, 14.785, 14.785, 14.939, 14.939, 14.939, 14.939, 14.939, 14.939, 15.097, 15.257, 15.257, 15.257, 15.257, 15.421, 15.421, 15.421, 15.589, 15.589, 15.589, 15.76, 15.76, 15.76, 15.935, 16.114, 16.298, 16.298, 16.298, 16.298, 16.298, 16.485, 16.485, 16.485, 16.485, 16.485, 16.677, 16.677, 16.677, 16.677, 16.677, 17.074, 17.074, 17.074, 17.279, 17.49, 17.49, 17.706, 17.706, 17.927, 17.927, 18.154, 18.154, 18.154, 18.387, 18.387, 18.387, 18.387, 18.626, 18.626, 18.626, 18.626, 18.871, 19.123, 19.123, 19.123, 19.381, 19.381, 19.381, 19.381, 19.381, 19.646, 19.646, 19.646, 19.646, 19.646, 19.919, 19.919, 19.919, 19.919, 19.919, 19.919, 19.919, 20.2, 20.2, 20.2, 20.2, 20.2, 20.488, 20.488, 20.488, 20.785, 21.091, 21.091, 21.091, 21.091, 21.406, 21.406, 21.406, 21.73, 21.73, 21.73, 21.73, 21.73, 22.064, 22.064, 22.064, 22.064, 22.064, 22.064, 22.064, 22.064, 22.409, 22.409, 22.409, 22.409, 22.409, 22.409, 22.409, 22.765, 23.132, 23.132, 23.132, 23.132, 23.511, 23.511, 23.511, 23.903, 23.903, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.727, 24.727, 24.727, 24.727, 25.161, 25.161, 25.161, 25.61, 25.61, 25.61, 25.61, 25.61, 26.076, 26.076, 26.076, 26.076, 27.06, 27.06, 27.06, 27.06, 27.581, 27.581, 27.581, 27.581, 28.684, 29.269, 29.879, 30.515, 30.515, 31.178, 31.178, 31.178, 31.178, 31.871, 34.147, 34.98, 34.98, 35.855, 36.774, 36.774, 36.774, 47.806, 49.455, 95.613, 143.419]\n",
      "::Val Loss = 6.5572 acc>0.5= 0, acc>=0.3= 0, Total=714 \n",
      "::Val Loss = 6.5707 acc>0.5= 0, acc>=0.3= 0, Total=714 \n",
      "[0]E, Avg Training loss = 6.5714 acc>0.5= 0, acc>=0.3= 0, Total=714 ::Val Loss = 6.5654 acc>0.5= 0, acc>=0.3= 0, Total=714 \n",
      "[1]E, Avg Training loss = 6.5696 acc>0.5= 0, acc>=0.3= 0, Total=714 ::Val Loss = 6.4798 acc>0.5= 2, acc>=0.3= 2, Total=714 \n",
      "[2]E, Avg Training loss = 6.541 acc>0.5= 0, acc>=0.3= 0, Total=714 ::Val Loss = 4.8044 acc>0.5= 3, acc>=0.3= 3, Total=714 \n",
      "[3]E, Avg Training loss = 5.9795 acc>0.5= 2, acc>=0.3= 2, Total=714 ::Val Loss = 6.1867 acc>0.5= 1, acc>=0.3= 2, Total=714 \n",
      "[4]E, Avg Training loss = 4.5881 acc>0.5= 2, acc>=0.3= 3, Total=714 ::Val Loss = 43.3006 acc>0.5= 1, acc>=0.3= 1, Total=714 \n",
      "[5]E, Avg Training loss = 9.9841 acc>0.5= 0, acc>=0.3= 1, Total=714 \n",
      "[6]E, Avg Training loss = 86.9071 acc>0.5= 0, acc>=0.3= 0, Total=714 Suggested lr = 0.001584893192461114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee219ad58e6c44a68d887db10af233c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Finding best initial lr'), FloatProgress(value=0.0), HTML(value='')))"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light",
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def weightCondition(w,avg_w):\n",
    "    if w<avg_w:\n",
    "        return w\n",
    "    else:\n",
    "        return avg_w\n",
    "\n",
    "# setup data\n",
    "config_data = {\n",
    "'dir-path' : \"../.data/\",\n",
    "'batch-size' :512, # Batch Size \n",
    "'seq-len' :128, # Sequence length\n",
    "}\n",
    "\n",
    "dm = MyDataModule(config=config_data)\n",
    "ws = dm.get_weight_per_class().cuda()\n",
    "\n",
    "print(\"Before\",[round(w.item(),3) for w in ws])\n",
    "# avg_w = sum(ws)/len(ws)\n",
    "# ws = torch.tensor([weightCondition(w,avg_w) for w in ws]).cuda()\n",
    "print(\"After\",[round(w.item(),3) for w in ws])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config_model = {\n",
    "    'lr' : 0.001,\n",
    "    'dropout' : 0.1,\n",
    "    'weight-decay': 0.1, #3.1,\n",
    "    'em-size' :256, # embedding dimension \n",
    "    'nhid' : 512, # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "    'nlayers' :3, # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    'seq-len': config_data['seq-len'], # dont use wandb config \n",
    "    'vocab-size':len(dm.vocab.stoi), # the size of vocabulary /also called tokens\n",
    "    'weight_per_class':ws,\n",
    "    \"val-file\":\"val-out-rnn.txt\",\n",
    "    \"train-file\":'train-out-rnn.txt',\n",
    "    \"vocab\": dm.vocab,\n",
    "    \"batch-size\":config_data['batch-size']\n",
    "}\n",
    "\n",
    "with open (config_model[\"val-file\"],'w') as f:\n",
    "    f.write(\">> Starting\")\n",
    "\n",
    "with open (config_model[\"train-file\"],'w') as f:\n",
    "    f.write(\">> Starting\")\n",
    "\n",
    "\n",
    "\n",
    "model = AlarmGRU(config=config_model)\n",
    "model.initialize_hidden()\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "        monitor='val_epoch_loss',\n",
    "        min_delta=0,\n",
    "        patience=100,\n",
    "        verbose=True,\n",
    "        mode='min'\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(auto_lr_find=0.0001, precision=16,gpus=-1, num_nodes=1,  max_epochs=500, check_val_every_n_epoch=1,deterministic=True,gradient_clip_val=0.5,enable_pl_optimizer=True,callbacks=[early_stop_callback],progress_bar_refresh_rate=0)\n",
    "\n",
    "# Run learning rate finder\n",
    "lr_finder = trainer.tuner.lr_find(model,dm)\n",
    "\n",
    "# Results can be found in\n",
    "lr_finder.results\n",
    "\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()\n",
    "\n",
    "# Pick point based on plot, or get suggestion\n",
    "new_lr = lr_finder.suggestion()\n",
    "\n",
    "print(f\"Suggested lr = {new_lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                | Type                   | Params\n",
      "----------------------------------------------------------------\n",
      "0  | val_CM_normalized   | ConfusionMatrix        | 0     \n",
      "1  | val_CM_raw          | ConfusionMatrix        | 0     \n",
      "2  | train_CM_normalized | ConfusionMatrix        | 0     \n",
      "3  | train_CM_raw        | ConfusionMatrix        | 0     \n",
      "4  | test_CM             | ConfusionMatrix        | 0     \n",
      "5  | val_MCR             | MyClassificationReport | 0     \n",
      "6  | test_MCR            | MyClassificationReport | 0     \n",
      "7  | embedding           | Embedding              | 182 K \n",
      "8  | gru                 | GRU                    | 4.3 M \n",
      "9  | fc3                 | Linear                 | 366 K \n",
      "10 | softmax             | LogSoftmax             | 0     \n",
      "----------------------------------------------------------------\n",
      "4.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.9 M     Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::Val Loss = 6.5536 acc>0.5= 0, acc>=0.3= 0, Total=714 \n",
      "::Val Loss = 4.4577 acc>0.5= 3, acc>=0.3= 4, Total=714 \n",
      "[1]E, Avg Training loss = 5.3634 acc>0.5= 1, acc>=0.3= 2, Total=714 ::Val Loss = 3.7966 acc>0.5= 4, acc>=0.3= 6, Total=714 \n",
      "[2]E, Avg Training loss = 4.0563 acc>0.5= 3, acc>=0.3= 5, Total=714 ::Val Loss = 4.0364 acc>0.5= 4, acc>=0.3= 5, Total=714 \n",
      "[3]E, Avg Training loss = 3.6933 acc>0.5= 4, acc>=0.3= 4, Total=714 ::Val Loss = 3.5289 acc>0.5= 6, acc>=0.3= 12, Total=714 \n",
      "[4]E, Avg Training loss = 3.6874 acc>0.5= 6, acc>=0.3= 8, Total=714 ::Val Loss = 3.3934 acc>0.5= 7, acc>=0.3= 10, Total=714 \n",
      "[5]E, Avg Training loss = 3.3994 acc>0.5= 8, acc>=0.3= 12, Total=714 ::Val Loss = 3.2856 acc>0.5= 6, acc>=0.3= 12, Total=714 \n",
      "[6]E, Avg Training loss = 3.2029 acc>0.5= 10, acc>=0.3= 15, Total=714 ::Val Loss = 3.0792 acc>0.5= 9, acc>=0.3= 19, Total=714 \n",
      "[7]E, Avg Training loss = 3.1134 acc>0.5= 12, acc>=0.3= 21, Total=714 ::Val Loss = 3.0738 acc>0.5= 10, acc>=0.3= 17, Total=714 \n",
      "[8]E, Avg Training loss = 2.9377 acc>0.5= 16, acc>=0.3= 31, Total=714 ::Val Loss = 3.1205 acc>0.5= 14, acc>=0.3= 26, Total=714 \n",
      "[9]E, Avg Training loss = 2.9468 acc>0.5= 21, acc>=0.3= 38, Total=714 ::Val Loss = 3.136 acc>0.5= 12, acc>=0.3= 24, Total=714 \n",
      "[10]E, Avg Training loss = 2.9348 acc>0.5= 26, acc>=0.3= 40, Total=714 ::Val Loss = 2.9654 acc>0.5= 16, acc>=0.3= 38, Total=714 \n",
      "[11]E, Avg Training loss = 2.86 acc>0.5= 25, acc>=0.3= 48, Total=714 ::Val Loss = 2.9036 acc>0.5= 13, acc>=0.3= 27, Total=714 \n",
      "[12]E, Avg Training loss = 2.7294 acc>0.5= 29, acc>=0.3= 56, Total=714 ::Val Loss = 2.9605 acc>0.5= 20, acc>=0.3= 37, Total=714 \n",
      "[13]E, Avg Training loss = 2.7449 acc>0.5= 34, acc>=0.3= 58, Total=714 ::Val Loss = 2.792 acc>0.5= 22, acc>=0.3= 41, Total=714 \n",
      "[14]E, Avg Training loss = 2.7513 acc>0.5= 43, acc>=0.3= 72, Total=714 ::Val Loss = 2.7488 acc>0.5= 21, acc>=0.3= 43, Total=714 \n",
      "[15]E, Avg Training loss = 2.5846 acc>0.5= 42, acc>=0.3= 80, Total=714 ::Val Loss = 2.7511 acc>0.5= 27, acc>=0.3= 54, Total=714 \n",
      "[16]E, Avg Training loss = 2.5895 acc>0.5= 42, acc>=0.3= 72, Total=714 ::Val Loss = 2.697 acc>0.5= 30, acc>=0.3= 51, Total=714 \n",
      "[17]E, Avg Training loss = 2.5916 acc>0.5= 50, acc>=0.3= 97, Total=714 ::Val Loss = 2.6506 acc>0.5= 32, acc>=0.3= 61, Total=714 \n",
      "[18]E, Avg Training loss = 2.453 acc>0.5= 47, acc>=0.3= 83, Total=714 ::Val Loss = 2.675 acc>0.5= 34, acc>=0.3= 53, Total=714 \n",
      "[19]E, Avg Training loss = 2.482 acc>0.5= 51, acc>=0.3= 98, Total=714 ::Val Loss = 2.6058 acc>0.5= 37, acc>=0.3= 66, Total=714 \n",
      "[20]E, Avg Training loss = 2.4175 acc>0.5= 46, acc>=0.3= 95, Total=714 ::Val Loss = 2.6249 acc>0.5= 35, acc>=0.3= 61, Total=714 \n",
      "[21]E, Avg Training loss = 2.5007 acc>0.5= 59, acc>=0.3= 104, Total=714 ::Val Loss = 2.5906 acc>0.5= 37, acc>=0.3= 69, Total=714 \n",
      "[22]E, Avg Training loss = 2.3534 acc>0.5= 48, acc>=0.3= 99, Total=714 ::Val Loss = 2.534 acc>0.5= 38, acc>=0.3= 65, Total=714 \n",
      "[23]E, Avg Training loss = 2.4247 acc>0.5= 66, acc>=0.3= 128, Total=714 ::Val Loss = 2.5656 acc>0.5= 38, acc>=0.3= 74, Total=714 \n",
      "[24]E, Avg Training loss = 2.2763 acc>0.5= 52, acc>=0.3= 113, Total=714 ::Val Loss = 2.513 acc>0.5= 42, acc>=0.3= 74, Total=714 \n",
      "[25]E, Avg Training loss = 2.3638 acc>0.5= 64, acc>=0.3= 131, Total=714 ::Val Loss = 2.5049 acc>0.5= 42, acc>=0.3= 73, Total=714 \n",
      "[26]E, Avg Training loss = 2.244 acc>0.5= 59, acc>=0.3= 122, Total=714 ::Val Loss = 2.4994 acc>0.5= 44, acc>=0.3= 86, Total=714 \n",
      "[27]E, Avg Training loss = 2.321 acc>0.5= 61, acc>=0.3= 123, Total=714 ::Val Loss = 2.477 acc>0.5= 42, acc>=0.3= 76, Total=714 \n",
      "[28]E, Avg Training loss = 2.2351 acc>0.5= 62, acc>=0.3= 127, Total=714 ::Val Loss = 2.4827 acc>0.5= 42, acc>=0.3= 85, Total=714 \n",
      "[29]E, Avg Training loss = 2.24 acc>0.5= 63, acc>=0.3= 135, Total=714 ::Val Loss = 2.4599 acc>0.5= 38, acc>=0.3= 79, Total=714 \n",
      "[30]E, Avg Training loss = 2.2449 acc>0.5= 70, acc>=0.3= 146, Total=714 ::Val Loss = 2.4539 acc>0.5= 47, acc>=0.3= 102, Total=714 \n",
      "[31]E, Avg Training loss = 2.2524 acc>0.5= 69, acc>=0.3= 150, Total=714 ::Val Loss = 2.4397 acc>0.5= 39, acc>=0.3= 82, Total=714 \n",
      "[32]E, Avg Training loss = 2.1804 acc>0.5= 65, acc>=0.3= 150, Total=714 ::Val Loss = 2.4279 acc>0.5= 43, acc>=0.3= 93, Total=714 \n",
      "[33]E, Avg Training loss = 2.2106 acc>0.5= 80, acc>=0.3= 158, Total=714 ::Val Loss = 2.425 acc>0.5= 48, acc>=0.3= 92, Total=714 \n",
      "[34]E, Avg Training loss = 2.1698 acc>0.5= 65, acc>=0.3= 145, Total=714 ::Val Loss = 2.4096 acc>0.5= 49, acc>=0.3= 94, Total=714 \n",
      "[35]E, Avg Training loss = 2.1953 acc>0.5= 85, acc>=0.3= 172, Total=714 ::Val Loss = 2.4084 acc>0.5= 45, acc>=0.3= 92, Total=714 \n",
      "[36]E, Avg Training loss = 2.1156 acc>0.5= 72, acc>=0.3= 156, Total=714 ::Val Loss = 2.3894 acc>0.5= 50, acc>=0.3= 97, Total=714 \n",
      "[37]E, Avg Training loss = 2.167 acc>0.5= 79, acc>=0.3= 164, Total=714 ::Val Loss = 2.3697 acc>0.5= 45, acc>=0.3= 101, Total=714 \n",
      "[38]E, Avg Training loss = 2.135 acc>0.5= 75, acc>=0.3= 165, Total=714 ::Val Loss = 2.3679 acc>0.5= 47, acc>=0.3= 101, Total=714 \n",
      "[39]E, Avg Training loss = 2.1311 acc>0.5= 83, acc>=0.3= 183, Total=714 ::Val Loss = 2.3677 acc>0.5= 49, acc>=0.3= 104, Total=714 \n",
      "[40]E, Avg Training loss = 2.0884 acc>0.5= 78, acc>=0.3= 166, Total=714 ::Val Loss = 2.3635 acc>0.5= 54, acc>=0.3= 106, Total=714 \n",
      "[41]E, Avg Training loss = 2.1136 acc>0.5= 93, acc>=0.3= 202, Total=714 ::Val Loss = 2.3645 acc>0.5= 50, acc>=0.3= 107, Total=714 \n",
      "[42]E, Avg Training loss = 2.0819 acc>0.5= 81, acc>=0.3= 168, Total=714 ::Val Loss = 2.3401 acc>0.5= 51, acc>=0.3= 115, Total=714 \n",
      "[43]E, Avg Training loss = 2.0976 acc>0.5= 101, acc>=0.3= 201, Total=714 ::Val Loss = 2.3687 acc>0.5= 51, acc>=0.3= 102, Total=714 \n",
      "[44]E, Avg Training loss = 2.0448 acc>0.5= 88, acc>=0.3= 194, Total=714 ::Val Loss = 2.3502 acc>0.5= 56, acc>=0.3= 109, Total=714 \n",
      "[45]E, Avg Training loss = 2.0942 acc>0.5= 93, acc>=0.3= 197, Total=714 ::Val Loss = 2.3495 acc>0.5= 50, acc>=0.3= 104, Total=714 \n",
      "[46]E, Avg Training loss = 2.0474 acc>0.5= 94, acc>=0.3= 195, Total=714 ::Val Loss = 2.3286 acc>0.5= 48, acc>=0.3= 112, Total=714 \n",
      "[47]E, Avg Training loss = 2.0818 acc>0.5= 98, acc>=0.3= 210, Total=714 ::Val Loss = 2.3368 acc>0.5= 49, acc>=0.3= 113, Total=714 \n",
      "[48]E, Avg Training loss = 2.0172 acc>0.5= 93, acc>=0.3= 194, Total=714 ::Val Loss = 2.3289 acc>0.5= 56, acc>=0.3= 118, Total=714 \n",
      "[49]E, Avg Training loss = 2.0546 acc>0.5= 99, acc>=0.3= 208, Total=714 ::Val Loss = 2.3225 acc>0.5= 51, acc>=0.3= 109, Total=714 \n",
      "[50]E, Avg Training loss = 2.0187 acc>0.5= 89, acc>=0.3= 203, Total=714 ::Val Loss = 2.3415 acc>0.5= 53, acc>=0.3= 123, Total=714 \n",
      "[51]E, Avg Training loss = 2.0448 acc>0.5= 94, acc>=0.3= 224, Total=714 ::Val Loss = 2.3006 acc>0.5= 52, acc>=0.3= 119, Total=714 \n",
      "[52]E, Avg Training loss = 1.9883 acc>0.5= 99, acc>=0.3= 222, Total=714 ::Val Loss = 2.3165 acc>0.5= 53, acc>=0.3= 121, Total=714 \n",
      "[53]E, Avg Training loss = 2.0028 acc>0.5= 101, acc>=0.3= 221, Total=714 ::Val Loss = 2.318 acc>0.5= 54, acc>=0.3= 109, Total=714 \n",
      "[54]E, Avg Training loss = 1.9995 acc>0.5= 95, acc>=0.3= 213, Total=714 ::Val Loss = 2.2993 acc>0.5= 55, acc>=0.3= 117, Total=714 \n",
      "[55]E, Avg Training loss = 2.0159 acc>0.5= 106, acc>=0.3= 225, Total=714 ::Val Loss = 2.2769 acc>0.5= 54, acc>=0.3= 114, Total=714 \n",
      "[56]E, Avg Training loss = 1.9724 acc>0.5= 100, acc>=0.3= 213, Total=714 ::Val Loss = 2.308 acc>0.5= 52, acc>=0.3= 112, Total=714 \n",
      "[57]E, Avg Training loss = 2.0114 acc>0.5= 112, acc>=0.3= 226, Total=714 ::Val Loss = 2.2634 acc>0.5= 55, acc>=0.3= 116, Total=714 \n",
      "[58]E, Avg Training loss = 1.9595 acc>0.5= 107, acc>=0.3= 237, Total=714 ::Val Loss = 2.301 acc>0.5= 53, acc>=0.3= 119, Total=714 \n",
      "[59]E, Avg Training loss = 1.9865 acc>0.5= 104, acc>=0.3= 230, Total=714 ::Val Loss = 2.271 acc>0.5= 53, acc>=0.3= 114, Total=714 \n",
      "[60]E, Avg Training loss = 1.9477 acc>0.5= 116, acc>=0.3= 247, Total=714 ::Val Loss = 2.2863 acc>0.5= 58, acc>=0.3= 131, Total=714 \n",
      "[61]E, Avg Training loss = 1.9829 acc>0.5= 107, acc>=0.3= 229, Total=714 ::Val Loss = 2.2821 acc>0.5= 50, acc>=0.3= 112, Total=714 \n",
      "[62]E, Avg Training loss = 1.9443 acc>0.5= 104, acc>=0.3= 239, Total=714 ::Val Loss = 2.2766 acc>0.5= 58, acc>=0.3= 134, Total=714 \n",
      "[63]E, Avg Training loss = 1.9686 acc>0.5= 108, acc>=0.3= 235, Total=714 ::Val Loss = 2.2993 acc>0.5= 43, acc>=0.3= 98, Total=714 \n",
      "[64]E, Avg Training loss = 1.919 acc>0.5= 104, acc>=0.3= 232, Total=714 ::Val Loss = 2.3025 acc>0.5= 55, acc>=0.3= 126, Total=714 \n",
      "[65]E, Avg Training loss = 1.9887 acc>0.5= 113, acc>=0.3= 233, Total=714 ::Val Loss = 2.2638 acc>0.5= 54, acc>=0.3= 114, Total=714 \n",
      "[66]E, Avg Training loss = 1.9392 acc>0.5= 109, acc>=0.3= 229, Total=714 ::Val Loss = 2.2925 acc>0.5= 51, acc>=0.3= 120, Total=714 \n",
      "[67]E, Avg Training loss = 1.9631 acc>0.5= 122, acc>=0.3= 249, Total=714 ::Val Loss = 2.2501 acc>0.5= 52, acc>=0.3= 118, Total=714 \n",
      "[68]E, Avg Training loss = 1.9134 acc>0.5= 118, acc>=0.3= 247, Total=714 ::Val Loss = 2.2666 acc>0.5= 57, acc>=0.3= 122, Total=714 \n",
      "[69]E, Avg Training loss = 1.9191 acc>0.5= 121, acc>=0.3= 263, Total=714 ::Val Loss = 2.2529 acc>0.5= 53, acc>=0.3= 115, Total=714 \n",
      "[70]E, Avg Training loss = 1.8876 acc>0.5= 133, acc>=0.3= 259, Total=714 ::Val Loss = 2.2624 acc>0.5= 59, acc>=0.3= 133, Total=714 \n",
      "[71]E, Avg Training loss = 1.9002 acc>0.5= 133, acc>=0.3= 261, Total=714 ::Val Loss = 2.2781 acc>0.5= 55, acc>=0.3= 110, Total=714 \n",
      "[72]E, Avg Training loss = 1.8847 acc>0.5= 117, acc>=0.3= 257, Total=714 ::Val Loss = 2.2587 acc>0.5= 53, acc>=0.3= 128, Total=714 \n",
      "[73]E, Avg Training loss = 1.9098 acc>0.5= 133, acc>=0.3= 257, Total=714 ::Val Loss = 2.268 acc>0.5= 56, acc>=0.3= 119, Total=714 \n",
      "[74]E, Avg Training loss = 1.8894 acc>0.5= 120, acc>=0.3= 261, Total=714 ::Val Loss = 2.2755 acc>0.5= 53, acc>=0.3= 118, Total=714 \n",
      "[75]E, Avg Training loss = 1.9038 acc>0.5= 141, acc>=0.3= 274, Total=714 ::Val Loss = 2.2675 acc>0.5= 61, acc>=0.3= 134, Total=714 \n",
      "[76]E, Avg Training loss = 1.8774 acc>0.5= 125, acc>=0.3= 256, Total=714 ::Val Loss = 2.2973 acc>0.5= 48, acc>=0.3= 108, Total=714 \n",
      "[77]E, Avg Training loss = 1.9257 acc>0.5= 137, acc>=0.3= 278, Total=714 ::Val Loss = 2.2908 acc>0.5= 60, acc>=0.3= 127, Total=714 \n",
      "[78]E, Avg Training loss = 1.8825 acc>0.5= 122, acc>=0.3= 246, Total=714 ::Val Loss = 2.2692 acc>0.5= 49, acc>=0.3= 119, Total=714 \n",
      "[79]E, Avg Training loss = 1.9124 acc>0.5= 150, acc>=0.3= 271, Total=714 Epoch    79: reducing learning rate of group 0 to 5.0000e-04.\n",
      "::Val Loss = 2.226 acc>0.5= 55, acc>=0.3= 113, Total=714 \n",
      "[80]E, Avg Training loss = 1.8407 acc>0.5= 138, acc>=0.3= 270, Total=714 ::Val Loss = 2.2184 acc>0.5= 53, acc>=0.3= 133, Total=714 \n",
      "[81]E, Avg Training loss = 1.8018 acc>0.5= 144, acc>=0.3= 285, Total=714 ::Val Loss = 2.2169 acc>0.5= 60, acc>=0.3= 114, Total=714 \n",
      "[82]E, Avg Training loss = 1.7797 acc>0.5= 149, acc>=0.3= 294, Total=714 ::Val Loss = 2.2176 acc>0.5= 51, acc>=0.3= 130, Total=714 \n",
      "[83]E, Avg Training loss = 1.7789 acc>0.5= 154, acc>=0.3= 295, Total=714 ::Val Loss = 2.2143 acc>0.5= 57, acc>=0.3= 116, Total=714 \n",
      "[84]E, Avg Training loss = 1.767 acc>0.5= 149, acc>=0.3= 292, Total=714 ::Val Loss = 2.2174 acc>0.5= 56, acc>=0.3= 122, Total=714 \n",
      "[85]E, Avg Training loss = 1.761 acc>0.5= 159, acc>=0.3= 308, Total=714 ::Val Loss = 2.2212 acc>0.5= 51, acc>=0.3= 115, Total=714 \n",
      "[86]E, Avg Training loss = 1.7589 acc>0.5= 151, acc>=0.3= 307, Total=714 ::Val Loss = 2.2104 acc>0.5= 59, acc>=0.3= 126, Total=714 \n",
      "[87]E, Avg Training loss = 1.7502 acc>0.5= 162, acc>=0.3= 304, Total=714 ::Val Loss = 2.226 acc>0.5= 51, acc>=0.3= 112, Total=714 \n",
      "[88]E, Avg Training loss = 1.7452 acc>0.5= 157, acc>=0.3= 298, Total=714 ::Val Loss = 2.2139 acc>0.5= 54, acc>=0.3= 121, Total=714 \n",
      "[89]E, Avg Training loss = 1.7467 acc>0.5= 167, acc>=0.3= 313, Total=714 ::Val Loss = 2.2272 acc>0.5= 53, acc>=0.3= 108, Total=714 \n",
      "[90]E, Avg Training loss = 1.7405 acc>0.5= 166, acc>=0.3= 302, Total=714 ::Val Loss = 2.218 acc>0.5= 55, acc>=0.3= 120, Total=714 \n",
      "[91]E, Avg Training loss = 1.7456 acc>0.5= 173, acc>=0.3= 315, Total=714 ::Val Loss = 2.2219 acc>0.5= 52, acc>=0.3= 116, Total=714 \n",
      "[92]E, Avg Training loss = 1.7373 acc>0.5= 163, acc>=0.3= 302, Total=714 ::Val Loss = 2.2227 acc>0.5= 55, acc>=0.3= 117, Total=714 \n",
      "[93]E, Avg Training loss = 1.7414 acc>0.5= 177, acc>=0.3= 327, Total=714 ::Val Loss = 2.2254 acc>0.5= 55, acc>=0.3= 118, Total=714 \n",
      "[94]E, Avg Training loss = 1.7336 acc>0.5= 162, acc>=0.3= 299, Total=714 ::Val Loss = 2.2288 acc>0.5= 48, acc>=0.3= 117, Total=714 \n",
      "[95]E, Avg Training loss = 1.7383 acc>0.5= 177, acc>=0.3= 323, Total=714 ::Val Loss = 2.2352 acc>0.5= 55, acc>=0.3= 113, Total=714 \n",
      "[96]E, Avg Training loss = 1.7291 acc>0.5= 168, acc>=0.3= 305, Total=714 ::Val Loss = 2.2338 acc>0.5= 45, acc>=0.3= 117, Total=714 \n",
      "[97]E, Avg Training loss = 1.7369 acc>0.5= 193, acc>=0.3= 330, Total=714 ::Val Loss = 2.236 acc>0.5= 59, acc>=0.3= 116, Total=714 \n",
      "[98]E, Avg Training loss = 1.7259 acc>0.5= 165, acc>=0.3= 309, Total=714 Epoch    98: reducing learning rate of group 0 to 2.5000e-04.\n",
      "::Val Loss = 2.2158 acc>0.5= 51, acc>=0.3= 120, Total=714 \n",
      "[99]E, Avg Training loss = 1.71 acc>0.5= 191, acc>=0.3= 320, Total=714 ::Val Loss = 2.2105 acc>0.5= 51, acc>=0.3= 112, Total=714 \n",
      "[100]E, Avg Training loss = 1.6821 acc>0.5= 188, acc>=0.3= 339, Total=714 ::Val Loss = 2.2148 acc>0.5= 48, acc>=0.3= 114, Total=714 \n",
      "[101]E, Avg Training loss = 1.6776 acc>0.5= 198, acc>=0.3= 341, Total=714 ::Val Loss = 2.2106 acc>0.5= 52, acc>=0.3= 113, Total=714 \n",
      "[102]E, Avg Training loss = 1.6732 acc>0.5= 194, acc>=0.3= 339, Total=714 ::Val Loss = 2.2165 acc>0.5= 47, acc>=0.3= 114, Total=714 \n",
      "[103]E, Avg Training loss = 1.6715 acc>0.5= 196, acc>=0.3= 345, Total=714 ::Val Loss = 2.21 acc>0.5= 52, acc>=0.3= 116, Total=714 \n",
      "[104]E, Avg Training loss = 1.669 acc>0.5= 194, acc>=0.3= 346, Total=714 ::Val Loss = 2.2168 acc>0.5= 46, acc>=0.3= 111, Total=714 \n",
      "[105]E, Avg Training loss = 1.6671 acc>0.5= 200, acc>=0.3= 345, Total=714 "
     ]
    }
   ],
   "source": [
    "model.hparams.lr = new_lr/100 #7.5e-12 # can devide by 10\n",
    "trainer.fit(model,dm) # Fit model\n",
    "trainer.test(datamodule=dm) # testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('dl': conda)",
   "language": "python",
   "name": "python38564bitdlconda5312cd9c08564c7aafb6787e3983aeca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}