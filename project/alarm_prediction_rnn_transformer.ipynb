{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Definition\n",
    "Recurrent Neural Networks (RNNs)  have shown a lot of potential in many natural language processing (NLP) and sequence learning tasks. The main idea behind sequential learning is that they make use of sequential information. In traditional feed-forward neural networks such as Convolutional Neural Networks (CNNs), it is assumed that all inputs are independent of each other, e.g., in an image classification task, the pixels of an image are independent of each other. However, this approach is not valid for sequence learning tasks. For instance, if you want to predict the next word in a sequence (sentence), you need the prior information (i.e., previous words in the sentence) to do so. RNNs can remember previous states (information), i.e., RNNs have a “memory” in the form of hidden states which store information about what has been processed so far. At each input of the sequence, the model not only takes the current input but also remembers the preceding information. Like the human way of processing sequential information, this allows the model to learn long-term dependencies in the sequence, i.e., it considers the entire context when making a prediction. \n",
    "\n",
    "Prediction of the next alarm can also be modelled as a sequence learning task: given a sorted sequence of alarms based on their start time, predict the upcoming alarm. In this report, we compared a RNN architecture and a Transfomrer architecture in terms of accuracy of predicitng next alarm. \n",
    "\n",
    "By predicting future alarms in real-time with the help of the AI module, the operator may avert abnormal situations by taking corrective actions or prepare for such situations in advance.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # for logging \n",
    "\n",
    "from comet_ml import Experiment\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.loggers import TestTubeLogger\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "# For metrics\n",
    "from pytorch_lightning import metrics\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import io\n",
    "import torchtext\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping # The EarlyStopping callback can be used to monitor a validation metric and stop the training when no improvement is observed.\n",
    "\"\"\"\n",
    "    To enable it:\n",
    "\n",
    "    Import EarlyStopping callback.\n",
    "\n",
    "    Log the metric you want to monitor using log() method.\n",
    "\n",
    "    Init the callback, and set monitor to the logged metric of your choice.\n",
    "\n",
    "    Pass the EarlyStopping callback to the Trainer callbacks flag.\n",
    "\"\"\"\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.metrics import Metric\n",
    "from pytorch_lightning.metrics.utils import _input_format_classification\n",
    "from sklearn.metrics import classification_report\n",
    "class MyClassificationReport(Metric):\n",
    "    def __init__(self,threshold: float = 0.5,compute_on_step: bool = True,dist_sync_on_step: bool = False):\n",
    "        super().__init__(\n",
    "            compute_on_step=compute_on_step,\n",
    "            dist_sync_on_step=dist_sync_on_step,\n",
    "        )\n",
    "\n",
    "        self.threshold = threshold\n",
    "        self.add_state(\"preds\", default=[], dist_reduce_fx=None)\n",
    "        self.add_state(\"target\", default=[], dist_reduce_fx=None)\n",
    "\n",
    "        # rank_zero_warn(\n",
    "        #     'Metric `MyClassificationReport` will save all targets and predictions in buffer.'\n",
    "        #     ' For large datasets this may lead to large memory footprint.'\n",
    "        # )\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        preds = preds.cpu()\n",
    "        target = target.cpu()\n",
    "        y_hat, y = preds.max(1).indices, target\n",
    "        assert y_hat.shape == y.shape\n",
    "        self.preds.append(y_hat)\n",
    "        self.target.append(y)\n",
    "\n",
    "    def compute(self):\n",
    "        preds = torch.cat(self.preds, dim=0)\n",
    "        target = torch.cat(self.target, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Prepartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class AlarmDataset(Dataset):\n",
    "    def __init__(self,data,seq_len,batch_size):\n",
    "        self.length = len(data)//seq_len # how much data i have         \n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "       \n",
    "    def __getitem__(self, index: int):\n",
    "        x = self.data[index*self.seq_len:(index*self.seq_len)+self.seq_len]\n",
    "        y = self.data[1+index*self.seq_len:1+(index*self.seq_len)+self.seq_len]\n",
    "        return x,y\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        dir_path = self.config['dir-path']\n",
    "        file_name = 'train.tokens'\n",
    "\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.vocab = build_vocab_from_iterator(map(self.tokenizer,iter(io.open(dir_path+file_name,encoding=\"utf8\"))))\n",
    "    \n",
    "\n",
    "        train_data = self.data_process(iter(io.open(dir_path +\"train.tokens\", encoding=\"utf8\")))\n",
    "        val_data = self.data_process(iter(io.open(dir_path +\"val.tokens\", encoding=\"utf8\")))\n",
    "        test_data = self.data_process(iter(io.open(dir_path +\"test.tokens\", encoding=\"utf8\")))\n",
    "\n",
    "    \n",
    "        self.train_dataset = AlarmDataset(train_data, self.config['seq-len'], self.config['batch-size'])\n",
    "        self.valid_dataset = AlarmDataset(val_data,self.config['seq-len'], self.config['batch-size'])\n",
    "        self.test_dataset = AlarmDataset(test_data, self.config['seq-len'], self.config['batch-size'])\n",
    "\n",
    "    \n",
    "    def data_process(self, raw_text_iter):\n",
    "        data = [torch.tensor([self.vocab[token] for token in self.tokenizer(item)],dtype=torch.long) for item in raw_text_iter]\n",
    "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "    \n",
    "    def get_weight_per_class(self):\n",
    "        def lambdaFun(total,v,num_classes):\n",
    "            if v>0:\n",
    "                return total/(v*num_classes) \n",
    "            return 0\n",
    "        \n",
    "        index_2_count = {self.vocab.stoi[k]:self.vocab.freqs[k]  for k in list(self.vocab.stoi)}\n",
    "        total = sum(index_2_count.values())\n",
    "        index_2_ws = {k:lambdaFun(total,v,len(index_2_count)) for k,v in index_2_count.items()}\n",
    "        index_2_ws[1] = 0.0 # MANUALLY Setting the weights to zero for the padding\n",
    "        # index_2_ws[0] = 0.0 # MANUALLY Setting the weights to zero for the padding\n",
    "        ws = torch.tensor([index_2_ws[i] for i in range(len(index_2_ws))])\n",
    "\n",
    "        return ws\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "            Use this method to do things that might write to disk or that need to be done only from a single GPU in distributed settings.\n",
    "            e.g., download,tokenize,etc…\n",
    "        \"\"\" \n",
    "        return None\n",
    "\n",
    "\n",
    "    def setup(self, stage: None):\n",
    "        \"\"\"\n",
    "            There are also data operations you might want to perform on every GPU. Use setup to do things like:\n",
    "            count number of classes,build vocabulary,perform train/val/test splits,apply transforms (defined explicitly in your datamodule or assigned in init),etc…\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.config['batch-size'], shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.valid_dataset, batch_size=self.config['batch-size'], shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "    \n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.test_dataset, batch_size=self.config['batch-size'], shuffle=False,num_workers=8,drop_last=True, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model\n",
    "\n",
    "The alarm modeling task is to assign a\n",
    "probability for the likelihood of a given sequence of words\n",
    "to follow a next alarm. A sequence of tokens are passed to the embedding\n",
    "layer first, followed by a positional encoding layer to account for the order\n",
    "of the word. The\n",
    "``nn.TransformerEncoder`` consists of multiple layers of\n",
    "`nn.TransformerEncoderLayer <https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer>`__. Along with the input sequence, a square\n",
    "attention mask is required because the self-attention layers in\n",
    "``nn.TransformerEncoder`` are only allowed to attend the earlier positions in\n",
    "the sequence. For the language modeling task, any tokens on the future\n",
    "positions should be masked. To have the actual alarms, the output\n",
    "of ``nn.TransformerEncoder`` model is sent to the final Linear\n",
    "layer, which is followed by a log-Softmax function.\n",
    "\n",
    "\n",
    "# Positional Encoding\n",
    "\n",
    "``PositionalEncoding`` module injects some information about the\n",
    "relative or absolute position of the tokens (i.e.,) in the sequence. The\n",
    "positional encodings have the same dimension as the embeddings so that\n",
    "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
    "different frequencies.\n",
    "\n",
    "# Loss Function\n",
    "\n",
    "`CrossEntropyLoss` is applied to track the loss and `AdamW`implements stochastic gradient descent method as the optimizer.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.accuraccy_50_count = 0\n",
    "        self.config = config        \n",
    "        self.lr = self.config[\"lr\"]\n",
    "        self.weight_decay = self.config[\"weight-decay\"]\n",
    "    \n",
    "        self.pos_encoder = PositionalEncoding(self.config['em-size'], self.config['dropout'])\n",
    "        encoder_layers = torch.nn.TransformerEncoderLayer(self.config['em-size'], self.config['nhead'], self.config['nhid'], self.config[\"dropout\"])\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layers, self.config['nlayers'])\n",
    "        self.encoder = torch.nn.Embedding(self.config[\"vocab-size\"], self.config['em-size'])\n",
    "        self.decoder = torch.nn.Linear(self.config['em-size'], self.config[\"vocab-size\"])\n",
    "        self.src_mask = self.generate_square_subsequent_mask(self.config['seq-len'])\n",
    "        self.init_weights()\n",
    "\n",
    "        self.class_weight = self.config['weight_per_class']\n",
    "\n",
    "        # self.train_F1 = metrics.classification.F1(num_classes=self.config[\"vocab-size\"],average = 'micro')\n",
    "        # self.val_F1 = metrics.classification.F1(num_classes=self.config[\"vocab-size\"],average = 'micro')\n",
    "        # self.test_F1 = metrics.classification.F1(num_classes=self.config[\"vocab-size\"],average = 'micro')\n",
    "        \n",
    "        self.val_CM_normalized = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"],normalize ='true')\n",
    "        self.val_CM_raw = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"])\n",
    "\n",
    "        self.train_CM_normalized = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"],normalize ='true')\n",
    "        self.train_CM_raw = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"])\n",
    "\n",
    "        self.test_CM = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"],normalize ='true')\n",
    "\n",
    "        self.val_MCR = MyClassificationReport()\n",
    "        self.test_MCR = MyClassificationReport()\n",
    "\n",
    "        self.log(\"Sequence length\",self.config['seq-len'])\n",
    "        self.log(\"lr\",self.lr)\n",
    "        self.log(\"# of tokens/vocab_size (unique alarms)\",self.config['vocab-size'])\n",
    "        self.log(\"weight_decay\",self.weight_decay)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src_mask = src_mask.to(self.device)\n",
    "        src = self.encoder(src) * math.sqrt(self.config['em-size'])\n",
    "        src = self.pos_encoder(src)\n",
    "        src_mask = src_mask.to(self.device)\n",
    "      \n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "   # The ReduceLROnPlateau scheduler requires a monitor\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr,weight_decay=self.weight_decay)\n",
    "        d = {\n",
    "       'optimizer': optimizer,\n",
    "       'lr_scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = \"min\", factor = 0.5, patience=10, verbose=True),\n",
    "       'monitor': 'val_epoch_loss',\n",
    "        'interval': 'epoch'\n",
    "        }\n",
    "        return d \n",
    "\n",
    "    def loss_function(self,logits,y):\n",
    "        return F.cross_entropy(logits,y,weight= self.class_weight,ignore_index=1) \n",
    "\n",
    "    def myPrintToFile(self,cm_normal,cm_raw,f):\n",
    "        cm_normal = cm_normal.cpu()\n",
    "        cm_raw = cm_raw.cpu()\n",
    "        \n",
    "\n",
    "        sum_of_each_class = cm_raw.sum(axis=1) # sum along the columns\n",
    "        print(f\"        ------ Epoch {self.current_epoch} ---------\",file=f)\n",
    "        print(f\"Total={[v.item() for v in sum_of_each_class]}\",file=f)\n",
    "        print(f\"Corret={[v.item() for v in torch.diagonal(cm_raw,0)]}\",file=f)\n",
    "        print(f\"Accuracy={[round(v.item(),3) for v in (torch.diagonal(cm_raw,0)/sum_of_each_class)]}\",file=f)\n",
    "\n",
    "        accs = [round(v.item(),3)  for v in torch.diagonal(cm_normal,0)]\n",
    "\n",
    "        source2acc = {self.config['vocab'].itos[i]:accs[i] for i in range(len(accs))}\n",
    "\n",
    "        source2_acc50 = {self.config['vocab'].itos[i]:accs[i] for i in range(len(accs)) if accs[i]>=0.5}\n",
    "\n",
    "        print(f\"Acc2={accs}\",file=f)\n",
    "        print(f\"source2_acc= {source2acc}\",file=f)\n",
    "        print(f\"source2_acc50= {source2_acc50}\",file=f)\n",
    "\n",
    "        a_50 = len([a for a in accs if a>=0.5])\n",
    "        a_30 = len([a for a in accs if a>=0.3])\n",
    "        out_str = f\"acc>0.5= {a_50}, acc>=0.3= {a_30}, Total={len(accs)}\"\n",
    "        print(out_str,file=f)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # if temp> self.accuraccy_50_count and train=:\n",
    "        #     self.accuraccy_50_count = temp\n",
    "        print(out_str,end=\" \") \n",
    "\n",
    "      \n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        x = x.T\n",
    "        y = y.T.reshape(-1)\n",
    "\n",
    "        if x.size(0) != self.config['seq-len']:\n",
    "           self.src_mask =  self.generate_square_subsequent_mask(x.size(0))\n",
    "        \n",
    "        y_hat = self(x,self.src_mask)\n",
    "        y_hat =  y_hat.view(-1, self.config['vocab-size'])\n",
    "        loss = self.loss_function(y_hat,y) # cross entropy itself compute softmax \n",
    "\n",
    "        self.train_CM_normalized(F.softmax(y_hat),y)\n",
    "        self.train_CM_raw(F.softmax(y_hat),y)\n",
    "        \n",
    "        self.log('train_loss',loss,logger=True)\n",
    "        # self.log('train_F1',self.train_F1(F.softmax(y_hat),y),logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch, batch_idx):\n",
    "        x,y = batch\n",
    "        x = x.T\n",
    "        y = y.T.reshape(-1)\n",
    "        \n",
    "        if x.size(0) != self.config['seq-len']:\n",
    "            # print(f\">> passed {x.size()}\")\n",
    "            self.src_mask =  self.generate_square_subsequent_mask(x.size(0))\n",
    "        \n",
    "        y_hat = self(x,self.src_mask)\n",
    "        y_hat =  y_hat.view(-1, self.config['vocab-size'])\n",
    "        loss = self.loss_function(y_hat,y)\n",
    "\n",
    "        self.val_MCR(F.softmax(y_hat),y)\n",
    "        self.val_CM_normalized(F.softmax(y_hat),y)\n",
    "        self.val_CM_raw(F.softmax(y_hat),y)\n",
    "\n",
    "        self.log('val_loss',loss,logger=True)\n",
    "        # self.log('val_F1',self.val_F1(F.softmax(y_hat) ,y),logger=True)\n",
    "        return {'val_loss':loss}\n",
    "    \n",
    "    def test_step(self,batch, batch_idx):\n",
    "        x,y = batch\n",
    "        x = x.T\n",
    "        y = y.T.reshape(-1)\n",
    "        if x.size(0) != self.config['seq-len']:\n",
    "           self.src_mask =  self.generate_square_subsequent_mask(x.size(0))\n",
    "\n",
    "        y_hat = self(x,self.src_mask)\n",
    "        y_hat =  y_hat.view(-1,  self.config['vocab-size'])\n",
    "        loss = self.loss_function(y_hat,y)\n",
    "\n",
    "        self.test_MCR(F.softmax(y_hat),y)\n",
    "        self.log('test_loss',loss,logger=True)\n",
    "        # self.log('test_F1', self.test_F1(F.softmax(y_hat) ,y),logger=True)\n",
    "        return {'test_loss':loss}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        avg_loss = torch.stack([d['loss']  for d in outputs]).mean()\n",
    "        # f1 = self.train_F1.compute()\n",
    "        print(f\"[{self.current_epoch}]E, Avg Training loss = {round(avg_loss.item(),4)}\",end=\" \")\n",
    "        \n",
    "        with open(self.config[\"train-file\"],'a') as f:\n",
    "            self.myPrintToFile(self.train_CM_normalized.compute(),self.train_CM_raw.compute(),f)\n",
    "        self.log(\"train_epoch_loss\",avg_loss,logger=True,prog_bar=True)\n",
    "        # self.log(\"train_epoch_F1\", f1, logger=True,prog_bar=True)\n",
    "  \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['val_loss'] for d in outputs]).mean()\n",
    "        # f1 = self.val_F1.compute()\n",
    "        # print(self.val_MCR.compute(),file=open(\"val-out.txt\",'w'))\n",
    "        # print(self.val_CM.compute(),file=open(\"val-cm-out.txt\",'w'))\n",
    "\n",
    "        # if self.current_epoch%4==0 and self.current_epoch>0:\n",
    "        # self.myPrintToFile(self.val_CM_normalized.compute(),self.val_CM_raw.compute())\n",
    "\n",
    "\n",
    "        print(f\"::Val Loss = {round(avg_loss.item(),4) }\",end=\" \")\n",
    "\n",
    "        with open(self.config[\"val-file\"],'a') as f:\n",
    "            self.myPrintToFile(self.val_CM_normalized.compute(),self.val_CM_raw.compute(),f)\n",
    "        \n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "        self.log(\"val_epoch_loss\",avg_loss,logger=True)\n",
    "        # self.log(\"val_epoch_F1\",f1,logger=True,prog_bar=True)\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['test_loss'] for d in outputs]).mean()\n",
    "        # f1 = self.test_F1.compute()\n",
    "        # print(self.test_MCR.compute(),file=open(\"test-out.txt\",'w'))\n",
    "        print(f\">Average Test Loss = {avg_loss.item()}\")\n",
    "        self.log(\"test_epoch_loss\",avg_loss, logger = True)\n",
    "        # self.log(\"test_epoch_F1\",f1, logger=True)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlarmGRU(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self,config):\n",
    "        # super().__init__()\n",
    "        super(AlarmGRU,self).__init__()\n",
    "        self.config =config\n",
    "        self.lr = self.config['lr']\n",
    "        \n",
    "\n",
    "        self.val_CM_normalized = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"],normalize ='true')\n",
    "        self.val_CM_raw = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"])\n",
    "\n",
    "        self.train_CM_normalized = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"],normalize ='true')\n",
    "        self.train_CM_raw = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"])\n",
    "\n",
    "        self.test_CM = metrics.classification.ConfusionMatrix(num_classes=self.config[\"vocab-size\"],normalize ='true')\n",
    "\n",
    "        self.val_MCR = MyClassificationReport()\n",
    "        self.test_MCR = MyClassificationReport()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ## TODO: define the layers of the model\n",
    "        self.h = None\n",
    "        self.embedding = torch.nn.Embedding(self.config['vocab-size'],self.config['em-size'])\n",
    "        self.gru = torch.nn.GRU(input_size=self.config['em-size'], hidden_size=self.config['nhid'], num_layers=self.config['nlayers'],dropout=self.config['dropout'], batch_first=True)\n",
    "        # self.droput = torch.nn.Dropout(p=self.drop_prob)\n",
    "        self.fc3 = torch.nn.Linear(in_features=self.config['nhid'], out_features=self.config['vocab-size'])\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)  \n",
    "    \n",
    "    def __init_hidden(self):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of GRU\n",
    "        device = None \n",
    "        if (torch.cuda.is_available()):\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\") \n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.config['nlayers'], self.config['batch-size'], self.config['nhid']).zero_().to(device)\n",
    "        return hidden\n",
    "\n",
    "    def initialize_hidden(self):\n",
    "        self.h = self.__init_hidden()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        out, hidden = self.gru(embeds,hidden)\n",
    "        # Contiguous variables: If you are stacking up multiple LSTM outputs, it may be necessary to use .contiguous() to reshape the output.\n",
    "        out = out.contiguous().view(-1,self.config['nhid']) \n",
    "        out = self.fc3(out)\n",
    "        out = self.softmax(out)\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr,weight_decay=self.config['weight-decay'])\n",
    "        d = {\n",
    "       'optimizer': optimizer,\n",
    "       'lr_scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = \"min\", factor = 0.5, patience=10, verbose=True),\n",
    "       'monitor': 'val_epoch_loss',\n",
    "        'interval': 'epoch'\n",
    "        }\n",
    "        return d\n",
    "\n",
    "    # def loss_function(self,logits,y):\n",
    "    #     return F.cross_entropy(logits,y,weight= self.class_weight,ignore_index=1) \n",
    "\n",
    "    def myPrintToFile(self,cm_normal,cm_raw,f):\n",
    "        cm_normal = cm_normal.cpu()\n",
    "        cm_raw = cm_raw.cpu()\n",
    "        \n",
    "\n",
    "        sum_of_each_class = cm_raw.sum(axis=1) # sum along the columns\n",
    "        print(f\"        ------ Epoch {self.current_epoch} ---------\",file=f)\n",
    "        print(f\"Total={[v.item() for v in sum_of_each_class]}\",file=f)\n",
    "        print(f\"Corret={[v.item() for v in torch.diagonal(cm_raw,0)]}\",file=f)\n",
    "        print(f\"Accuracy={[round(v.item(),3) for v in (torch.diagonal(cm_raw,0)/sum_of_each_class)]}\",file=f)\n",
    "\n",
    "        accs = [round(v.item(),3)  for v in torch.diagonal(cm_normal,0)]\n",
    "\n",
    "        source2acc = {self.config['vocab'].itos[i]:accs[i] for i in range(len(accs))}\n",
    "\n",
    "        source2_acc50 = {self.config['vocab'].itos[i]:accs[i] for i in range(len(accs)) if accs[i]>=0.5}\n",
    "\n",
    "        print(f\"Acc2={accs}\",file=f)\n",
    "        print(f\"source2_acc= {source2acc}\",file=f)\n",
    "        print(f\"source2_acc50= {source2_acc50}\",file=f)\n",
    "\n",
    "        a_50 = len([a for a in accs if a>=0.5])\n",
    "        a_30 = len([a for a in accs if a>=0.3])\n",
    "        out_str = f\"acc>0.5= {a_50}, acc>=0.3= {a_30}, Total={len(accs)}\"\n",
    "        print(out_str,file=f)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # if temp> self.accuraccy_50_count and train=:\n",
    "        #     self.accuraccy_50_count = temp\n",
    "        print(out_str,end=\" \")\n",
    "    \n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        y =  y.view(self.config['batch-size']*self.config['seq-len']).long()\n",
    "\n",
    "        self.h = self.h.data # for GRU\n",
    "        y_hat, self.h = self(x,self.h)\n",
    "        #  ignore_index=self.char2int[\"NoName\"]\n",
    "        loss = F.nll_loss(y_hat,y)\n",
    "        \n",
    "        self.train_CM_normalized(y_hat,y)\n",
    "        self.train_CM_raw(y_hat,y)    \n",
    "        # result = pl.TrainResult(loss) # logging\n",
    "        self.log('train_loss',loss,logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch, batch_idx):\n",
    "        x,y = batch\n",
    "        y =  y.view(self.config['batch-size']*self.config['seq-len']).long()\n",
    "\n",
    "        self.h = self.h.data # for GRU\n",
    "        y_hat, self.h = self(x,self.h)\n",
    "        #  ignore_index=self.char2int[\"NoName\"]\n",
    "        loss = F.nll_loss(y_hat,y)\n",
    "        \n",
    "        self.val_CM_normalized(y_hat,y)\n",
    "        self.val_CM_raw(y_hat,y)\n",
    "        self.log('val_loss',loss,logger=True)\n",
    "        return {'val_loss':loss}\n",
    "    \n",
    "    def test_step(self,batch, batch_idx):\n",
    "        x,y = batch\n",
    "        y =  y.view(self.config['batch-size']*self.config['seq-len']).long()\n",
    "\n",
    "        self.h = self.h.data # for GRU\n",
    "        y_hat, self.h = self(x,self.h)\n",
    "        #  ignore_index=self.char2int[\"NoName\"]\n",
    "        loss = F.nll_loss(y_hat,y)\n",
    "        \n",
    "        self.val_CM_normalized(y_hat,y)\n",
    "        self.val_CM_raw(y_hat,y)\n",
    "        self.log('test_loss',loss,logger=True)\n",
    "        return {'test_loss':loss}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        avg_loss = torch.stack([d['loss']  for d in outputs]).mean()\n",
    "        # f1 = self.train_F1.compute()\n",
    "        print(f\"[{self.current_epoch}]E, Avg Training loss = {round(avg_loss.item(),4)}\",end=\" \")\n",
    "        \n",
    "        with open(self.config[\"train-file\"],'a') as f:\n",
    "            self.myPrintToFile(self.train_CM_normalized.compute(),self.train_CM_raw.compute(),f)\n",
    "        self.log(\"train_epoch_loss\",avg_loss,logger=True,prog_bar=True)\n",
    "        # self.log(\"train_epoch_F1\", f1, logger=True,prog_bar=True)\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['val_loss'] for d in outputs]).mean()\n",
    "        # f1 = self.val_F1.compute()\n",
    "        # print(self.val_MCR.compute(),file=open(\"val-out.txt\",'w'))\n",
    "        # print(self.val_CM.compute(),file=open(\"val-cm-out.txt\",'w'))\n",
    "\n",
    "        # if self.current_epoch%4==0 and self.current_epoch>0:\n",
    "        # self.myPrintToFile(self.val_CM_normalized.compute(),self.val_CM_raw.compute())\n",
    "\n",
    "\n",
    "        print(f\"::Val Loss = {round(avg_loss.item(),4) }\",end=\" \")\n",
    "\n",
    "        with open(self.config[\"val-file\"],'a') as f:\n",
    "            self.myPrintToFile(self.val_CM_normalized.compute(),self.val_CM_raw.compute(),f)\n",
    "        \n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "        self.log(\"val_epoch_loss\",avg_loss,logger=True)\n",
    "        # self.log(\"val_epoch_F1\",f1,logger=True,prog_bar=True)\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([d['test_loss'] for d in outputs]).mean()\n",
    "        # f1 = self.test_F1.compute()\n",
    "        # print(self.test_MCR.compute(),file=open(\"test-out.txt\",'w'))\n",
    "        print(f\">Average Test Loss = {avg_loss.item()}\")\n",
    "        self.log(\"test_epoch_loss\",avg_loss, logger = True)\n",
    "        # self.log(\"test_epoch_F1\",f1, logger=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning Transformers\n",
    "\n",
    "**Note: When monitoring any parameter after the validation epoch end then you should pass check_val_every_n_epoch=1  not to other. This is very important.**\n",
    "\n",
    "### Finding the learning rate for the Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48652lines [00:00, 81715.44lines/s]\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n",
      "\n",
      "   | Name                | Type                   | Params\n",
      "----------------------------------------------------------------\n",
      "0  | pos_encoder         | PositionalEncoding     | 0     \n",
      "1  | transformer_encoder | TransformerEncoder     | 1.3 M \n",
      "2  | encoder             | Embedding              | 182 K \n",
      "3  | decoder             | Linear                 | 183 K \n",
      "4  | val_CM_normalized   | ConfusionMatrix        | 0     \n",
      "5  | val_CM_raw          | ConfusionMatrix        | 0     \n",
      "6  | train_CM_normalized | ConfusionMatrix        | 0     \n",
      "7  | train_CM_raw        | ConfusionMatrix        | 0     \n",
      "8  | test_CM             | ConfusionMatrix        | 0     \n",
      "9  | val_MCR             | MyClassificationReport | 0     \n",
      "10 | test_MCR            | MyClassificationReport | 0     \n",
      "----------------------------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "Restored states from the checkpoint file at /home/waris/Github/predict-future-alarms/project/lr_find_temp_model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before [0.0, 0.0, 0.008, 0.009, 0.059, 0.061, 0.063, 0.075, 0.075, 0.075, 0.076, 0.078, 0.083, 0.098, 0.101, 0.11, 0.143, 0.157, 0.158, 0.172, 0.176, 0.18, 0.186, 0.187, 0.251, 0.269, 0.31, 0.327, 0.337, 0.344, 0.35, 0.363, 0.371, 0.377, 0.377, 0.389, 0.39, 0.404, 0.418, 0.419, 0.426, 0.428, 0.428, 0.467, 0.47, 0.476, 0.48, 0.48, 0.486, 0.492, 0.495, 0.495, 0.513, 0.531, 0.537, 0.539, 0.546, 0.576, 0.58, 0.601, 0.624, 0.629, 0.63, 0.644, 0.644, 0.654, 0.66, 0.662, 0.667, 0.673, 0.673, 0.687, 0.706, 0.707, 0.708, 0.735, 0.746, 0.749, 0.764, 0.785, 0.802, 0.803, 0.807, 0.827, 0.848, 0.904, 0.905, 0.905, 0.909, 0.911, 0.919, 0.935, 0.966, 0.976, 0.978, 0.982, 0.983, 1.004, 1.013, 1.03, 1.039, 1.05, 1.051, 1.054, 1.081, 1.082, 1.096, 1.109, 1.115, 1.124, 1.14, 1.141, 1.155, 1.16, 1.173, 1.177, 1.188, 1.198, 1.2, 1.201, 1.215, 1.235, 1.24, 1.25, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.264, 1.279, 1.283, 1.284, 1.303, 1.304, 1.316, 1.333, 1.338, 1.392, 1.398, 1.447, 1.496, 1.502, 1.508, 1.513, 1.516, 1.527, 1.547, 1.571, 1.588, 1.597, 1.645, 1.666, 1.691, 1.703, 1.703, 1.718, 1.736, 1.738, 1.747, 1.749, 1.758, 1.762, 1.809, 1.827, 1.839, 1.865, 1.867, 1.867, 1.89, 1.89, 1.895, 1.941, 1.959, 1.97, 2.023, 2.049, 2.052, 2.061, 2.109, 2.112, 2.115, 2.118, 2.173, 2.176, 2.241, 2.259, 2.259, 2.269, 2.269, 2.295, 2.302, 2.321, 2.336, 2.355, 2.355, 2.39, 2.443, 2.452, 2.538, 2.538, 2.538, 2.57, 2.57, 2.598, 2.636, 2.656, 2.656, 2.711, 2.711, 2.721, 2.727, 2.742, 2.779, 2.807, 2.807, 2.818, 2.823, 2.829, 2.829, 2.846, 2.851, 2.851, 2.857, 2.863, 2.874, 2.88, 2.963, 2.994, 3.0, 3.007, 3.007, 3.051, 3.058, 3.084, 3.111, 3.118, 3.125, 3.223, 3.223, 3.237, 3.245, 3.245, 3.274, 3.282, 3.312, 3.328, 3.335, 3.415, 3.423, 3.448, 3.464, 3.55, 3.577, 3.622, 3.687, 3.696, 3.725, 3.745, 3.804, 3.835, 3.835, 3.897, 3.908, 3.919, 3.919, 3.94, 3.951, 3.973, 3.995, 3.995, 4.04, 4.074, 4.074, 4.098, 4.098, 4.109, 4.145, 4.157, 4.169, 4.181, 4.194, 4.194, 4.194, 4.218, 4.294, 4.333, 4.413, 4.427, 4.567, 4.612, 4.641, 4.672, 4.687, 4.733, 4.797, 4.813, 4.829, 4.862, 4.928, 4.963, 4.98, 5.015, 5.122, 5.14, 5.178, 5.196, 5.234, 5.253, 5.253, 5.273, 5.273, 5.351, 5.392, 5.412, 5.433, 5.433, 5.474, 5.516, 5.537, 5.537, 5.602, 5.624, 5.624, 5.646, 5.669, 5.669, 5.691, 5.691, 5.691, 5.737, 5.737, 5.737, 5.783, 5.806, 5.83, 5.83, 5.854, 5.878, 5.926, 5.951, 5.951, 5.976, 6.001, 6.001, 6.051, 6.103, 6.103, 6.103, 6.129, 6.182, 6.209, 6.318, 6.318, 6.374, 6.431, 6.431, 6.46, 6.49, 6.549, 6.579, 6.671, 6.862, 6.895, 6.962, 6.962, 7.03, 7.065, 7.135, 7.171, 7.28, 7.317, 7.355, 7.355, 7.393, 7.47, 7.509, 7.548, 7.669, 7.669, 7.711, 7.752, 7.752, 7.794, 7.794, 7.794, 7.794, 7.837, 7.837, 7.837, 7.88, 7.88, 7.968, 7.968, 7.968, 7.968, 8.012, 8.012, 8.012, 8.057, 8.103, 8.149, 8.149, 8.195, 8.195, 8.242, 8.387, 8.387, 8.436, 8.486, 8.486, 8.486, 8.64, 8.745, 8.853, 8.908, 8.964, 9.194, 9.253, 9.253, 9.374, 9.435, 9.435, 9.435, 9.435, 9.498, 9.498, 9.498, 9.561, 9.69, 9.69, 9.69, 9.823, 9.823, 10.1, 10.1, 10.1, 10.172, 10.393, 10.393, 10.393, 10.469, 10.469, 10.545, 10.703, 10.703, 10.783, 10.865, 11.032, 11.032, 11.118, 11.205, 11.205, 11.205, 11.293, 11.382, 11.474, 11.474, 11.566, 11.66, 11.66, 11.66, 11.756, 11.756, 11.853, 11.853, 11.853, 11.952, 11.952, 11.952, 12.052, 12.154, 12.154, 12.258, 12.364, 12.364, 12.364, 12.471, 12.581, 12.581, 12.581, 12.581, 12.692, 12.692, 12.692, 12.805, 12.921, 12.921, 12.921, 13.158, 13.28, 13.28, 13.404, 13.404, 13.404, 13.404, 13.659, 13.659, 13.659, 13.659, 13.659, 13.79, 13.79, 13.79, 14.061, 14.061, 14.2, 14.2, 14.2, 14.2, 14.2, 14.2, 14.2, 14.342, 14.342, 14.342, 14.342, 14.342, 14.487, 14.487, 14.487, 14.487, 14.635, 14.635, 14.785, 14.785, 14.785, 14.785, 14.939, 14.939, 14.939, 14.939, 14.939, 14.939, 15.097, 15.257, 15.257, 15.257, 15.257, 15.421, 15.421, 15.421, 15.589, 15.589, 15.589, 15.76, 15.76, 15.76, 15.935, 16.114, 16.298, 16.298, 16.298, 16.298, 16.298, 16.485, 16.485, 16.485, 16.485, 16.485, 16.677, 16.677, 16.677, 16.677, 16.677, 17.074, 17.074, 17.074, 17.279, 17.49, 17.49, 17.706, 17.706, 17.927, 17.927, 18.154, 18.154, 18.154, 18.387, 18.387, 18.387, 18.387, 18.626, 18.626, 18.626, 18.626, 18.871, 19.123, 19.123, 19.123, 19.381, 19.381, 19.381, 19.381, 19.381, 19.646, 19.646, 19.646, 19.646, 19.646, 19.919, 19.919, 19.919, 19.919, 19.919, 19.919, 19.919, 20.2, 20.2, 20.2, 20.2, 20.2, 20.488, 20.488, 20.488, 20.785, 21.091, 21.091, 21.091, 21.091, 21.406, 21.406, 21.406, 21.73, 21.73, 21.73, 21.73, 21.73, 22.064, 22.064, 22.064, 22.064, 22.064, 22.064, 22.064, 22.064, 22.409, 22.409, 22.409, 22.409, 22.409, 22.409, 22.409, 22.765, 23.132, 23.132, 23.132, 23.132, 23.511, 23.511, 23.511, 23.903, 23.903, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.727, 24.727, 24.727, 24.727, 25.161, 25.161, 25.161, 25.61, 25.61, 25.61, 25.61, 25.61, 26.076, 26.076, 26.076, 26.076, 27.06, 27.06, 27.06, 27.06, 27.581, 27.581, 27.581, 27.581, 28.684, 29.269, 29.879, 30.515, 30.515, 31.178, 31.178, 31.178, 31.178, 31.871, 34.147, 34.98, 34.98, 35.855, 36.774, 36.774, 36.774, 47.806, 49.455, 95.613, 143.419]\n",
      "After [0.0, 0.0, 0.008, 0.009, 0.059, 0.061, 0.063, 0.075, 0.075, 0.075, 0.076, 0.078, 0.083, 0.098, 0.101, 0.11, 0.143, 0.157, 0.158, 0.172, 0.176, 0.18, 0.186, 0.187, 0.251, 0.269, 0.31, 0.327, 0.337, 0.344, 0.35, 0.363, 0.371, 0.377, 0.377, 0.389, 0.39, 0.404, 0.418, 0.419, 0.426, 0.428, 0.428, 0.467, 0.47, 0.476, 0.48, 0.48, 0.486, 0.492, 0.495, 0.495, 0.513, 0.531, 0.537, 0.539, 0.546, 0.576, 0.58, 0.601, 0.624, 0.629, 0.63, 0.644, 0.644, 0.654, 0.66, 0.662, 0.667, 0.673, 0.673, 0.687, 0.706, 0.707, 0.708, 0.735, 0.746, 0.749, 0.764, 0.785, 0.802, 0.803, 0.807, 0.827, 0.848, 0.904, 0.905, 0.905, 0.909, 0.911, 0.919, 0.935, 0.966, 0.976, 0.978, 0.982, 0.983, 1.004, 1.013, 1.03, 1.039, 1.05, 1.051, 1.054, 1.081, 1.082, 1.096, 1.109, 1.115, 1.124, 1.14, 1.141, 1.155, 1.16, 1.173, 1.177, 1.188, 1.198, 1.2, 1.201, 1.215, 1.235, 1.24, 1.25, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.264, 1.279, 1.283, 1.284, 1.303, 1.304, 1.316, 1.333, 1.338, 1.392, 1.398, 1.447, 1.496, 1.502, 1.508, 1.513, 1.516, 1.527, 1.547, 1.571, 1.588, 1.597, 1.645, 1.666, 1.691, 1.703, 1.703, 1.718, 1.736, 1.738, 1.747, 1.749, 1.758, 1.762, 1.809, 1.827, 1.839, 1.865, 1.867, 1.867, 1.89, 1.89, 1.895, 1.941, 1.959, 1.97, 2.023, 2.049, 2.052, 2.061, 2.109, 2.112, 2.115, 2.118, 2.173, 2.176, 2.241, 2.259, 2.259, 2.269, 2.269, 2.295, 2.302, 2.321, 2.336, 2.355, 2.355, 2.39, 2.443, 2.452, 2.538, 2.538, 2.538, 2.57, 2.57, 2.598, 2.636, 2.656, 2.656, 2.711, 2.711, 2.721, 2.727, 2.742, 2.779, 2.807, 2.807, 2.818, 2.823, 2.829, 2.829, 2.846, 2.851, 2.851, 2.857, 2.863, 2.874, 2.88, 2.963, 2.994, 3.0, 3.007, 3.007, 3.051, 3.058, 3.084, 3.111, 3.118, 3.125, 3.223, 3.223, 3.237, 3.245, 3.245, 3.274, 3.282, 3.312, 3.328, 3.335, 3.415, 3.423, 3.448, 3.464, 3.55, 3.577, 3.622, 3.687, 3.696, 3.725, 3.745, 3.804, 3.835, 3.835, 3.897, 3.908, 3.919, 3.919, 3.94, 3.951, 3.973, 3.995, 3.995, 4.04, 4.074, 4.074, 4.098, 4.098, 4.109, 4.145, 4.157, 4.169, 4.181, 4.194, 4.194, 4.194, 4.218, 4.294, 4.333, 4.413, 4.427, 4.567, 4.612, 4.641, 4.672, 4.687, 4.733, 4.797, 4.813, 4.829, 4.862, 4.928, 4.963, 4.98, 5.015, 5.122, 5.14, 5.178, 5.196, 5.234, 5.253, 5.253, 5.273, 5.273, 5.351, 5.392, 5.412, 5.433, 5.433, 5.474, 5.516, 5.537, 5.537, 5.602, 5.624, 5.624, 5.646, 5.669, 5.669, 5.691, 5.691, 5.691, 5.737, 5.737, 5.737, 5.783, 5.806, 5.83, 5.83, 5.854, 5.878, 5.926, 5.951, 5.951, 5.976, 6.001, 6.001, 6.051, 6.103, 6.103, 6.103, 6.129, 6.182, 6.209, 6.318, 6.318, 6.374, 6.431, 6.431, 6.46, 6.49, 6.549, 6.579, 6.671, 6.862, 6.895, 6.962, 6.962, 7.03, 7.065, 7.135, 7.171, 7.28, 7.317, 7.355, 7.355, 7.393, 7.47, 7.509, 7.548, 7.669, 7.669, 7.711, 7.752, 7.752, 7.794, 7.794, 7.794, 7.794, 7.837, 7.837, 7.837, 7.88, 7.88, 7.968, 7.968, 7.968, 7.968, 8.012, 8.012, 8.012, 8.057, 8.103, 8.149, 8.149, 8.195, 8.195, 8.242, 8.387, 8.387, 8.436, 8.486, 8.486, 8.486, 8.64, 8.745, 8.853, 8.908, 8.964, 9.194, 9.253, 9.253, 9.374, 9.435, 9.435, 9.435, 9.435, 9.498, 9.498, 9.498, 9.561, 9.69, 9.69, 9.69, 9.823, 9.823, 10.1, 10.1, 10.1, 10.172, 10.393, 10.393, 10.393, 10.469, 10.469, 10.545, 10.703, 10.703, 10.783, 10.865, 11.032, 11.032, 11.118, 11.205, 11.205, 11.205, 11.293, 11.382, 11.474, 11.474, 11.566, 11.66, 11.66, 11.66, 11.756, 11.756, 11.853, 11.853, 11.853, 11.952, 11.952, 11.952, 12.052, 12.154, 12.154, 12.258, 12.364, 12.364, 12.364, 12.471, 12.581, 12.581, 12.581, 12.581, 12.692, 12.692, 12.692, 12.805, 12.921, 12.921, 12.921, 13.158, 13.28, 13.28, 13.404, 13.404, 13.404, 13.404, 13.659, 13.659, 13.659, 13.659, 13.659, 13.79, 13.79, 13.79, 14.061, 14.061, 14.2, 14.2, 14.2, 14.2, 14.2, 14.2, 14.2, 14.342, 14.342, 14.342, 14.342, 14.342, 14.487, 14.487, 14.487, 14.487, 14.635, 14.635, 14.785, 14.785, 14.785, 14.785, 14.939, 14.939, 14.939, 14.939, 14.939, 14.939, 15.097, 15.257, 15.257, 15.257, 15.257, 15.421, 15.421, 15.421, 15.589, 15.589, 15.589, 15.76, 15.76, 15.76, 15.935, 16.114, 16.298, 16.298, 16.298, 16.298, 16.298, 16.485, 16.485, 16.485, 16.485, 16.485, 16.677, 16.677, 16.677, 16.677, 16.677, 17.074, 17.074, 17.074, 17.279, 17.49, 17.49, 17.706, 17.706, 17.927, 17.927, 18.154, 18.154, 18.154, 18.387, 18.387, 18.387, 18.387, 18.626, 18.626, 18.626, 18.626, 18.871, 19.123, 19.123, 19.123, 19.381, 19.381, 19.381, 19.381, 19.381, 19.646, 19.646, 19.646, 19.646, 19.646, 19.919, 19.919, 19.919, 19.919, 19.919, 19.919, 19.919, 20.2, 20.2, 20.2, 20.2, 20.2, 20.488, 20.488, 20.488, 20.785, 21.091, 21.091, 21.091, 21.091, 21.406, 21.406, 21.406, 21.73, 21.73, 21.73, 21.73, 21.73, 22.064, 22.064, 22.064, 22.064, 22.064, 22.064, 22.064, 22.064, 22.409, 22.409, 22.409, 22.409, 22.409, 22.409, 22.409, 22.765, 23.132, 23.132, 23.132, 23.132, 23.511, 23.511, 23.511, 23.903, 23.903, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.727, 24.727, 24.727, 24.727, 25.161, 25.161, 25.161, 25.61, 25.61, 25.61, 25.61, 25.61, 26.076, 26.076, 26.076, 26.076, 27.06, 27.06, 27.06, 27.06, 27.581, 27.581, 27.581, 27.581, 28.684, 29.269, 29.879, 30.515, 30.515, 31.178, 31.178, 31.178, 31.178, 31.871, 34.147, 34.98, 34.98, 35.855, 36.774, 36.774, 36.774, 47.806, 49.455, 95.613, 143.419]\n",
      "::Val Loss = 6.9607 acc>0.5= 1, acc>=0.3= 3, Total=714 \n",
      "::Val Loss = 6.9858 acc>0.5= 1, acc>=0.3= 2, Total=714 \n",
      "[0]E, Avg Training loss = 6.9803 acc>0.5= 0, acc>=0.3= 1, Total=714 ::Val Loss = 6.9834 acc>0.5= 1, acc>=0.3= 2, Total=714 \n",
      "[1]E, Avg Training loss = 6.9755 acc>0.5= 0, acc>=0.3= 1, Total=714 ::Val Loss = 6.9468 acc>0.5= 1, acc>=0.3= 2, Total=714 \n",
      "[2]E, Avg Training loss = 6.9691 acc>0.5= 0, acc>=0.3= 1, Total=714 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "::Val Loss = 6.6258 \n",
      "acc>0.5= 0, acc>=0.3= 2, Total=714 \n",
      "[3]E, Avg Training loss = 6.8443 acc>0.5= 0, acc>=0.3= 1, Total=714 ::Val Loss = 6.3592 acc>0.5= 5, acc>=0.3= 16, Total=714 \n",
      "[4]E, Avg Training loss = 6.4388 acc>0.5= 2, acc>=0.3= 4, Total=714 ::Val Loss = 7.4179 acc>0.5= 1, acc>=0.3= 1, Total=714 \n",
      "[5]E, Avg Training loss = 6.5074 acc>0.5= 0, acc>=0.3= 0, Total=714 [6]E, Avg Training loss = 7.2565 acc>0.5= 0, acc>=0.3= 0, Total=714 Suggested lr = 0.005754399373371567\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e249c6b61f034014af02cdc56e9b1b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Finding best initial lr'), FloatProgress(value=0.0), HTML(value='')))"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light",
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def weightCondition(w,avg_w):\n",
    "    if w<avg_w:\n",
    "        return w\n",
    "    else:\n",
    "        return avg_w\n",
    "\n",
    "# setup data\n",
    "config_data = {\n",
    "'dir-path' : \"../.data/\",\n",
    "'batch-size' :512, # Batch Size \n",
    "'seq-len' :128, # Sequence length\n",
    "}\n",
    "\n",
    "dm = MyDataModule(config=config_data)\n",
    "ws = dm.get_weight_per_class().cuda()\n",
    "\n",
    "print(\"Before\",[round(w.item(),3) for w in ws])\n",
    "# avg_w = sum(ws)/len(ws)\n",
    "# ws = torch.tensor([weightCondition(w,avg_w) for w in ws]).cuda()\n",
    "print(\"After\",[round(w.item(),3) for w in ws])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config_model = {\n",
    "    'lr' : 0.001,\n",
    "    'dropout' : 0.2,\n",
    "    'weight-decay': 3.1,\n",
    "    'em-size' :256, # embedding dimension \n",
    "    'nhid' : 128, # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "    'nlayers' :4, # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    'nhead' : 2, # the number of heads in the multiheadattention models\n",
    "    'seq-len': config_data['seq-len'], # dont use wandb config \n",
    "    'vocab-size':len(dm.vocab.stoi), # the size of vocabulary /also called tokens\n",
    "    'weight_per_class':ws,\n",
    "    \"val-file\":\"val-out.txt\",\n",
    "    \"train-file\":'train-out.txt',\n",
    "    \"vocab\": dm.vocab\n",
    "}\n",
    "\n",
    "with open (config_model[\"val-file\"],'w') as f:\n",
    "    f.write(\">> Starting\")\n",
    "\n",
    "with open (config_model[\"train-file\"],'w') as f:\n",
    "    f.write(\">> Starting\")\n",
    "\n",
    "# setup model - note how we refer to sweep parameters with wandb.config\n",
    "model = TransformerModel(config=config_model)\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "        monitor='val_epoch_loss',\n",
    "        min_delta=0,\n",
    "        patience=600,\n",
    "        verbose=True,\n",
    "        mode='min'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(auto_lr_find=0.0001, precision=16,gpus=-1, num_nodes=1,  max_epochs=100, check_val_every_n_epoch=1,deterministic=True,gradient_clip_val=0.5,enable_pl_optimizer=True,callbacks=[early_stop_callback],progress_bar_refresh_rate=0)\n",
    "\n",
    "# Run learning rate finder\n",
    "lr_finder = trainer.tuner.lr_find(model,dm)\n",
    "\n",
    "# Results can be found in\n",
    "lr_finder.results\n",
    "\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()\n",
    "\n",
    "# Pick point based on plot, or get suggestion\n",
    "new_lr = lr_finder.suggestion()\n",
    "\n",
    "print(f\"Suggested lr = {new_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                | Type                   | Params\n",
      "----------------------------------------------------------------\n",
      "0  | pos_encoder         | PositionalEncoding     | 0     \n",
      "1  | transformer_encoder | TransformerEncoder     | 1.3 M \n",
      "2  | encoder             | Embedding              | 182 K \n",
      "3  | decoder             | Linear                 | 183 K \n",
      "4  | val_CM_normalized   | ConfusionMatrix        | 0     \n",
      "5  | val_CM_raw          | ConfusionMatrix        | 0     \n",
      "6  | train_CM_normalized | ConfusionMatrix        | 0     \n",
      "7  | train_CM_raw        | ConfusionMatrix        | 0     \n",
      "8  | test_CM             | ConfusionMatrix        | 0     \n",
      "9  | val_MCR             | MyClassificationReport | 0     \n",
      "10 | test_MCR            | MyClassificationReport | 0     \n",
      "----------------------------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::Val Loss = 6.9607 acc>0.5= 1, acc>=0.3= 3, Total=714 \n",
      "::Val Loss = 6.6953 acc>0.5= 2, acc>=0.3= 2, Total=714 \n",
      "[1]E, Avg Training loss = 6.9605 acc>0.5= 0, acc>=0.3= 0, Total=714 ::Val Loss = 6.2927 acc>0.5= 4, acc>=0.3= 18, Total=714 \n",
      "[2]E, Avg Training loss = 6.5591 acc>0.5= 1, acc>=0.3= 2, Total=714 ::Val Loss = 5.8698 acc>0.5= 26, acc>=0.3= 81, Total=714 \n",
      "[3]E, Avg Training loss = 5.996 acc>0.5= 14, acc>=0.3= 37, Total=714 ::Val Loss = 5.503 acc>0.5= 59, acc>=0.3= 132, Total=714 \n",
      "[4]E, Avg Training loss = 5.5356 acc>0.5= 46, acc>=0.3= 119, Total=714 ::Val Loss = 5.2612 acc>0.5= 78, acc>=0.3= 153, Total=714 \n",
      "[5]E, Avg Training loss = 5.2409 acc>0.5= 80, acc>=0.3= 184, Total=714 ::Val Loss = 5.0282 acc>0.5= 89, acc>=0.3= 171, Total=714 \n",
      "[6]E, Avg Training loss = 4.9778 acc>0.5= 105, acc>=0.3= 225, Total=714 ::Val Loss = 4.8711 acc>0.5= 104, acc>=0.3= 189, Total=714 \n",
      "[7]E, Avg Training loss = 4.7696 acc>0.5= 127, acc>=0.3= 244, Total=714 ::Val Loss = 4.694 acc>0.5= 110, acc>=0.3= 198, Total=714 \n",
      "[8]E, Avg Training loss = 4.6165 acc>0.5= 140, acc>=0.3= 256, Total=714 ::Val Loss = 4.5686 acc>0.5= 114, acc>=0.3= 209, Total=714 \n",
      "[9]E, Avg Training loss = 4.4846 acc>0.5= 150, acc>=0.3= 275, Total=714 ::Val Loss = 4.4919 acc>0.5= 120, acc>=0.3= 227, Total=714 \n",
      "[10]E, Avg Training loss = 4.3874 acc>0.5= 152, acc>=0.3= 281, Total=714 ::Val Loss = 4.4235 acc>0.5= 113, acc>=0.3= 235, Total=714 \n",
      "[11]E, Avg Training loss = 4.2929 acc>0.5= 160, acc>=0.3= 284, Total=714 ::Val Loss = 4.3393 acc>0.5= 120, acc>=0.3= 233, Total=714 \n",
      "[12]E, Avg Training loss = 4.2005 acc>0.5= 154, acc>=0.3= 297, Total=714 ::Val Loss = 4.2735 acc>0.5= 121, acc>=0.3= 234, Total=714 \n",
      "[13]E, Avg Training loss = 4.1594 acc>0.5= 157, acc>=0.3= 297, Total=714 ::Val Loss = 4.2375 acc>0.5= 122, acc>=0.3= 232, Total=714 \n",
      "[14]E, Avg Training loss = 4.0964 acc>0.5= 166, acc>=0.3= 306, Total=714 ::Val Loss = 4.223 acc>0.5= 120, acc>=0.3= 230, Total=714 \n",
      "[15]E, Avg Training loss = 4.0396 acc>0.5= 168, acc>=0.3= 305, Total=714 ::Val Loss = 4.1569 acc>0.5= 124, acc>=0.3= 240, Total=714 \n",
      "[16]E, Avg Training loss = 3.9954 acc>0.5= 169, acc>=0.3= 310, Total=714 ::Val Loss = 4.1708 acc>0.5= 119, acc>=0.3= 236, Total=714 \n",
      "[17]E, Avg Training loss = 3.973 acc>0.5= 170, acc>=0.3= 321, Total=714 ::Val Loss = 4.1459 acc>0.5= 129, acc>=0.3= 237, Total=714 \n",
      "[18]E, Avg Training loss = 3.9477 acc>0.5= 165, acc>=0.3= 314, Total=714 ::Val Loss = 4.1643 acc>0.5= 132, acc>=0.3= 237, Total=714 \n",
      "[19]E, Avg Training loss = 3.9487 acc>0.5= 168, acc>=0.3= 320, Total=714 ::Val Loss = 4.1282 acc>0.5= 124, acc>=0.3= 242, Total=714 \n",
      "[20]E, Avg Training loss = 3.9285 acc>0.5= 173, acc>=0.3= 316, Total=714 ::Val Loss = 4.1048 acc>0.5= 127, acc>=0.3= 242, Total=714 \n",
      "[21]E, Avg Training loss = 3.9067 acc>0.5= 176, acc>=0.3= 325, Total=714 ::Val Loss = 4.1007 acc>0.5= 126, acc>=0.3= 238, Total=714 \n",
      "[22]E, Avg Training loss = 3.9027 acc>0.5= 169, acc>=0.3= 331, Total=714 ::Val Loss = 4.1122 acc>0.5= 131, acc>=0.3= 237, Total=714 \n",
      "[23]E, Avg Training loss = 3.8983 acc>0.5= 178, acc>=0.3= 323, Total=714 ::Val Loss = 4.1211 acc>0.5= 130, acc>=0.3= 237, Total=714 \n",
      "[24]E, Avg Training loss = 3.8951 acc>0.5= 168, acc>=0.3= 331, Total=714 ::Val Loss = 4.1413 acc>0.5= 124, acc>=0.3= 242, Total=714 \n",
      "[25]E, Avg Training loss = 3.8895 acc>0.5= 175, acc>=0.3= 320, Total=714 ::Val Loss = 4.0974 acc>0.5= 134, acc>=0.3= 243, Total=714 \n",
      "[26]E, Avg Training loss = 3.8966 acc>0.5= 174, acc>=0.3= 327, Total=714 ::Val Loss = 4.122 acc>0.5= 127, acc>=0.3= 238, Total=714 \n",
      "[27]E, Avg Training loss = 3.8985 acc>0.5= 176, acc>=0.3= 325, Total=714 ::Val Loss = 4.1116 acc>0.5= 130, acc>=0.3= 238, Total=714 \n",
      "[28]E, Avg Training loss = 3.9115 acc>0.5= 170, acc>=0.3= 327, Total=714 ::Val Loss = 4.1297 acc>0.5= 133, acc>=0.3= 243, Total=714 \n",
      "[29]E, Avg Training loss = 3.9102 acc>0.5= 180, acc>=0.3= 338, Total=714 ::Val Loss = 4.1358 acc>0.5= 131, acc>=0.3= 235, Total=714 \n",
      "[30]E, Avg Training loss = 3.9143 acc>0.5= 180, acc>=0.3= 333, Total=714 ::Val Loss = 4.1377 acc>0.5= 133, acc>=0.3= 244, Total=714 \n",
      "[31]E, Avg Training loss = 3.9398 acc>0.5= 177, acc>=0.3= 325, Total=714 ::Val Loss = 4.1548 acc>0.5= 134, acc>=0.3= 235, Total=714 \n",
      "[32]E, Avg Training loss = 3.9275 acc>0.5= 184, acc>=0.3= 327, Total=714 ::Val Loss = 4.1795 acc>0.5= 133, acc>=0.3= 235, Total=714 \n",
      "[33]E, Avg Training loss = 3.9497 acc>0.5= 176, acc>=0.3= 318, Total=714 ::Val Loss = 4.1596 acc>0.5= 133, acc>=0.3= 245, Total=714 \n",
      "[34]E, Avg Training loss = 3.965 acc>0.5= 174, acc>=0.3= 319, Total=714 ::Val Loss = 4.1843 acc>0.5= 135, acc>=0.3= 234, Total=714 \n",
      "[35]E, Avg Training loss = 3.9782 acc>0.5= 183, acc>=0.3= 312, Total=714 ::Val Loss = 4.227 acc>0.5= 125, acc>=0.3= 234, Total=714 \n",
      "[36]E, Avg Training loss = 3.9833 acc>0.5= 184, acc>=0.3= 320, Total=714 ::Val Loss = 4.2249 acc>0.5= 135, acc>=0.3= 243, Total=714 \n",
      "[37]E, Avg Training loss = 4.0115 acc>0.5= 174, acc>=0.3= 314, Total=714 Epoch    37: reducing learning rate of group 0 to 5.0000e-04.\n",
      "::Val Loss = 4.2053 acc>0.5= 127, acc>=0.3= 235, Total=714 \n",
      "[38]E, Avg Training loss = 4.011 acc>0.5= 179, acc>=0.3= 320, Total=714 ::Val Loss = 4.1947 acc>0.5= 134, acc>=0.3= 241, Total=714 \n",
      "[39]E, Avg Training loss = 3.9816 acc>0.5= 181, acc>=0.3= 314, Total=714 ::Val Loss = 4.1885 acc>0.5= 138, acc>=0.3= 239, Total=714 \n",
      "[40]E, Avg Training loss = 3.9918 acc>0.5= 180, acc>=0.3= 310, Total=714 ::Val Loss = 4.1849 acc>0.5= 136, acc>=0.3= 231, Total=714 \n",
      "[41]E, Avg Training loss = 3.984 acc>0.5= 177, acc>=0.3= 316, Total=714 ::Val Loss = 4.1737 acc>0.5= 132, acc>=0.3= 242, Total=714 \n",
      "[42]E, Avg Training loss = 3.9657 acc>0.5= 177, acc>=0.3= 312, Total=714 ::Val Loss = 4.1758 acc>0.5= 130, acc>=0.3= 244, Total=714 \n",
      "[43]E, Avg Training loss = 3.9641 acc>0.5= 180, acc>=0.3= 320, Total=714 ::Val Loss = 4.1814 acc>0.5= 132, acc>=0.3= 236, Total=714 \n",
      "[44]E, Avg Training loss = 3.9798 acc>0.5= 181, acc>=0.3= 316, Total=714 ::Val Loss = 4.1783 acc>0.5= 132, acc>=0.3= 236, Total=714 \n",
      "[45]E, Avg Training loss = 3.978 acc>0.5= 180, acc>=0.3= 314, Total=714 ::Val Loss = 4.1741 acc>0.5= 137, acc>=0.3= 235, Total=714 \n",
      "[46]E, Avg Training loss = 3.9733 acc>0.5= 185, acc>=0.3= 313, Total=714 ::Val Loss = 4.1919 acc>0.5= 134, acc>=0.3= 235, Total=714 \n",
      "[47]E, Avg Training loss = 3.9913 acc>0.5= 182, acc>=0.3= 318, Total=714 ::Val Loss = 4.2006 acc>0.5= 132, acc>=0.3= 235, Total=714 \n",
      "[48]E, Avg Training loss = 3.9957 acc>0.5= 179, acc>=0.3= 316, Total=714 Epoch    48: reducing learning rate of group 0 to 2.5000e-04.\n",
      "::Val Loss = 4.1831 acc>0.5= 133, acc>=0.3= 241, Total=714 \n",
      "[49]E, Avg Training loss = 3.9824 acc>0.5= 182, acc>=0.3= 313, Total=714 ::Val Loss = 4.1764 acc>0.5= 132, acc>=0.3= 241, Total=714 \n",
      "[50]E, Avg Training loss = 3.9672 acc>0.5= 184, acc>=0.3= 316, Total=714 ::Val Loss = 4.1731 acc>0.5= 134, acc>=0.3= 237, Total=714 \n",
      "[51]E, Avg Training loss = 3.969 acc>0.5= 186, acc>=0.3= 320, Total=714 ::Val Loss = 4.1701 acc>0.5= 136, acc>=0.3= 239, Total=714 \n",
      "[52]E, Avg Training loss = 3.9621 acc>0.5= 179, acc>=0.3= 312, Total=714 ::Val Loss = 4.1716 acc>0.5= 134, acc>=0.3= 246, Total=714 \n",
      "[53]E, Avg Training loss = 3.9588 acc>0.5= 181, acc>=0.3= 317, Total=714 ::Val Loss = 4.1777 acc>0.5= 131, acc>=0.3= 243, Total=714 \n",
      "[54]E, Avg Training loss = 3.9709 acc>0.5= 183, acc>=0.3= 313, Total=714 ::Val Loss = 4.1871 acc>0.5= 136, acc>=0.3= 241, Total=714 \n",
      "[55]E, Avg Training loss = 3.9688 \n",
      "acc>0.5= 181, acc>=0.3= 317, Total=714 ::Val Loss = 4.1771 acc>0.5= 135, acc>=0.3= 242, Total=714 \n",
      "[56]E, Avg Training loss = 3.9716 acc>0.5= 184, acc>=0.3= 317, Total=714 ::Val Loss = 4.1799 acc>0.5= 135, acc>=0.3= 240, Total=714 \n",
      "[57]E, Avg Training loss = 3.9808 acc>0.5= 182, acc>=0.3= 322, Total=714 ::Val Loss = 4.1904 acc>0.5= 137, acc>=0.3= 246, Total=714 \n",
      "[58]E, Avg Training loss = 3.9854 acc>0.5= 178, acc>=0.3= 315, Total=714 ::Val Loss = 4.1909 acc>0.5= 135, acc>=0.3= 243, Total=714 \n",
      "[59]E, Avg Training loss = 3.9818 acc>0.5= 188, acc>=0.3= 317, Total=714 Epoch    59: reducing learning rate of group 0 to 1.2500e-04.\n",
      "::Val Loss = 4.1803 acc>0.5= 135, acc>=0.3= 240, Total=714 \n",
      "[60]E, Avg Training loss = 3.9775 acc>0.5= 185, acc>=0.3= 317, Total=714 ::Val Loss = 4.1774 acc>0.5= 133, acc>=0.3= 240, Total=714 \n",
      "[61]E, Avg Training loss = 3.9693 acc>0.5= 185, acc>=0.3= 313, Total=714 ::Val Loss = 4.171 acc>0.5= 135, acc>=0.3= 242, Total=714 \n",
      "[62]E, Avg Training loss = 3.9616 acc>0.5= 186, acc>=0.3= 314, Total=714 ::Val Loss = 4.1677 acc>0.5= 137, acc>=0.3= 242, Total=714 \n",
      "[63]E, Avg Training loss = 3.9576 acc>0.5= 181, acc>=0.3= 318, Total=714 ::Val Loss = 4.1632 acc>0.5= 136, acc>=0.3= 241, Total=714 \n",
      "[64]E, Avg Training loss = 3.9518 acc>0.5= 190, acc>=0.3= 318, Total=714 ::Val Loss = 4.1616 acc>0.5= 136, acc>=0.3= 239, Total=714 \n",
      "[65]E, Avg Training loss = 3.9492 acc>0.5= 187, acc>=0.3= 312, Total=714 ::Val Loss = 4.158 acc>0.5= 135, acc>=0.3= 242, Total=714 \n",
      "[66]E, Avg Training loss = 3.9459 acc>0.5= 188, acc>=0.3= 315, Total=714 ::Val Loss = 4.1587 acc>0.5= 137, acc>=0.3= 241, Total=714 \n",
      "[67]E, Avg Training loss = 3.9437 acc>0.5= 186, acc>=0.3= 317, Total=714 ::Val Loss = 4.1573 acc>0.5= 136, acc>=0.3= 239, Total=714 \n",
      "[68]E, Avg Training loss = 3.9411 acc>0.5= 182, acc>=0.3= 317, Total=714 ::Val Loss = 4.1561 acc>0.5= 137, acc>=0.3= 241, Total=714 \n",
      "[69]E, Avg Training loss = 3.9412 acc>0.5= 183, acc>=0.3= 323, Total=714 ::Val Loss = 4.1555 acc>0.5= 136, acc>=0.3= 241, Total=714 \n",
      "[70]E, Avg Training loss = 3.9426 acc>0.5= 185, acc>=0.3= 320, Total=714 Epoch    70: reducing learning rate of group 0 to 6.2500e-05.\n",
      "::Val Loss = 4.154 acc>0.5= 133, acc>=0.3= 242, Total=714 \n",
      "[71]E, Avg Training loss = 3.9376 acc>0.5= 188, acc>=0.3= 314, Total=714 ::Val Loss = 4.1513 acc>0.5= 135, acc>=0.3= 242, Total=714 \n",
      "[72]E, Avg Training loss = 3.935 acc>0.5= 181, acc>=0.3= 320, Total=714 ::Val Loss = 4.1499 acc>0.5= 135, acc>=0.3= 241, Total=714 \n",
      "[73]E, Avg Training loss = 3.9335 acc>0.5= 187, acc>=0.3= 317, Total=714 ::Val Loss = 4.1484 acc>0.5= 135, acc>=0.3= 239, Total=714 \n",
      "[74]E, Avg Training loss = 3.9316 acc>0.5= 180, acc>=0.3= 317, Total=714 ::Val Loss = 4.1469 acc>0.5= 135, acc>=0.3= 239, Total=714 \n",
      "[75]E, Avg Training loss = 3.9296 acc>0.5= 188, acc>=0.3= 315, Total=714 ::Val Loss = 4.1453 acc>0.5= 134, acc>=0.3= 239, Total=714 \n",
      "[76]E, Avg Training loss = 3.9261 acc>0.5= 187, acc>=0.3= 323, Total=714 ::Val Loss = 4.1441 acc>0.5= 134, acc>=0.3= 239, Total=714 \n",
      "[77]E, Avg Training loss = 3.924 acc>0.5= 186, acc>=0.3= 319, Total=714 ::Val Loss = 4.1427 acc>0.5= 135, acc>=0.3= 239, Total=714 \n",
      "[78]E, Avg Training loss = 3.9217 acc>0.5= 192, acc>=0.3= 322, Total=714 ::Val Loss = 4.1415 acc>0.5= 136, acc>=0.3= 238, Total=714 \n",
      "[79]E, Avg Training loss = 3.9214 acc>0.5= 184, acc>=0.3= 320, Total=714 ::Val Loss = 4.1403 acc>0.5= 135, acc>=0.3= 238, Total=714 \n",
      "[80]E, Avg Training loss = 3.9183 acc>0.5= 193, acc>=0.3= 319, Total=714 ::Val Loss = 4.1392 acc>0.5= 136, acc>=0.3= 238, Total=714 \n",
      "[81]E, Avg Training loss = 3.9181 acc>0.5= 184, acc>=0.3= 323, Total=714 Epoch    81: reducing learning rate of group 0 to 3.1250e-05.\n",
      "::Val Loss = 4.1388 acc>0.5= 136, acc>=0.3= 238, Total=714 \n",
      "[82]E, Avg Training loss = 3.9156 acc>0.5= 186, acc>=0.3= 321, Total=714 ::Val Loss = 4.1383 acc>0.5= 135, acc>=0.3= 238, Total=714 \n",
      "[83]E, Avg Training loss = 3.9155 acc>0.5= 189, acc>=0.3= 321, Total=714 ::Val Loss = 4.1378 acc>0.5= 135, acc>=0.3= 238, Total=714 \n",
      "[84]E, Avg Training loss = 3.9152 acc>0.5= 186, acc>=0.3= 314, Total=714 ::Val Loss = 4.1374 acc>0.5= 136, acc>=0.3= 238, Total=714 \n",
      "[85]E, Avg Training loss = 3.9148 acc>0.5= 185, acc>=0.3= 322, Total=714 ::Val Loss = 4.1368 acc>0.5= 137, acc>=0.3= 238, Total=714 \n",
      "[86]E, Avg Training loss = 3.9134 acc>0.5= 191, acc>=0.3= 317, Total=714 ::Val Loss = 4.1362 acc>0.5= 136, acc>=0.3= 238, Total=714 \n",
      "[87]E, Avg Training loss = 3.9132 acc>0.5= 182, acc>=0.3= 321, Total=714 ::Val Loss = 4.1358 acc>0.5= 137, acc>=0.3= 238, Total=714 \n",
      "[88]E, Avg Training loss = 3.9131 acc>0.5= 189, acc>=0.3= 320, Total=714 ::Val Loss = 4.1352 acc>0.5= 137, acc>=0.3= 238, Total=714 \n",
      "[89]E, Avg Training loss = 3.9115 acc>0.5= 186, acc>=0.3= 316, Total=714 ::Val Loss = 4.1348 acc>0.5= 136, acc>=0.3= 238, Total=714 \n",
      "[90]E, Avg Training loss = 3.911 acc>0.5= 188, acc>=0.3= 313, Total=714 ::Val Loss = 4.1342 acc>0.5= 137, acc>=0.3= 238, Total=714 \n",
      "[91]E, Avg Training loss = 3.9099 acc>0.5= 188, acc>=0.3= 316, Total=714 ::Val Loss = 4.1338 acc>0.5= 137, acc>=0.3= 238, Total=714 \n",
      "[92]E, Avg Training loss = 3.9089 acc>0.5= 188, acc>=0.3= 319, Total=714 Epoch    92: reducing learning rate of group 0 to 1.5625e-05.\n",
      "::Val Loss = 4.1336 acc>0.5= 137, acc>=0.3= 238, Total=714 \n",
      "[93]E, Avg Training loss = 3.9095 acc>0.5= 185, acc>=0.3= 317, Total=714 ::Val Loss = 4.1335 acc>0.5= 137, acc>=0.3= 238, Total=714 \n",
      "[94]E, Avg Training loss = 3.9086 acc>0.5= 188, acc>=0.3= 317, Total=714 ::Val Loss = 4.1332 acc>0.5= 137, acc>=0.3= 238, Total=714 \n",
      "[95]E, Avg Training loss = 3.9082 acc>0.5= 186, acc>=0.3= 315, Total=714 ::Val Loss = 4.1331 acc>0.5= 137, acc>=0.3= 238, Total=714 \n",
      "[96]E, Avg Training loss = 3.9075 acc>0.5= 186, acc>=0.3= 319, Total=714 ::Val Loss = 4.1329 acc>0.5= 137, acc>=0.3= 238, Total=714 \n",
      "[97]E, Avg Training loss = 3.9077 acc>0.5= 184, acc>=0.3= 314, Total=714 ::Val Loss = 4.1327 acc>0.5= 137, acc>=0.3= 238, Total=714 \n",
      "[98]E, Avg Training loss = 3.9065 acc>0.5= 189, acc>=0.3= 319, Total=714 ::Val Loss = 4.1325 acc>0.5= 137, acc>=0.3= 238, Total=714 \n",
      "[99]E, Avg Training loss = 3.9073 acc>0.5= 191, acc>=0.3= 318, Total=714 "
     ]
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hparams.lr = new_lr/100 #7.5e-12 # can devide by 10\n",
    "\n",
    "trainer.fit(model,dm) # Fit model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test(datamodule=dm) # testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traninng RNNs\n",
    "Finding the learning rate for the RNN model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48652lines [00:00, 86825.05lines/s]\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n",
      "\n",
      "   | Name                | Type                   | Params\n",
      "----------------------------------------------------------------\n",
      "0  | val_CM_normalized   | ConfusionMatrix        | 0     \n",
      "1  | val_CM_raw          | ConfusionMatrix        | 0     \n",
      "2  | train_CM_normalized | ConfusionMatrix        | 0     \n",
      "3  | train_CM_raw        | ConfusionMatrix        | 0     \n",
      "4  | test_CM             | ConfusionMatrix        | 0     \n",
      "5  | val_MCR             | MyClassificationReport | 0     \n",
      "6  | test_MCR            | MyClassificationReport | 0     \n",
      "7  | embedding           | Embedding              | 182 K \n",
      "8  | gru                 | GRU                    | 4.3 M \n",
      "9  | fc3                 | Linear                 | 366 K \n",
      "10 | softmax             | LogSoftmax             | 0     \n",
      "----------------------------------------------------------------\n",
      "4.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.9 M     Total params\n",
      "Restored states from the checkpoint file at /home/waris/Github/predict-future-alarms/project/lr_find_temp_model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before [0.0, 0.0, 0.008, 0.009, 0.059, 0.061, 0.063, 0.075, 0.075, 0.075, 0.076, 0.078, 0.083, 0.098, 0.101, 0.11, 0.143, 0.157, 0.158, 0.172, 0.176, 0.18, 0.186, 0.187, 0.251, 0.269, 0.31, 0.327, 0.337, 0.344, 0.35, 0.363, 0.371, 0.377, 0.377, 0.389, 0.39, 0.404, 0.418, 0.419, 0.426, 0.428, 0.428, 0.467, 0.47, 0.476, 0.48, 0.48, 0.486, 0.492, 0.495, 0.495, 0.513, 0.531, 0.537, 0.539, 0.546, 0.576, 0.58, 0.601, 0.624, 0.629, 0.63, 0.644, 0.644, 0.654, 0.66, 0.662, 0.667, 0.673, 0.673, 0.687, 0.706, 0.707, 0.708, 0.735, 0.746, 0.749, 0.764, 0.785, 0.802, 0.803, 0.807, 0.827, 0.848, 0.904, 0.905, 0.905, 0.909, 0.911, 0.919, 0.935, 0.966, 0.976, 0.978, 0.982, 0.983, 1.004, 1.013, 1.03, 1.039, 1.05, 1.051, 1.054, 1.081, 1.082, 1.096, 1.109, 1.115, 1.124, 1.14, 1.141, 1.155, 1.16, 1.173, 1.177, 1.188, 1.198, 1.2, 1.201, 1.215, 1.235, 1.24, 1.25, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.264, 1.279, 1.283, 1.284, 1.303, 1.304, 1.316, 1.333, 1.338, 1.392, 1.398, 1.447, 1.496, 1.502, 1.508, 1.513, 1.516, 1.527, 1.547, 1.571, 1.588, 1.597, 1.645, 1.666, 1.691, 1.703, 1.703, 1.718, 1.736, 1.738, 1.747, 1.749, 1.758, 1.762, 1.809, 1.827, 1.839, 1.865, 1.867, 1.867, 1.89, 1.89, 1.895, 1.941, 1.959, 1.97, 2.023, 2.049, 2.052, 2.061, 2.109, 2.112, 2.115, 2.118, 2.173, 2.176, 2.241, 2.259, 2.259, 2.269, 2.269, 2.295, 2.302, 2.321, 2.336, 2.355, 2.355, 2.39, 2.443, 2.452, 2.538, 2.538, 2.538, 2.57, 2.57, 2.598, 2.636, 2.656, 2.656, 2.711, 2.711, 2.721, 2.727, 2.742, 2.779, 2.807, 2.807, 2.818, 2.823, 2.829, 2.829, 2.846, 2.851, 2.851, 2.857, 2.863, 2.874, 2.88, 2.963, 2.994, 3.0, 3.007, 3.007, 3.051, 3.058, 3.084, 3.111, 3.118, 3.125, 3.223, 3.223, 3.237, 3.245, 3.245, 3.274, 3.282, 3.312, 3.328, 3.335, 3.415, 3.423, 3.448, 3.464, 3.55, 3.577, 3.622, 3.687, 3.696, 3.725, 3.745, 3.804, 3.835, 3.835, 3.897, 3.908, 3.919, 3.919, 3.94, 3.951, 3.973, 3.995, 3.995, 4.04, 4.074, 4.074, 4.098, 4.098, 4.109, 4.145, 4.157, 4.169, 4.181, 4.194, 4.194, 4.194, 4.218, 4.294, 4.333, 4.413, 4.427, 4.567, 4.612, 4.641, 4.672, 4.687, 4.733, 4.797, 4.813, 4.829, 4.862, 4.928, 4.963, 4.98, 5.015, 5.122, 5.14, 5.178, 5.196, 5.234, 5.253, 5.253, 5.273, 5.273, 5.351, 5.392, 5.412, 5.433, 5.433, 5.474, 5.516, 5.537, 5.537, 5.602, 5.624, 5.624, 5.646, 5.669, 5.669, 5.691, 5.691, 5.691, 5.737, 5.737, 5.737, 5.783, 5.806, 5.83, 5.83, 5.854, 5.878, 5.926, 5.951, 5.951, 5.976, 6.001, 6.001, 6.051, 6.103, 6.103, 6.103, 6.129, 6.182, 6.209, 6.318, 6.318, 6.374, 6.431, 6.431, 6.46, 6.49, 6.549, 6.579, 6.671, 6.862, 6.895, 6.962, 6.962, 7.03, 7.065, 7.135, 7.171, 7.28, 7.317, 7.355, 7.355, 7.393, 7.47, 7.509, 7.548, 7.669, 7.669, 7.711, 7.752, 7.752, 7.794, 7.794, 7.794, 7.794, 7.837, 7.837, 7.837, 7.88, 7.88, 7.968, 7.968, 7.968, 7.968, 8.012, 8.012, 8.012, 8.057, 8.103, 8.149, 8.149, 8.195, 8.195, 8.242, 8.387, 8.387, 8.436, 8.486, 8.486, 8.486, 8.64, 8.745, 8.853, 8.908, 8.964, 9.194, 9.253, 9.253, 9.374, 9.435, 9.435, 9.435, 9.435, 9.498, 9.498, 9.498, 9.561, 9.69, 9.69, 9.69, 9.823, 9.823, 10.1, 10.1, 10.1, 10.172, 10.393, 10.393, 10.393, 10.469, 10.469, 10.545, 10.703, 10.703, 10.783, 10.865, 11.032, 11.032, 11.118, 11.205, 11.205, 11.205, 11.293, 11.382, 11.474, 11.474, 11.566, 11.66, 11.66, 11.66, 11.756, 11.756, 11.853, 11.853, 11.853, 11.952, 11.952, 11.952, 12.052, 12.154, 12.154, 12.258, 12.364, 12.364, 12.364, 12.471, 12.581, 12.581, 12.581, 12.581, 12.692, 12.692, 12.692, 12.805, 12.921, 12.921, 12.921, 13.158, 13.28, 13.28, 13.404, 13.404, 13.404, 13.404, 13.659, 13.659, 13.659, 13.659, 13.659, 13.79, 13.79, 13.79, 14.061, 14.061, 14.2, 14.2, 14.2, 14.2, 14.2, 14.2, 14.2, 14.342, 14.342, 14.342, 14.342, 14.342, 14.487, 14.487, 14.487, 14.487, 14.635, 14.635, 14.785, 14.785, 14.785, 14.785, 14.939, 14.939, 14.939, 14.939, 14.939, 14.939, 15.097, 15.257, 15.257, 15.257, 15.257, 15.421, 15.421, 15.421, 15.589, 15.589, 15.589, 15.76, 15.76, 15.76, 15.935, 16.114, 16.298, 16.298, 16.298, 16.298, 16.298, 16.485, 16.485, 16.485, 16.485, 16.485, 16.677, 16.677, 16.677, 16.677, 16.677, 17.074, 17.074, 17.074, 17.279, 17.49, 17.49, 17.706, 17.706, 17.927, 17.927, 18.154, 18.154, 18.154, 18.387, 18.387, 18.387, 18.387, 18.626, 18.626, 18.626, 18.626, 18.871, 19.123, 19.123, 19.123, 19.381, 19.381, 19.381, 19.381, 19.381, 19.646, 19.646, 19.646, 19.646, 19.646, 19.919, 19.919, 19.919, 19.919, 19.919, 19.919, 19.919, 20.2, 20.2, 20.2, 20.2, 20.2, 20.488, 20.488, 20.488, 20.785, 21.091, 21.091, 21.091, 21.091, 21.406, 21.406, 21.406, 21.73, 21.73, 21.73, 21.73, 21.73, 22.064, 22.064, 22.064, 22.064, 22.064, 22.064, 22.064, 22.064, 22.409, 22.409, 22.409, 22.409, 22.409, 22.409, 22.409, 22.765, 23.132, 23.132, 23.132, 23.132, 23.511, 23.511, 23.511, 23.903, 23.903, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.727, 24.727, 24.727, 24.727, 25.161, 25.161, 25.161, 25.61, 25.61, 25.61, 25.61, 25.61, 26.076, 26.076, 26.076, 26.076, 27.06, 27.06, 27.06, 27.06, 27.581, 27.581, 27.581, 27.581, 28.684, 29.269, 29.879, 30.515, 30.515, 31.178, 31.178, 31.178, 31.178, 31.871, 34.147, 34.98, 34.98, 35.855, 36.774, 36.774, 36.774, 47.806, 49.455, 95.613, 143.419]\n",
      "After [0.0, 0.0, 0.008, 0.009, 0.059, 0.061, 0.063, 0.075, 0.075, 0.075, 0.076, 0.078, 0.083, 0.098, 0.101, 0.11, 0.143, 0.157, 0.158, 0.172, 0.176, 0.18, 0.186, 0.187, 0.251, 0.269, 0.31, 0.327, 0.337, 0.344, 0.35, 0.363, 0.371, 0.377, 0.377, 0.389, 0.39, 0.404, 0.418, 0.419, 0.426, 0.428, 0.428, 0.467, 0.47, 0.476, 0.48, 0.48, 0.486, 0.492, 0.495, 0.495, 0.513, 0.531, 0.537, 0.539, 0.546, 0.576, 0.58, 0.601, 0.624, 0.629, 0.63, 0.644, 0.644, 0.654, 0.66, 0.662, 0.667, 0.673, 0.673, 0.687, 0.706, 0.707, 0.708, 0.735, 0.746, 0.749, 0.764, 0.785, 0.802, 0.803, 0.807, 0.827, 0.848, 0.904, 0.905, 0.905, 0.909, 0.911, 0.919, 0.935, 0.966, 0.976, 0.978, 0.982, 0.983, 1.004, 1.013, 1.03, 1.039, 1.05, 1.051, 1.054, 1.081, 1.082, 1.096, 1.109, 1.115, 1.124, 1.14, 1.141, 1.155, 1.16, 1.173, 1.177, 1.188, 1.198, 1.2, 1.201, 1.215, 1.235, 1.24, 1.25, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.254, 1.264, 1.279, 1.283, 1.284, 1.303, 1.304, 1.316, 1.333, 1.338, 1.392, 1.398, 1.447, 1.496, 1.502, 1.508, 1.513, 1.516, 1.527, 1.547, 1.571, 1.588, 1.597, 1.645, 1.666, 1.691, 1.703, 1.703, 1.718, 1.736, 1.738, 1.747, 1.749, 1.758, 1.762, 1.809, 1.827, 1.839, 1.865, 1.867, 1.867, 1.89, 1.89, 1.895, 1.941, 1.959, 1.97, 2.023, 2.049, 2.052, 2.061, 2.109, 2.112, 2.115, 2.118, 2.173, 2.176, 2.241, 2.259, 2.259, 2.269, 2.269, 2.295, 2.302, 2.321, 2.336, 2.355, 2.355, 2.39, 2.443, 2.452, 2.538, 2.538, 2.538, 2.57, 2.57, 2.598, 2.636, 2.656, 2.656, 2.711, 2.711, 2.721, 2.727, 2.742, 2.779, 2.807, 2.807, 2.818, 2.823, 2.829, 2.829, 2.846, 2.851, 2.851, 2.857, 2.863, 2.874, 2.88, 2.963, 2.994, 3.0, 3.007, 3.007, 3.051, 3.058, 3.084, 3.111, 3.118, 3.125, 3.223, 3.223, 3.237, 3.245, 3.245, 3.274, 3.282, 3.312, 3.328, 3.335, 3.415, 3.423, 3.448, 3.464, 3.55, 3.577, 3.622, 3.687, 3.696, 3.725, 3.745, 3.804, 3.835, 3.835, 3.897, 3.908, 3.919, 3.919, 3.94, 3.951, 3.973, 3.995, 3.995, 4.04, 4.074, 4.074, 4.098, 4.098, 4.109, 4.145, 4.157, 4.169, 4.181, 4.194, 4.194, 4.194, 4.218, 4.294, 4.333, 4.413, 4.427, 4.567, 4.612, 4.641, 4.672, 4.687, 4.733, 4.797, 4.813, 4.829, 4.862, 4.928, 4.963, 4.98, 5.015, 5.122, 5.14, 5.178, 5.196, 5.234, 5.253, 5.253, 5.273, 5.273, 5.351, 5.392, 5.412, 5.433, 5.433, 5.474, 5.516, 5.537, 5.537, 5.602, 5.624, 5.624, 5.646, 5.669, 5.669, 5.691, 5.691, 5.691, 5.737, 5.737, 5.737, 5.783, 5.806, 5.83, 5.83, 5.854, 5.878, 5.926, 5.951, 5.951, 5.976, 6.001, 6.001, 6.051, 6.103, 6.103, 6.103, 6.129, 6.182, 6.209, 6.318, 6.318, 6.374, 6.431, 6.431, 6.46, 6.49, 6.549, 6.579, 6.671, 6.862, 6.895, 6.962, 6.962, 7.03, 7.065, 7.135, 7.171, 7.28, 7.317, 7.355, 7.355, 7.393, 7.47, 7.509, 7.548, 7.669, 7.669, 7.711, 7.752, 7.752, 7.794, 7.794, 7.794, 7.794, 7.837, 7.837, 7.837, 7.88, 7.88, 7.968, 7.968, 7.968, 7.968, 8.012, 8.012, 8.012, 8.057, 8.103, 8.149, 8.149, 8.195, 8.195, 8.242, 8.387, 8.387, 8.436, 8.486, 8.486, 8.486, 8.64, 8.745, 8.853, 8.908, 8.964, 9.194, 9.253, 9.253, 9.374, 9.435, 9.435, 9.435, 9.435, 9.498, 9.498, 9.498, 9.561, 9.69, 9.69, 9.69, 9.823, 9.823, 10.1, 10.1, 10.1, 10.172, 10.393, 10.393, 10.393, 10.469, 10.469, 10.545, 10.703, 10.703, 10.783, 10.865, 11.032, 11.032, 11.118, 11.205, 11.205, 11.205, 11.293, 11.382, 11.474, 11.474, 11.566, 11.66, 11.66, 11.66, 11.756, 11.756, 11.853, 11.853, 11.853, 11.952, 11.952, 11.952, 12.052, 12.154, 12.154, 12.258, 12.364, 12.364, 12.364, 12.471, 12.581, 12.581, 12.581, 12.581, 12.692, 12.692, 12.692, 12.805, 12.921, 12.921, 12.921, 13.158, 13.28, 13.28, 13.404, 13.404, 13.404, 13.404, 13.659, 13.659, 13.659, 13.659, 13.659, 13.79, 13.79, 13.79, 14.061, 14.061, 14.2, 14.2, 14.2, 14.2, 14.2, 14.2, 14.2, 14.342, 14.342, 14.342, 14.342, 14.342, 14.487, 14.487, 14.487, 14.487, 14.635, 14.635, 14.785, 14.785, 14.785, 14.785, 14.939, 14.939, 14.939, 14.939, 14.939, 14.939, 15.097, 15.257, 15.257, 15.257, 15.257, 15.421, 15.421, 15.421, 15.589, 15.589, 15.589, 15.76, 15.76, 15.76, 15.935, 16.114, 16.298, 16.298, 16.298, 16.298, 16.298, 16.485, 16.485, 16.485, 16.485, 16.485, 16.677, 16.677, 16.677, 16.677, 16.677, 17.074, 17.074, 17.074, 17.279, 17.49, 17.49, 17.706, 17.706, 17.927, 17.927, 18.154, 18.154, 18.154, 18.387, 18.387, 18.387, 18.387, 18.626, 18.626, 18.626, 18.626, 18.871, 19.123, 19.123, 19.123, 19.381, 19.381, 19.381, 19.381, 19.381, 19.646, 19.646, 19.646, 19.646, 19.646, 19.919, 19.919, 19.919, 19.919, 19.919, 19.919, 19.919, 20.2, 20.2, 20.2, 20.2, 20.2, 20.488, 20.488, 20.488, 20.785, 21.091, 21.091, 21.091, 21.091, 21.406, 21.406, 21.406, 21.73, 21.73, 21.73, 21.73, 21.73, 22.064, 22.064, 22.064, 22.064, 22.064, 22.064, 22.064, 22.064, 22.409, 22.409, 22.409, 22.409, 22.409, 22.409, 22.409, 22.765, 23.132, 23.132, 23.132, 23.132, 23.511, 23.511, 23.511, 23.903, 23.903, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.308, 24.727, 24.727, 24.727, 24.727, 25.161, 25.161, 25.161, 25.61, 25.61, 25.61, 25.61, 25.61, 26.076, 26.076, 26.076, 26.076, 27.06, 27.06, 27.06, 27.06, 27.581, 27.581, 27.581, 27.581, 28.684, 29.269, 29.879, 30.515, 30.515, 31.178, 31.178, 31.178, 31.178, 31.871, 34.147, 34.98, 34.98, 35.855, 36.774, 36.774, 36.774, 47.806, 49.455, 95.613, 143.419]\n",
      "::Val Loss = 6.5738 acc>0.5= 0, acc>=0.3= 1, Total=714 \n",
      "::Val Loss = 6.5801 acc>0.5= 1, acc>=0.3= 1, Total=714 \n",
      "[0]E, Avg Training loss = 6.5808 acc>0.5= 1, acc>=0.3= 1, Total=714 ::Val Loss = 6.5751 acc>0.5= 1, acc>=0.3= 1, Total=714 \n",
      "[1]E, Avg Training loss = 6.5793 acc>0.5= 1, acc>=0.3= 1, Total=714 ::Val Loss = 6.4958 acc>0.5= 3, acc>=0.3= 4, Total=714 \n",
      "[2]E, Avg Training loss = 6.5541 acc>0.5= 1, acc>=0.3= 1, Total=714 ::Val Loss = 4.6603 acc>0.5= 2, acc>=0.3= 3, Total=714 \n",
      "[3]E, Avg Training loss = 6.0428 acc>0.5= 3, acc>=0.3= 3, Total=714 ::Val Loss = 6.5935 acc>0.5= 0, acc>=0.3= 1, Total=714 \n",
      "[4]E, Avg Training loss = 4.4694 acc>0.5= 2, acc>=0.3= 3, Total=714 ::Val Loss = 12.8655 acc>0.5= 1, acc>=0.3= 1, Total=714 \n",
      "[5]E, Avg Training loss = 8.8668 acc>0.5= 0, acc>=0.3= 1, Total=714 [6]E, Avg Training loss = 33.173 acc>0.5= 0, acc>=0.3= 0, Total=714 Suggested lr = 0.001584893192461114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3ca3be8e0847c1b3760ac8af16eb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value='Finding best initial lr'), FloatProgress(value=0.0), HTML(value='')))"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light",
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setup data\n",
    "config_data = {\n",
    "'dir-path' : \"../.data/\",\n",
    "'batch-size' :512, # Batch Size \n",
    "'seq-len' :128, # Sequence length\n",
    "}\n",
    "\n",
    "dm = MyDataModule(config=config_data)\n",
    "ws = dm.get_weight_per_class().cuda()\n",
    "\n",
    "print(\"Before\",[round(w.item(),3) for w in ws])\n",
    "# avg_w = sum(ws)/len(ws)\n",
    "# ws = torch.tensor([weightCondition(w,avg_w) for w in ws]).cuda()\n",
    "print(\"After\",[round(w.item(),3) for w in ws])\n",
    "\n",
    "\n",
    "config_model = {\n",
    "    'lr' : 0.001,\n",
    "    'dropout' : 0.05,\n",
    "    'weight-decay': 3.1, #3.1,\n",
    "    'em-size' :256, # embedding dimension \n",
    "    'nhid' : 512, # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "    'nlayers' :3, # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    'seq-len': config_data['seq-len'], # dont use wandb config \n",
    "    'vocab-size':len(dm.vocab.stoi), # the size of vocabulary /also called tokens\n",
    "    'weight_per_class':ws,\n",
    "    \"val-file\":\"val-out-rnn.txt\",\n",
    "    \"train-file\":'train-out-rnn.txt',\n",
    "    \"vocab\": dm.vocab,\n",
    "    \"batch-size\":config_data['batch-size']\n",
    "}\n",
    "\n",
    "with open (config_model[\"val-file\"],'w') as f:\n",
    "    f.write(\">> Starting\")\n",
    "\n",
    "with open (config_model[\"train-file\"],'w') as f:\n",
    "    f.write(\">> Starting\")\n",
    "\n",
    "\n",
    "\n",
    "model = AlarmGRU(config=config_model)\n",
    "model.initialize_hidden()\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "        monitor='val_epoch_loss',\n",
    "        min_delta=0,\n",
    "        patience=100,\n",
    "        verbose=True,\n",
    "        mode='min'\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(auto_lr_find=0.0001, precision=16,gpus=-1, num_nodes=1,  max_epochs=100, check_val_every_n_epoch=1,deterministic=True,gradient_clip_val=0.5,enable_pl_optimizer=True,callbacks=[early_stop_callback],progress_bar_refresh_rate=0)\n",
    "\n",
    "# Run learning rate finder\n",
    "lr_finder = trainer.tuner.lr_find(model,dm)\n",
    "\n",
    "# Results can be found in\n",
    "lr_finder.results\n",
    "\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()\n",
    "\n",
    "# Pick point based on plot, or get suggestion\n",
    "new_lr = lr_finder.suggestion()\n",
    "\n",
    "print(f\"Suggested lr = {new_lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                | Type                   | Params\n",
      "----------------------------------------------------------------\n",
      "0  | val_CM_normalized   | ConfusionMatrix        | 0     \n",
      "1  | val_CM_raw          | ConfusionMatrix        | 0     \n",
      "2  | train_CM_normalized | ConfusionMatrix        | 0     \n",
      "3  | train_CM_raw        | ConfusionMatrix        | 0     \n",
      "4  | test_CM             | ConfusionMatrix        | 0     \n",
      "5  | val_MCR             | MyClassificationReport | 0     \n",
      "6  | test_MCR            | MyClassificationReport | 0     \n",
      "7  | embedding           | Embedding              | 182 K \n",
      "8  | gru                 | GRU                    | 4.3 M \n",
      "9  | fc3                 | Linear                 | 366 K \n",
      "10 | softmax             | LogSoftmax             | 0     \n",
      "----------------------------------------------------------------\n",
      "4.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.9 M     Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::Val Loss = 6.5756 acc>0.5= 0, acc>=0.3= 1, Total=714 \n",
      "::Val Loss = 4.3221 acc>0.5= 2, acc>=0.3= 4, Total=714 \n",
      "[1]E, Avg Training loss = 5.566 acc>0.5= 1, acc>=0.3= 1, Total=714 ::Val Loss = 4.0119 acc>0.5= 3, acc>=0.3= 4, Total=714 \n",
      "[2]E, Avg Training loss = 3.9717 acc>0.5= 2, acc>=0.3= 5, Total=714 ::Val Loss = 3.7301 acc>0.5= 6, acc>=0.3= 8, Total=714 \n",
      "[3]E, Avg Training loss = 3.6393 acc>0.5= 6, acc>=0.3= 6, Total=714 ::Val Loss = 3.6232 acc>0.5= 7, acc>=0.3= 7, Total=714 \n",
      "[4]E, Avg Training loss = 3.5213 acc>0.5= 6, acc>=0.3= 9, Total=714 ::Val Loss = 3.6809 acc>0.5= 6, acc>=0.3= 7, Total=714 \n",
      "[5]E, Avg Training loss = 3.4759 acc>0.5= 9, acc>=0.3= 11, Total=714 ::Val Loss = 3.5323 acc>0.5= 8, acc>=0.3= 10, Total=714 \n",
      "[6]E, Avg Training loss = 3.4237 acc>0.5= 10, acc>=0.3= 10, Total=714 ::Val Loss = 3.3341 acc>0.5= 7, acc>=0.3= 8, Total=714 \n",
      "[7]E, Avg Training loss = 3.329 acc>0.5= 11, acc>=0.3= 12, Total=714 ::Val Loss = 3.3445 acc>0.5= 9, acc>=0.3= 12, Total=714 \n",
      "[8]E, Avg Training loss = 3.2427 acc>0.5= 8, acc>=0.3= 10, Total=714 ::Val Loss = 3.3328 acc>0.5= 8, acc>=0.3= 16, Total=714 \n",
      "[9]E, Avg Training loss = 3.1609 acc>0.5= 15, acc>=0.3= 16, Total=714 ::Val Loss = 3.2668 acc>0.5= 7, acc>=0.3= 9, Total=714 \n",
      "[10]E, Avg Training loss = 3.1272 acc>0.5= 12, acc>=0.3= 16, Total=714 ::Val Loss = 3.2097 acc>0.5= 12, acc>=0.3= 15, Total=714 \n",
      "[11]E, Avg Training loss = 3.0664 acc>0.5= 13, acc>=0.3= 17, Total=714 ::Val Loss = 3.2512 acc>0.5= 6, acc>=0.3= 13, Total=714 \n",
      "[12]E, Avg Training loss = 2.9899 acc>0.5= 15, acc>=0.3= 24, Total=714 ::Val Loss = 3.1678 acc>0.5= 10, acc>=0.3= 16, Total=714 \n",
      "[13]E, Avg Training loss = 3.0397 acc>0.5= 11, acc>=0.3= 21, Total=714 ::Val Loss = 3.0474 acc>0.5= 10, acc>=0.3= 18, Total=714 \n",
      "[14]E, Avg Training loss = 2.9625 acc>0.5= 15, acc>=0.3= 30, Total=714 ::Val Loss = 3.0839 acc>0.5= 12, acc>=0.3= 20, Total=714 \n",
      "[15]E, Avg Training loss = 2.8555 acc>0.5= 13, acc>=0.3= 24, Total=714 ::Val Loss = 3.0311 acc>0.5= 10, acc>=0.3= 20, Total=714 \n",
      "[16]E, Avg Training loss = 2.867 acc>0.5= 23, acc>=0.3= 37, Total=714 ::Val Loss = 3.0654 acc>0.5= 16, acc>=0.3= 24, Total=714 \n",
      "[17]E, Avg Training loss = 2.7977 acc>0.5= 20, acc>=0.3= 36, Total=714 ::Val Loss = 3.0403 acc>0.5= 7, acc>=0.3= 15, Total=714 \n",
      "[18]E, Avg Training loss = 2.8815 acc>0.5= 21, acc>=0.3= 38, Total=714 ::Val Loss = 3.0934 acc>0.5= 17, acc>=0.3= 26, Total=714 \n",
      "[19]E, Avg Training loss = 2.7945 acc>0.5= 21, acc>=0.3= 32, Total=714 ::Val Loss = 2.9805 acc>0.5= 15, acc>=0.3= 26, Total=714 \n",
      "[20]E, Avg Training loss = 2.8099 acc>0.5= 30, acc>=0.3= 47, Total=714 ::Val Loss = 2.8604 acc>0.5= 18, acc>=0.3= 32, Total=714 \n",
      "[21]E, Avg Training loss = 2.6885 acc>0.5= 24, acc>=0.3= 42, Total=714 ::Val Loss = 2.8931 acc>0.5= 10, acc>=0.3= 24, Total=714 \n",
      "[22]E, Avg Training loss = 2.6485 acc>0.5= 29, acc>=0.3= 55, Total=714 ::Val Loss = 2.8988 acc>0.5= 16, acc>=0.3= 34, Total=714 \n",
      "[23]E, Avg Training loss = 2.6619 acc>0.5= 27, acc>=0.3= 48, Total=714 ::Val Loss = 2.9279 acc>0.5= 11, acc>=0.3= 25, Total=714 \n",
      "[24]E, Avg Training loss = 2.6694 acc>0.5= 34, acc>=0.3= 66, Total=714 ::Val Loss = 2.9027 acc>0.5= 17, acc>=0.3= 38, Total=714 \n",
      "[25]E, Avg Training loss = 2.7083 acc>0.5= 22, acc>=0.3= 55, Total=714 ::Val Loss = 2.8759 acc>0.5= 13, acc>=0.3= 25, Total=714 \n",
      "[26]E, Avg Training loss = 2.7048 acc>0.5= 42, acc>=0.3= 67, Total=714 ::Val Loss = 2.8608 acc>0.5= 14, acc>=0.3= 30, Total=714 \n",
      "[27]E, Avg Training loss = 2.6353 acc>0.5= 24, acc>=0.3= 56, Total=714 ::Val Loss = 2.8733 acc>0.5= 19, acc>=0.3= 33, Total=714 \n",
      "[28]E, Avg Training loss = 2.6211 acc>0.5= 28, acc>=0.3= 52, Total=714 ::Val Loss = 2.8501 acc>0.5= 15, acc>=0.3= 30, Total=714 \n",
      "[29]E, Avg Training loss = 2.6728 acc>0.5= 33, acc>=0.3= 59, Total=714 ::Val Loss = 2.8937 acc>0.5= 22, acc>=0.3= 38, Total=714 \n",
      "[30]E, Avg Training loss = 2.6261 acc>0.5= 31, acc>=0.3= 53, Total=714 ::Val Loss = 2.8863 acc>0.5= 13, acc>=0.3= 35, Total=714 \n",
      "[31]E, Avg Training loss = 2.6315 acc>0.5= 41, acc>=0.3= 65, Total=714 ::Val Loss = 2.8073 acc>0.5= 17, acc>=0.3= 35, Total=714 \n",
      "[32]E, Avg Training loss = 2.6017 acc>0.5= 29, acc>=0.3= 65, Total=714 ::Val Loss = 2.8585 acc>0.5= 20, acc>=0.3= 43, Total=714 \n",
      "[33]E, Avg Training loss = 2.5576 acc>0.5= 44, acc>=0.3= 69, Total=714 ::Val Loss = 2.7537 acc>0.5= 20, acc>=0.3= 40, Total=714 \n",
      "[34]E, Avg Training loss = 2.5879 acc>0.5= 42, acc>=0.3= 71, Total=714 ::Val Loss = 2.8064 acc>0.5= 27, acc>=0.3= 44, Total=714 \n",
      "[35]E, Avg Training loss = 2.518 acc>0.5= 44, acc>=0.3= 74, Total=714 ::Val Loss = 2.7936 acc>0.5= 24, acc>=0.3= 48, Total=714 \n",
      "[36]E, Avg Training loss = 2.5663 acc>0.5= 44, acc>=0.3= 79, Total=714 ::Val Loss = 2.7966 acc>0.5= 20, acc>=0.3= 43, Total=714 \n",
      "[37]E, Avg Training loss = 2.6004 acc>0.5= 45, acc>=0.3= 82, Total=714 ::Val Loss = 2.7423 acc>0.5= 22, acc>=0.3= 49, Total=714 \n",
      "[38]E, Avg Training loss = 2.542 acc>0.5= 42, acc>=0.3= 77, Total=714 ::Val Loss = 2.7193 acc>0.5= 22, acc>=0.3= 46, Total=714 \n",
      "[39]E, Avg Training loss = 2.4961 acc>0.5= 44, acc>=0.3= 80, Total=714 ::Val Loss = 2.7611 acc>0.5= 23, acc>=0.3= 44, Total=714 \n",
      "[40]E, Avg Training loss = 2.4758 acc>0.5= 42, acc>=0.3= 83, Total=714 ::Val Loss = 2.7587 acc>0.5= 19, acc>=0.3= 47, Total=714 \n",
      "[41]E, Avg Training loss = 2.5301 acc>0.5= 43, acc>=0.3= 76, Total=714 ::Val Loss = 2.7497 acc>0.5= 24, acc>=0.3= 46, Total=714 \n",
      "[42]E, Avg Training loss = 2.531 acc>0.5= 49, acc>=0.3= 82, Total=714 ::Val Loss = 2.7774 acc>0.5= 21, acc>=0.3= 48, Total=714 \n",
      "[43]E, Avg Training loss = 2.5182 acc>0.5= 42, acc>=0.3= 81, Total=714 ::Val Loss = 2.7085 \n",
      "acc>0.5= 24, acc>=0.3= 43, Total=714 \n",
      "[44]E, Avg Training loss = 2.5303 acc>0.5= 40, acc>=0.3= 79, Total=714 ::Val Loss = 2.7804 acc>0.5= 18, acc>=0.3= 38, Total=714 \n",
      "[45]E, Avg Training loss = 2.4946 acc>0.5= 42, acc>=0.3= 83, Total=714 ::Val Loss = 2.7646 acc>0.5= 23, acc>=0.3= 41, Total=714 \n",
      "[46]E, Avg Training loss = 2.5404 acc>0.5= 39, acc>=0.3= 66, Total=714 ::Val Loss = 2.7426 acc>0.5= 20, acc>=0.3= 43, Total=714 \n",
      "[47]E, Avg Training loss = 2.5516 acc>0.5= 43, acc>=0.3= 68, Total=714 ::Val Loss = 2.755 acc>0.5= 25, acc>=0.3= 52, Total=714 \n",
      "[48]E, Avg Training loss = 2.5333 acc>0.5= 38, acc>=0.3= 68, Total=714 ::Val Loss = 2.7719 acc>0.5= 27, acc>=0.3= 50, Total=714 \n",
      "[49]E, Avg Training loss = 2.5921 acc>0.5= 54, acc>=0.3= 93, Total=714 ::Val Loss = 2.6589 acc>0.5= 25, acc>=0.3= 52, Total=714 \n",
      "[50]E, Avg Training loss = 2.4495 acc>0.5= 49, acc>=0.3= 93, Total=714 ::Val Loss = 2.7004 acc>0.5= 26, acc>=0.3= 52, Total=714 \n",
      "[51]E, Avg Training loss = 2.4078 acc>0.5= 43, acc>=0.3= 80, Total=714 ::Val Loss = 2.7088 acc>0.5= 23, acc>=0.3= 50, Total=714 \n",
      "[52]E, Avg Training loss = 2.4814 acc>0.5= 53, acc>=0.3= 93, Total=714 ::Val Loss = 2.7138 acc>0.5= 27, acc>=0.3= 59, Total=714 \n",
      "[53]E, Avg Training loss = 2.4722 acc>0.5= 51, acc>=0.3= 101, Total=714 ::Val Loss = 2.6606 acc>0.5= 21, acc>=0.3= 61, Total=714 \n",
      "[54]E, Avg Training loss = 2.4391 acc>0.5= 53, acc>=0.3= 104, Total=714 ::Val Loss = 2.6987 acc>0.5= 28, acc>=0.3= 58, Total=714 \n",
      "[55]E, Avg Training loss = 2.3997 acc>0.5= 45, acc>=0.3= 92, Total=714 ::Val Loss = 2.6578 acc>0.5= 28, acc>=0.3= 54, Total=714 \n",
      "[56]E, Avg Training loss = 2.4604 acc>0.5= 57, acc>=0.3= 97, Total=714 ::Val Loss = 2.7343 acc>0.5= 23, acc>=0.3= 43, Total=714 \n",
      "[57]E, Avg Training loss = 2.4342 acc>0.5= 48, acc>=0.3= 99, Total=714 ::Val Loss = 2.6871 acc>0.5= 27, acc>=0.3= 57, Total=714 \n",
      "[58]E, Avg Training loss = 2.4521 acc>0.5= 45, acc>=0.3= 90, Total=714 ::Val Loss = 2.6859 acc>0.5= 24, acc>=0.3= 49, Total=714 \n",
      "[59]E, Avg Training loss = 2.5132 acc>0.5= 51, acc>=0.3= 109, Total=714 ::Val Loss = 2.7 acc>0.5= 28, acc>=0.3= 64, Total=714 \n",
      "[60]E, Avg Training loss = 2.4483 acc>0.5= 42, acc>=0.3= 85, Total=714 ::Val Loss = 2.6726 acc>0.5= 26, acc>=0.3= 51, Total=714 \n",
      "[61]E, Avg Training loss = 2.4087 acc>0.5= 52, acc>=0.3= 99, Total=714 ::Val Loss = 2.644 acc>0.5= 31, acc>=0.3= 64, Total=714 \n",
      "[62]E, Avg Training loss = 2.4172 acc>0.5= 41, acc>=0.3= 97, Total=714 ::Val Loss = 2.6853 acc>0.5= 28, acc>=0.3= 52, Total=714 \n",
      "[63]E, Avg Training loss = 2.378 acc>0.5= 50, acc>=0.3= 101, Total=714 ::Val Loss = 2.6542 acc>0.5= 25, acc>=0.3= 56, Total=714 \n",
      "[64]E, Avg Training loss = 2.4438 acc>0.5= 48, acc>=0.3= 97, Total=714 ::Val Loss = 2.6892 acc>0.5= 24, acc>=0.3= 53, Total=714 \n",
      "[65]E, Avg Training loss = 2.3705 acc>0.5= 51, acc>=0.3= 103, Total=714 ::Val Loss = 2.6673 acc>0.5= 28, acc>=0.3= 63, Total=714 \n",
      "[66]E, Avg Training loss = 2.4377 acc>0.5= 47, acc>=0.3= 100, Total=714 ::Val Loss = 2.6452 acc>0.5= 31, acc>=0.3= 57, Total=714 \n",
      "[67]E, Avg Training loss = 2.3941 acc>0.5= 45, acc>=0.3= 102, Total=714 ::Val Loss = 2.6599 acc>0.5= 19, acc>=0.3= 47, Total=714 \n",
      "[68]E, Avg Training loss = 2.3823 acc>0.5= 55, acc>=0.3= 112, Total=714 ::Val Loss = 2.5977 acc>0.5= 39, acc>=0.3= 85, Total=714 \n",
      "[69]E, Avg Training loss = 2.3846 acc>0.5= 45, acc>=0.3= 93, Total=714 ::Val Loss = 2.6309 acc>0.5= 20, acc>=0.3= 43, Total=714 \n",
      "[70]E, Avg Training loss = 2.3508 acc>0.5= 61, acc>=0.3= 115, Total=714 ::Val Loss = 2.5938 acc>0.5= 38, acc>=0.3= 79, Total=714 \n",
      "[71]E, Avg Training loss = 2.366 acc>0.5= 43, acc>=0.3= 91, Total=714 ::Val Loss = 2.6103 acc>0.5= 37, acc>=0.3= 66, Total=714 \n",
      "[72]E, Avg Training loss = 2.3465 acc>0.5= 58, acc>=0.3= 112, Total=714 ::Val Loss = 2.6817 acc>0.5= 24, acc>=0.3= 53, Total=714 \n",
      "[73]E, Avg Training loss = 2.356 acc>0.5= 58, acc>=0.3= 111, Total=714 ::Val Loss = 2.6117 acc>0.5= 31, acc>=0.3= 53, Total=714 \n",
      "[74]E, Avg Training loss = 2.4034 acc>0.5= 48, acc>=0.3= 100, Total=714 ::Val Loss = 2.6572 acc>0.5= 46, acc>=0.3= 80, Total=714 \n",
      "[75]E, Avg Training loss = 2.3375 acc>0.5= 53, acc>=0.3= 96, Total=714 ::Val Loss = 2.6025 acc>0.5= 29, acc>=0.3= 59, Total=714 \n",
      "[76]E, Avg Training loss = 2.3862 acc>0.5= 63, acc>=0.3= 124, Total=714 ::Val Loss = 2.6081 acc>0.5= 38, acc>=0.3= 69, Total=714 \n",
      "[77]E, Avg Training loss = 2.4255 acc>0.5= 57, acc>=0.3= 123, Total=714 ::Val Loss = 2.6677 acc>0.5= 34, acc>=0.3= 70, Total=714 \n",
      "[78]E, Avg Training loss = 2.3431 acc>0.5= 47, acc>=0.3= 101, Total=714 ::Val Loss = 2.6883 acc>0.5= 31, acc>=0.3= 57, Total=714 \n",
      "[79]E, Avg Training loss = 2.4029 acc>0.5= 63, acc>=0.3= 122, Total=714 ::Val Loss = 2.6444 acc>0.5= 43, acc>=0.3= 69, Total=714 \n",
      "[80]E, Avg Training loss = 2.3915 acc>0.5= 46, acc>=0.3= 93, Total=714 ::Val Loss = 2.5928 acc>0.5= 30, acc>=0.3= 68, Total=714 \n",
      "[81]E, Avg Training loss = 2.3888 acc>0.5= 65, acc>=0.3= 136, Total=714 ::Val Loss = 2.5739 acc>0.5= 39, acc>=0.3= 75, Total=714 \n",
      "[82]E, Avg Training loss = 2.3563 acc>0.5= 47, acc>=0.3= 105, Total=714 ::Val Loss = 2.6144 acc>0.5= 34, acc>=0.3= 68, Total=714 \n",
      "[83]E, Avg Training loss = 2.3329 acc>0.5= 56, acc>=0.3= 130, Total=714 ::Val Loss = 2.5574 acc>0.5= 33, acc>=0.3= 80, Total=714 \n",
      "[84]E, Avg Training loss = 2.3635 acc>0.5= 59, acc>=0.3= 115, Total=714 ::Val Loss = 2.5832 acc>0.5= 32, acc>=0.3= 72, Total=714 \n",
      "[85]E, Avg Training loss = 2.3311 acc>0.5= 58, acc>=0.3= 127, Total=714 ::Val Loss = 2.5718 acc>0.5= 42, acc>=0.3= 81, Total=714 \n",
      "[86]E, Avg Training loss = 2.293 acc>0.5= 47, acc>=0.3= 112, Total=714 ::Val Loss = 2.623 acc>0.5= 31, acc>=0.3= 68, Total=714 \n",
      "[87]E, Avg Training loss = 2.3677 acc>0.5= 68, acc>=0.3= 138, Total=714 ::Val Loss = 2.5869 acc>0.5= 30, acc>=0.3= 82, Total=714 \n",
      "[88]E, Avg Training loss = 2.3511 acc>0.5= 54, acc>=0.3= 123, Total=714 ::Val Loss = 2.6296 acc>0.5= 29, acc>=0.3= 59, Total=714 \n",
      "[89]E, Avg Training loss = 2.3149 acc>0.5= 61, acc>=0.3= 126, Total=714 ::Val Loss = 2.5762 acc>0.5= 34, acc>=0.3= 64, Total=714 \n",
      "[90]E, Avg Training loss = 2.3563 acc>0.5= 54, acc>=0.3= 121, Total=714 ::Val Loss = 2.6009 acc>0.5= 32, acc>=0.3= 67, Total=714 \n",
      "[91]E, Avg Training loss = 2.3146 acc>0.5= 56, acc>=0.3= 115, Total=714 ::Val Loss = 2.5112 acc>0.5= 40, acc>=0.3= 83, Total=714 \n",
      "[92]E, Avg Training loss = 2.354 acc>0.5= 51, acc>=0.3= 118, Total=714 ::Val Loss = 2.6113 acc>0.5= 29, acc>=0.3= 60, Total=714 \n",
      "[93]E, Avg Training loss = 2.3014 acc>0.5= 54, acc>=0.3= 124, Total=714 ::Val Loss = 2.5883 acc>0.5= 30, acc>=0.3= 65, Total=714 \n",
      "[94]E, Avg Training loss = 2.3747 acc>0.5= 47, acc>=0.3= 115, Total=714 ::Val Loss = 2.5476 acc>0.5= 43, acc>=0.3= 87, Total=714 \n",
      "[95]E, Avg Training loss = 2.2898 acc>0.5= 58, acc>=0.3= 125, Total=714 ::Val Loss = 2.5342 acc>0.5= 36, acc>=0.3= 70, Total=714 \n",
      "[96]E, Avg Training loss = 2.3471 acc>0.5= 54, acc>=0.3= 135, Total=714 ::Val Loss = 2.5552 acc>0.5= 32, acc>=0.3= 71, Total=714 \n",
      "[97]E, Avg Training loss = 2.2647 acc>0.5= 63, acc>=0.3= 118, Total=714 ::Val Loss = 2.5819 acc>0.5= 36, acc>=0.3= 86, Total=714 \n",
      "[98]E, Avg Training loss = 2.3324 acc>0.5= 60, acc>=0.3= 123, Total=714 ::Val Loss = 2.5433 acc>0.5= 51, acc>=0.3= 89, Total=714 \n",
      "[99]E, Avg Training loss = 2.2746 acc>0.5= 60, acc>=0.3= 134, Total=714 "
     ]
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hparams.lr = new_lr/100 #7.5e-12 # can devide by 10\n",
    "trainer.fit(model,dm) # Fit model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test(datamodule=dm) # testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "As we can see from the above outputs, in 100 epochs the Transformers models predict roughly more than 135 alarms sources with more than 50% accuracy while RNN model only able to predict 51 alrarm sources with more than 50% accuracy. Thus, the Transformers performs relative better as compared to RNNs on next alarm prediction task. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('dl': conda)",
   "language": "python",
   "name": "python38564bitdlconda5312cd9c08564c7aafb6787e3983aeca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}